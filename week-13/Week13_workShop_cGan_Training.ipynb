{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional GAN\n",
    "---\n",
    "*Responsible:* Robert Currie (<rob.currie@ed.ac.uk>)\n",
    "\n",
    "## What is the target of this workshop?\n",
    "\n",
    "### cGAN (paper - https://arxiv.org/abs/1411.1784)\n",
    "\n",
    "The goal of this workshop is to construct and train a cGAN model from scratch using the mnist dataset.\n",
    "\n",
    "This will be making use of the TensorFlow functional API to build a Generator and Discriminator model in separate and them combining these to train a complete cGAN model.\n",
    "\n",
    "### FID (FrÃ©chet Inception Distance)\n",
    "\n",
    "Using the Inception v3 model from Google (paper - http://arxiv.org/abs/1512.00567) it's possible to calculate the FID metric from our trained model.\n",
    "\n",
    "This gives us an ability to score the behaviour of our model compared to our real or training dataset.\n",
    "\n",
    "$ FID = d^2 = \\|\\mu_1 - \\mu_2\\|^2 + Tr\\left(\\sigma_1 + \\sigma_2 - 2\\sqrt{ \\sigma_1 \\cdot \\sigma_2 }\\right) $\n",
    "\n",
    "\n",
    "## Mark Schema\n",
    "\n",
    "As with previous ML notebooks the sections marked **##FINISH ME##** are to be completed by you.\n",
    "\n",
    "Marks for the different parts are shown below.\n",
    "\n",
    "* Sections are intended to be tackled in order, i.e. 1->9\n",
    "* In this notebook different sections can be tackled independently\n",
    "* There are bonus problems at the end to tackle but the maximum mark is 10/10\n",
    "\n",
    "| <p align='left'> Title                         | <p align='left'> Parts | <p align='left'> Number of marks |\n",
    "| ------------------------------------- | ----- | --- |\n",
    "| <p align='left'> 1. Load the dataset and normalise to $ \\left[-1, 1\\right] $ | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 2. Complete the Generator class | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 3. Use the Generator to generate some 'pseudo-numbers' with the correct shape | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 4. Complete the Discriminator class and use the Discriminator to decide if an image is real or fake | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 5. Complete the training methods for the cGAN class | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 6. Why are 2 separate optimizers needed for this cGAN model? | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 7. Write a method to generate&plot 10x10 images with numbers 0-9 left to right | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 8. Train the cGAN for 20 epochs | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 9. Complete the code required to calculate the FID | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 10. Calculate the FID for your trained model | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> (Optional) 11. Plot how the FID for models varies vs training epochs | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> **Total** | | <p align='left'> max **10** |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Imports and Fixing Reproducibility\n",
    "\n",
    "TF on a GPU will use different algorithms behind the scenes then with a CPU.\n",
    "\n",
    "Some of these algorithms will make some assumptions/sacrifices which will give up among other things exact numerical reproducibility in favour of speed/performance.\n",
    "\n",
    "In addition to this, we should always try to fix the seeds for the AI/ML framework and NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "_FIXED_SEED=5432\n",
    "os.environ[\"PYTHONHASHSEED\"]=str(_FIXED_SEED)\n",
    "random.seed(_FIXED_SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(_FIXED_SEED)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(_FIXED_SEED)\n",
    "tf.keras.utils.set_random_seed(_FIXED_SEED)  # sets seeds for base-python, numpy and tf\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the rest of the tools needed for the workshop\n",
    "\n",
    "This workshop will focus on using the MNIST dataset again.\n",
    "\n",
    "One of the reasons for this is that this datasdet has good ojects and features which means that we can recover the different species with relatively short training on a CPU.\n",
    "If you have access to a GPU you can try running the same algorithms on more complex datasets to see the output. One of these is the fashion_mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Used for building/training a cGAN\n",
    "from numpy.random import randint\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "#from tensorflow.keras.datasets.fashion_mnist import load_data\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "\n",
    "# Used for calculating the FID of the model\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from scipy.linalg import sqrtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target size\n",
    "height_px, width_px, n_channels = (28, 28, 1)\n",
    "BATCH_SIZE = 128\n",
    "N_CLASSES = 10\n",
    "\n",
    "SAVE_RESULT = \"results\"\n",
    "\n",
    "LATENT_DIM = 100\n",
    "\n",
    "# Set the number of epochs for trainining.\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train data\n",
    "(x_train, y_train), (x_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise the dataset to have values between $\\left[-1, 1\\right]$\n",
    "\n",
    "We also want to build a TensorFlow dataset using `tf.data.Dataset.from_tensor_slices`\n",
    "\n",
    "It's perhaps more of a convention that when traiing a GAN we normalise the dataset to be between [-1, 1] rather than [0,1]. This should have little impact in the final result other than changing your Generators final Activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## FINISH_ME ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator model\n",
    "\n",
    "The GeneratorClass is where we want to collect everything together for the Generation of new pseudo-number images.\n",
    "\n",
    "The intention is that we be able to call an instance of this class to generate a new value such as:\n",
    "\n",
    "```\n",
    "generator_instance = GeneratorClass(...)\n",
    "output_image = generator_instance(noise=noise, label=label)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorClass(tf.Module):\n",
    "    def __init__(self, out_dim, n_classes=10, h_low=7, w_low=7):\n",
    "        tf.Module.__init__(self)\n",
    "\n",
    "        # 7x7 chosen to give us a 28x28 generated output\n",
    "        n_nodes = h_low * w_low\n",
    "\n",
    "        # Labels inputs\n",
    "        # We need a sequential set of layers to take our label data and 'Embedding' this with a Dense output.\n",
    "        # The output of this layer set has to output with the format of: h_low x w_low x 1\n",
    "\n",
    "        self.label_embedding = ## FINISH_ME ##\n",
    "\n",
    "        # Noise inputs\n",
    "        # We need a sequential set of layers to go from the latent-space dim to be able to up-scale an image\n",
    "        # The output of this layer set has to output with teh format of: h_low x w_low x 64\n",
    "\n",
    "        self.noise_sampler = ## FINISH_ME ##\n",
    "\n",
    "\n",
    "        # Model layers\n",
    "        # Building on the CNN model in the last workshop we need to build a sequential model to upscale using:\n",
    "        # Conv2DTranspose 4x4 stride (2,2)\n",
    "        # Conv2DTranspose 4x4 stride (2,2)\n",
    "        # Conv2D 4x4 stride (1,1)\n",
    "        # This model should take as input a 7x7 model and output a 28x28 \n",
    "\n",
    "        self.upscale_model = ## FINISH_ME ##\n",
    "\n",
    "        # merge layer for optim\n",
    "        self.merge = L.Concatenate()\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, labels, noise, training=False):\n",
    "\n",
    "        # Take labels and add dense layer to connect them to noise images\n",
    "        labels = self.label_embedding(labels, training=training)\n",
    "        # Take noise 'seed' and to input images\n",
    "        noise = self.noise_sampler(noise, training=training)\n",
    "\n",
    "        # Merge the noise and labels\n",
    "        x = self.merge([noise, labels])\n",
    "\n",
    "        # Generate an output image from 'noise' seed using input label\n",
    "        output_img = self.upscale_model(x, training=training)\n",
    "\n",
    "        return output_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Construct a Generator instance and use it to 'generate' an image with a given label\n",
    "\n",
    "We're going to use the g_model later, but we should check that it works as expected before we move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a Generator instance\n",
    "g_model = GeneratorClass(n_channels, n_classes=N_CLASSES)\n",
    "# This line is needed to help define our generator for later\n",
    "call = g_model.__call__.get_concrete_function(tf.TensorSpec((1, 1), tf.int32, name='label'), tf.TensorSpec((1, LATENT_DIM), tf.float32, name='noise'))\n",
    "\n",
    "\n",
    "## FINISH_ME ##\n",
    "# Generate a random sample in the latent space with a random label and use this to seed the generator model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator model\n",
    "\n",
    "This is our discriminator model which gives us an output of how 'real' or 'fake' an image as a single score between [0,1].\n",
    "\n",
    "As with the generator we intend to be able to call an instance of this class to perform an evaluation on an input for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorClass(tf.Module):\n",
    "    def __init__(self, in_shape, out_dim, n_classes=10):\n",
    "        tf.Module.__init__(self)\n",
    "\n",
    "        n_nodes = in_shape[0]*in_shape[1]*in_shape[2]\n",
    "\n",
    "        # Labels inputs\n",
    "        # Build a short sequential model which embeds the the label data into a layer with the same dim as the input\n",
    "        self.label_embedding = ## FINISH_ME ##\n",
    "\n",
    "        # Model layers \n",
    "        # Building on the CNN model in the last workshop we need to build a sequential model to downscale the input image into the latent-space\n",
    "        self.downsample_model = ## FINISH_ME ##\n",
    "\n",
    "        # merge layer for optim\n",
    "        self.merge = L.Concatenate()\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def __call__(self, labels, images, training=False):\n",
    "        \n",
    "        # Take labels and add dense layer to connect them to noise images\n",
    "        labels = self.label_embedding(labels, training=training)\n",
    "\n",
    "        # Merge labels and images\n",
    "        x = self.merge([images, labels])\n",
    "        \n",
    "        # Discriminate input and produce a vector representation of the input in latent-space\n",
    "        latent_rep = self.downsample_model(x, training=training)\n",
    "\n",
    "        return latent_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct a Discriminator instance and use it to make a decision if an image is real ('1') or fake ('0')\n",
    "\n",
    "We want to test the functionality again here, we haven't trained the model so don't expect the output to be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = DiscriminatorClass(in_shape=(height_px, width_px, n_channels), out_dim=1, n_classes=N_CLASSES)\n",
    "\n",
    "## FINISH_ME ##\n",
    "# Test the discriminator model and see if it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define whole cGAN class which controls the training process\n",
    "\n",
    "The intention of the GAN model is that it captures everything to do with the training of the GAN within the train_step method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGAN_Class(tf.Module):\n",
    "\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        tf.Module.__init__(self)\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        # Instantiate the optimizer for both networks\n",
    "        # (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
    "        self.d_optimizer = ## FINISH_ME ##\n",
    "        self.g_optimizer = ## FINISH_ME ##\n",
    "        self.bce = BinaryCrossentropy()\n",
    "\n",
    "\n",
    "    def train_step(self, real_images, real_labels):\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "\n",
    "        ## First train the Discriminator\n",
    "        #################################\n",
    "\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            d_loss = self.discriminator_train_step(real_images, real_labels, random_latent_vectors)\n",
    "\n",
    "        # Get the gradients w.r.t the discriminator loss\n",
    "        d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "        # Update the weights of the discriminator using the discriminator optimizer\n",
    "        self.d_optimizer.apply_gradients(zip(d_gradient, self.discriminator.trainable_variables))\n",
    "\n",
    "\n",
    "        ## Now train the Generator\n",
    "        #################################\n",
    "\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_labels = tf.random.uniform([batch_size], minval=0, maxval=N_CLASSES, dtype=tf.int32)\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        with tf.GradientTape() as tape:\n",
    "            g_loss = self.generator_train_step(random_latent_vectors, random_labels)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(zip(gen_gradient, self.generator.trainable_variables))   \n",
    "\n",
    "        \n",
    "        ## Return the Discriminator and the Generator loss\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n",
    "\n",
    "\n",
    "    def discriminator_train_step(self, real_images, real_labels, random_latent_vectors):\n",
    "\n",
    "        # Generate fake images from the latent vector\n",
    "        fake_images = self.generator(labels=real_labels, noise=random_latent_vectors, training=False)\n",
    "        # Get the logits for the fake images\n",
    "        fake_logits = self.discriminator(labels=real_labels, images=fake_images, training=True)\n",
    "        # Get the logits for the real images\n",
    "        real_logits = self.discriminator(labels=real_labels, images=real_images, training=True)\n",
    "\n",
    "\n",
    "        ## Calculate the BCE for the Real and Fake images\n",
    "        ## Hint: `tf.ones_like` and `tf.zeros_like` gives us a Tensor with dimensions similar to input to use in BCE\n",
    "        real_loss = self.bce( ## FINISH_ME ## )\n",
    "        fake_loss = self.bce( ## FINISH_ME ## )\n",
    "        return (fake_loss + real_loss) * 0.5\n",
    "\n",
    "\n",
    "    def generator_train_step(self, random_latent_vectors, random_labels):\n",
    "        # Train the generator\n",
    "\n",
    "        # Generate fake images using the generator\n",
    "        generated_images = self.generator(labels=random_labels, noise=random_latent_vectors, training=True)\n",
    "        # Get the discriminator logits for fake images\n",
    "        gen_img_logits = self.discriminator(labels=random_labels, images=generated_images, training=False)\n",
    "\n",
    "        ## Calculate the generator loss\n",
    "        ## Hint: we're training the generator to 'trick' the Discriminator\n",
    "        g_loss = self.bce( ## FINISH_ME ## )\n",
    "        \n",
    "        return g_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model.\n",
    "cgan_model = CGAN_Class(discriminator=d_model, generator=g_model, latent_dim=LATENT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you say why this cGAN model is using 2 separate optimizers for training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`## FINISH_ME ##`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a method to generate and plot 10x10 images going 0-9, left-right\n",
    "\n",
    "This method should take a model and use the Generator to generate 100 images with 10 zeros, ones, ... left to right in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numbers(model, latent_dim):\n",
    "\n",
    "    ## FINISH_ME ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training\n",
    "\n",
    "We now want to train our cGAN.\n",
    "\n",
    "We want to report on the progress of our training every n steps and every epoch to make sure that the training is converging correctly.\n",
    "\n",
    "Training the cGAN on CPLab machines should take ~1hr for 20 steps. You should expect to see number-like output from the model after 5-6 epochs and if you're stuck for time you can consider just training for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_loss, disc_loss = [], []\n",
    "\n",
    "for ep in range(epochs):\n",
    "    \n",
    "    # Shuffle and batch the dataset\n",
    "    _dataset = train_dataset.shuffle(60000)\n",
    "    # Create an iterable over the dataset\n",
    "    train_iter = iter(_dataset)\n",
    "    for i, (real_images, real_labels) in enumerate(train_iter):\n",
    "\n",
    "        data_losses = cgan_model.train_step(real_images=real_images, real_labels=real_labels)\n",
    "\n",
    "        plot_numbers( ## FINISH_ME ## )\n",
    "\n",
    "        ## FINISH_ME ##\n",
    "\n",
    "        # Report on the progress of the training to make sure the training doesn't go wrong\n",
    "        # At the start of each epoch, generate 10x10 new images and display them to see if the trained generator is improving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot loss graphics\n",
    "\n",
    "We want to see the loss functions of Generator and the Discriminator as the models are trained against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smoothed_values(data_list, decay=0.1):\n",
    "\n",
    "    ## FINISH_ME ##\n",
    "    # This is technically an optional step, but plotting an averaged loss function is less intensive than throwing all of the raw values into pyplot\n",
    "    \n",
    "    return final_values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(get_smoothed_values(gen_loss, decay=0.2))\n",
    "plt.plot(get_smoothed_values(disc_loss, decay=0.2))\n",
    "#plt.plot(gen_loss, decay=0.2)\n",
    "#plt.plot(disc_loss, decay=0.2)\n",
    "\n",
    "plt.legend(['gen loss', 'disc loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to allow us to save our model correctly and be able to call it after we load it from disk\n",
    "call = g_model.__call__.get_concrete_function(tf.TensorSpec((1, 1), tf.int32, name='label'), tf.TensorSpec((1, LATENT_DIM), tf.float32, name='noise'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(g_model, 'final_model_{}'.format(epochs), signatures=call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip model in order to download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r 'final_model.zip' 'final_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to download final model - click link below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1><a href=\"final_model.zip\"> Download trained generator </a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test saved model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.saved_model.load('final_model_{}'.format(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_latent_vectors = np.random.normal(size=(100, LATENT_DIM))\n",
    "random_labels = np.asarray([min(x, N_CLASSES-1)  for _ in range(10) for x in range(10)])\n",
    "#generated_images = model(label_i=random_labels, noise_i=random_latent_vectors)\n",
    "generated_images = model(random_labels, random_latent_vectors)\n",
    "# scale from [-1,1] to [0,1]\n",
    "generated_images = ((generated_images + 1) / 2.0).numpy()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=6, figsize=(15,10), subplot_kw={'xticks':[], 'yticks':[]})\n",
    "indx = 0\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = generated_images[i]#.reshape(height_px, width_px)\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    label_leg = random_labels[i]\n",
    "    ax.set_title(label_leg)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate FID metric\n",
    "\n",
    "This is calculating the FID distance between 2 distributions as defined at the top of the notebook.\n",
    "\n",
    "This uses the InceptionV3 model from Google to calculate 'activations' based on the presented images. We then effevctively want to compare the distribution of 'real' and 'fake' images in this activation-space.\n",
    "\n",
    "If the Generator is perfectly the same as the training data this distance would be zero as the generator would be generating new numbers. Hence the smaller the distance between the 2 distributions of images the closer the generator generatig new instances from the input datadet. Large numbers here suggest the generator may be generating new datapoints which may not be realistic or physical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_DATA = 60000\n",
    "N_BATCH = 30\n",
    "\n",
    "#x_test, y_test = shuffle(x_test, y_test)\n",
    "\n",
    "generated_imgs_list = []\n",
    "for label in y_test[:N_DATA]:\n",
    "\n",
    "    ## GENERATE N_DATA 'pseudo-datapoints' from the same labels as the real _training_ data ##\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test plot\n",
    "plt.imshow(generated_imgs_list[400][0], cmap='gray'), y_test[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meet inception model minimum size and channels, 75x75x3\n",
    "inception_model_input = (height_px*3, width_px*3, n_channels*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale an array of images to a new size\n",
    "def scale_images(images, new_shape=inception_model_input):\n",
    "    images_list = list()\n",
    "\n",
    "    ## FINISH_ME ##\n",
    "\n",
    "    ## IMAGES FROM OUR GENERATOR NEED TO BE SCALED TO A SIZE WE CAN USE THEM WITH InceptionV3 ##\n",
    "    ## HINT: tf.image.resize exists for such problems ##\n",
    "\n",
    "    return np.asarray(images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_prediction(model, images1, images2):\n",
    "    # calculate activations\n",
    "    act1 = model.predict(images1)\n",
    "    act2 = model.predict(images2)\n",
    "    return act1, act2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_prediction(model, images1, images2, bs=N_BATCH):\n",
    "    \"\"\"\n",
    "    Collect prediction on `images1`/`imges2` datasets with `model` \n",
    "    using batch size equal to `bs`\n",
    "    \"\"\"\n",
    "    final_act1_l = []\n",
    "    final_act2_l = []\n",
    "    \n",
    "    batch_i1 = []\n",
    "    batch_i2 = []\n",
    "\n",
    "    ## FINISH_ME ##\n",
    "\n",
    "    ## We need to:\n",
    "    ## 1) iterate through all of out input and generated data\n",
    "    ## 2) Scale all images to the correct size so we can use this model\n",
    "    ## 3) Convert the images from B&W to RGB using `tf.image.grayscale_to_rgb` or equivalent (again just for compatability with the model)\n",
    "    ## 4) Calculate and store the predictions for the real and fake data\n",
    "\n",
    "\n",
    "    final_act1_l = np.concatenate(final_act1_l, axis=0)\n",
    "    final_act2_l = np.concatenate(final_act2_l, axis=0)\n",
    "    return final_act1_l, final_act2_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate frechet inception distance\n",
    "def calculate_fid(model, images1, images2, bs=N_BATCH):\n",
    "\n",
    "    # Calculate activations for real and fake data\n",
    "    # The returned list/vector of activations should be the same length\n",
    "    act1, act2 = collect_prediction(model, images1, images2, bs)\n",
    "\n",
    "    # calculate mean and covariance-matrix over all analyzed images in 'activation-space'\n",
    "    # You should have a 1D tensor of vectors in the activation-space in numpy\n",
    "    # With that in mind you should be able to use the numpy built-in methods to extract the means and varience\n",
    "    mu1, sigma1 = ## FINISH_ME ##\n",
    "    mu2, sigma2 = ## FINISH_ME ##\n",
    "\n",
    "    # difference between means ^2\n",
    "    mu_diff = np.sum((mu1 - mu2)**2.0)\n",
    "    # SQRT of dot-product of sigmas\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    # Only take real values\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    # calculate distance\n",
    "    fid = mu_diff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the inception v3 model\n",
    "classifier_model = InceptionV3(include_top=False, pooling='avg', input_shape=inception_model_input)\n",
    "# Take parts of datasets\n",
    "images1 = generated_imgs_list[:N_DATA]\n",
    "images2 = x_test[:N_DATA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fid between images1 and images2\n",
    "fid = calculate_fid(classifier_model, images1, images2, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FID (different): %.3f' % fid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show how FID changes as models improve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've provided a few examples of the cGAN model which have been trained by a GPU.\n",
    "\n",
    "As a short 'free-form' exercise, load each of these models and use them to calculate how the FID varies for a more trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

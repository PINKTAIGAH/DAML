{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2\n",
    "---\n",
    "This is an example Jupyter notebook for some of the topics covered in lecture 2, part of the _\"Data science tools and Machine Learning\"_ track. See also the `data-science-tools.ipynb` notebook for a good overview of the available methods in `numpy`, `pandas`, and `matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data handling\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn off some warnings which we can ignore for this example\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Standard import(s)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris dataset \n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available properties of the dataset\n",
    "iris.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show dataset description\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show size of dataset\n",
    "# The dataset has 150 observations and 4 features\n",
    "iris.data.shape, iris.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show feature and target names\n",
    "print(\"Feature names: {}\".format(iris.feature_names))\n",
    "print(\"Target names:  {}\".format(iris.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create pandas.DataFrame\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column(s) for target (type)\n",
    "df['type']   = iris.target_names[iris.target]\n",
    "df['target'] = iris.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In cases with incomplete and/or corrupted data, it may be necessary to do some\n",
    "# cleaning.\n",
    "# -- Remove duplicates *if* you have reason to suspect that repeated, identical \n",
    "#    entries are pathological (they might not be)\n",
    "# Drop duplicates and null values \n",
    "df = df.drop_duplicates().dropna()\n",
    "\n",
    "# -- Remove rows with 'not-a-number' in the features\n",
    "nan = np.any(np.isnan(df[iris.feature_names]), axis=1)\n",
    "df  = df[~nan]\n",
    "\n",
    "print (\"Shape after cleaning: {}\".format(df.shape))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to, and reload from, CSV file\n",
    "df.to_csv(\"iris.csv\")\n",
    "\n",
    "# ...\n",
    "\n",
    "df2 = pd.read_csv(\"iris.csv\", index_col=0)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Access column two ways\n",
    "df.type\n",
    "df['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List subset of columns\n",
    "df[['sepal width (cm)', 'petal width (cm)', 'type']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List subset of rows by row number\n",
    "df[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... or, equivalently\n",
    "df.iloc[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes, row number and index differ (e.g. when shuffling)\n",
    "df = df.sample(frac=1, replace=False, random_state=1234)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This illustrates the difference in accessing row by row number \n",
    "df.iloc[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... or by *index*\n",
    "df.loc[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the measurement with target > 0 and the type for these measurements. Find the unique such type\n",
    "df.query('target  > 0').type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of rows in each class\n",
    "df.groupby('type').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the rows (highest values -> ascending=False) by sepal length showing only the relevant columns (sepal width and petal lenght) and the first 2 rows\n",
    "df.sort_values(by=['sepal length (cm)'], ascending=False)[['sepal width (cm)', 'petal length (cm)']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grop data by type and get the mean of sepal length for each group, and sort by descending value\n",
    "df.groupby('type')['sepal length (cm)'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only events with sepal length greater than 6.0 and count them grouped by type\n",
    "mask = df['sepal length (cm)'] > 6.0\n",
    "df[mask].groupby('type').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas.DataFrame to numpy.array\n",
    "array = df['type'].values\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy.array to python list\n",
    "array.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Visualisation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature to plot\n",
    "feat = 'petal width (cm)'\n",
    "\n",
    "# Bin range\n",
    "bins = np.linspace(0, 3, 15 + 1, endpoint=True)\n",
    "\n",
    "# Create figure and axis objects.\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for t in iris.target_names:\n",
    "    # Boolean mask\n",
    "    mask = df['type'] == t\n",
    "    \n",
    "    # Make histogram for current type\n",
    "    ax.hist(df[mask][feat], bins=bins, alpha=0.5, label=t)\n",
    "    pass\n",
    "\n",
    "# Decorations\n",
    "ax.legend()\n",
    "ax.set_xlabel(feat)\n",
    "ax.set_ylabel(\"Number of entries\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to plot\n",
    "featx = 'sepal length (cm)'\n",
    "featy = 'sepal width (cm)'\n",
    "\n",
    "# Create figure and axis objects.\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for t in iris['target_names']:\n",
    "    # Boolean mask\n",
    "    mask = df['type'] == t\n",
    "\n",
    "    # Scatter plot for current type\n",
    "    ax.scatter(df[mask][featx], df[mask][featy], label=t, alpha=0.5)\n",
    "    pass\n",
    "\n",
    "# Draw legend\n",
    "ax.legend()\n",
    "ax.set_xlabel(featx)\n",
    "ax.set_ylabel(featy)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue = 'type');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Trees\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics # Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import preprocessing # Import preprocessing for String-Int conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, 2:] # only focus on petal length and width\n",
    "Y = iris.target\n",
    "feature_names = iris.feature_names[2:]\n",
    "print(\"given:\",feature_names, \n",
    "      \"\\npredict whether:\", iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use matplotlib as you did on previous labs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color_map = [\"yo\", \"bs\", \"g^\"]\n",
    "for target_index, target_name in enumerate(iris.target_names):\n",
    "    plt.plot(X[:, 0][Y==target_index], # petal length on X axis (the ones that equal to target)\n",
    "             X[:, 1][Y==target_index], # petal width on Y axis (the ones that equal to target)\n",
    "             color_map[target_index], \n",
    "             label=target_name)\n",
    "plt.xlabel(\"petal length\")\n",
    "plt.ylabel(\"petal width\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split of the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split the dataset into training and test set is fundamental. Usually the training set consists of 70% of data and the test set of 30%. (test_size option on the train_test_split function is used for the percentage of splitting).\n",
    "* After the split, the train set (x_train) is used for the training of the algorithm.\n",
    "* After the training, the test set (y_test) is used to predict the outcome of unseen data.\n",
    "* The accuracy_score function is then used to give an estimate of the accuracy of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into training set and test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=1) # 70% training and 30% test\n",
    "\n",
    "# Create Decision Tree classifer object with these parameters\n",
    "dt = DecisionTreeClassifier(criterion = 'entropy',max_depth = 3)\n",
    "# Train Decision Tree Classifer\n",
    "dt = dt.fit(x_train,y_train)\n",
    "# Predict the response for test dataset\n",
    "y_pred = dt.predict(x_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(dt,\n",
    "                out_file=\"iris_tree.dot\",\n",
    "                rounded=True,\n",
    "                filled=True,\n",
    "                feature_names=iris.feature_names\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the decision_tree as png\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(dt, \n",
    "                   filled=True,\n",
    "                  feature_names=iris.feature_names, rounded=True, class_names=list(iris.target_names))\n",
    "fig.savefig(\"decision_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance from the DT is calculated during the training (`feature_importances_`) and encodes how successful each feature is at splitting the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isort = np.argsort(dt.feature_importances_)\n",
    "forest_importances = pd.Series(dt.feature_importances_[isort], index=np.array(iris.feature_names)[isort])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar( ax=ax)\n",
    "ax.set_title(\"Feature importances\")\n",
    "ax.set_ylabel(\"Feature importance\")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensamble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting: GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we use the Gradient boosting classifier\n",
    "n_estimators = 20\n",
    "\n",
    "bdt = GradientBoostingClassifier(max_depth=3, n_estimators=n_estimators)\n",
    "bdt.fit(x_train, y_train)\n",
    "y_pred = bdt.predict(x_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging: Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=20, max_depth=4)\n",
    "rf.fit(x_train, y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold, HalvingGridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### CONSTANTS ########\n",
    "FILEDIR = \"/home/s1835083/Desktop/numu_energy_studies.csv\"\n",
    "FEATURES_HEADERS = [\"total_hits2\", \"total_ring_PEs2\", \"recoDWallR2\", \"recoDWallZ2\", \"lambda_max_2\",]\n",
    "LABEL_HEADERS = [\"trueKE\",]\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(FILEDIR, sep=\",\", header=0, index_col=0).set_index(\"i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Nans\n",
    "dataframe.dropna(inplace=True)\n",
    "\n",
    "# Visualise the data\n",
    "dataframe.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of variables which will be used\n",
    "\n",
    "fig, ax = plt.subplots(2, 3,)\n",
    "plot_headers = []\n",
    "# Append all headers which will be plot\n",
    "plot_headers.extend(FEATURES_HEADERS)\n",
    "plot_headers.extend(LABEL_HEADERS)\n",
    "\n",
    "xlabels = [\"n_muons (a.u)\", \"n_hits (a.u)\", \"distance (a.u)\", \"distance (a.u)\", \"distance (a.u)\", \"energy (MeV)\"]\n",
    "\n",
    "for header, axis, xlabel in zip(plot_headers, ax.flatten(), xlabels):\n",
    "    axis.hist(dataframe[header], log=True, color=\"maroon\", bins=50, histtype=\"step\")\n",
    "    axis.set(\n",
    "        title=header,\n",
    "        ylabel=\"Counts\",\n",
    "        xlabel=xlabel,\n",
    "    )\n",
    "\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset used as input\n",
    "train_data, test_data, train_target, test_target = train_test_split (\n",
    "    dataframe[FEATURES_HEADERS],\n",
    "    dataframe[LABEL_HEADERS],\n",
    "    test_size = 0.3,\n",
    "    random_state = SEED, \n",
    ")\n",
    "\n",
    "# Print size of datasets\n",
    "print(f\"Size of train features: {train_data.shape}\")\n",
    "print(f\"Size of train labels:   {train_target.shape}\")\n",
    "print(f\"Size of test features:  {test_data.shape}\")\n",
    "print(f\"Size of test labels:    {test_target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regressor(input_features=5, output_features=1, loss=\"mean_squared_error\", optimizer=\"adam\"):\n",
    "    \"\"\"\n",
    "    Create a simple wide layer wide dense neural network w/ no hidden layers\n",
    "    \"\"\"\n",
    "    # Create network\n",
    "    network = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(32, input_dim=input_features, kernel_initializer=\"normal\"),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Dense(16, kernel_initializer=\"normal\"),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Dense(8, kernel_initializer=\"normal\"),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Dense(4, kernel_initializer=\"normal\"),\n",
    "        tf.keras.layers.LeakyReLU(),\n",
    "        tf.keras.layers.Dense(output_features, kernel_initializer=\"normal\"),\n",
    "    ])\n",
    "\n",
    "    # Compile and return network\n",
    "    network.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "    return network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our callbacks \n",
    "callbacks_ = [\n",
    "    EarlyStopping(verbose=False, patience=10, monitor=\"loss\"),\n",
    "    ModelCheckpoint(\"/home/s1835083/Desktop/model.h5\", monitor=\"loss\", verbose=0, save_best_only=True , mode=\"max\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparametrs\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 40\n",
    "\n",
    "# Make pseudorandom packages deterministic\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define input pipeline\n",
    "estimators = []\n",
    "estimators.append ((\"mlp\", KerasRegressor(build_fn=build_regressor, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose =1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits =10, random_state=SEED, shuffle=True)\n",
    "results = cross_val_score(pipeline , train_data , train_target , cv=kfold , scoring=\"r2\")\n",
    "\n",
    "print(\"Result: %.2f %s %.2f\" % (results.mean(), u\"\\u00B1\", results.std ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the network to the train dataset\n",
    "network = build_regressor()\n",
    "history = network.fit(train_data, train_target, validation_data=(test_data, test_target), epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss functions\n",
    "plt.plot(history.history[\"loss\"][1:], label=\"Train\", color=\"darkblue\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.title(\"Loss function (Training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction using traind model\n",
    "reco_KE = network.predict(test_data)\n",
    "\n",
    "# Create a plot of reco vs MC truth energy\n",
    "plt.scatter(test_target, reco_KE, s=0.5, c=\"maroon\", label=\"Prediction\")\n",
    "plt.plot(test_target, test_target, ls=\"--\", label=\"Ground truth\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Truth KE (MeV)\")\n",
    "plt.ylabel(\"Reco KE (MeV)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBRT Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dirty fit using basic params for regression tree\n",
    "regressor_tree = GradientBoostingRegressor(n_estimators=100)\n",
    "regressor_tree.fit(train_data , train_target.to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the r2 scores for the train data and test data\n",
    "\n",
    "train_r2 = regressor_tree.score(train_data, train_target.to_numpy())\n",
    "test_r2 = regressor_tree.score(test_data, test_target.to_numpy())\n",
    "\n",
    "print(f\"Train data r2: {train_r2:.4f}\")\n",
    "print(f\"Validation data r2: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search parameters\n",
    "param_grid_ = {\n",
    "    \"n_estimators\": [100],\n",
    "    \"learning_rate\": np.linspace(1e-2, 4e-2, 10),\n",
    "    \"max_depth\":    np.arange(10, 30),\n",
    "    \"min_samples_leaf\": np.arange(40, 50,),\n",
    "}\n",
    "n_jobs_ = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "regressor = GradientBoostingRegressor ()\n",
    "classifier = HalvinGGridSearchCV(estimator=regressor, cv=kfold , param_grid=param_grid_, n_jobs=n_jobs_, verbose =1)\n",
    "classifier.fit(train_data, train_target.to_numpy().ravel())\n",
    "print(\"Best estimator:\")\n",
    "print(classifier.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

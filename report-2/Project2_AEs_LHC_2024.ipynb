{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c322e2ea",
   "metadata": {},
   "source": [
    "# Project2: Anomaly Detection for Exotic Event Identification at the Large Hadron Collider \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d014c",
   "metadata": {},
   "source": [
    "## Brief Introduction to the Standard Model and Large Hadron Collider\n",
    "\n",
    "\n",
    "The Standard model (SM) of Particle Physics is the most complete model physicists have for understanding the interactions of the fundamental particles in the universe. The elementary particles of the SM are shown in Fig.1.\n",
    "\n",
    "---\n",
    "<figure>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Standard_Model_of_Elementary_Particles.svg/627px-Standard_Model_of_Elementary_Particles.svg.png\" alt=\"SM\" style=\"width: 600px;\"/>\n",
    "    <figcaption>Fig.1 - Elementary particles of the Standard Model.</figcaption>\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "It is comprised of matter particles (**fermions**):\n",
    "- **leptons**\n",
    "    - electrons\n",
    "    - muon\n",
    "    - tau\n",
    "    - and respective neutrinos\n",
    "- **quarks** which are the building blocks of protons\n",
    "\n",
    "as well as force carrier particles (**bosons**):\n",
    "- photon and W/Z bosons (electroweak force)\n",
    "- gluons (strong force)\n",
    "\n",
    "and the Higgs boson which is attributed to the mechanism which gives particles their mass.\n",
    "\n",
    "\n",
    "Though the SM has experimentally stood the test of time, many outstanding questions about the universe and the model itself remain, and scientist continue to probe for inconsistencies in the SM in order to find new physics. More exotic models such as **Supersymmetry (SUSY)** predic mirror particles which may exist and have alluded detection thus far. \n",
    "\n",
    "---\n",
    "\n",
    "The **Large Hadron Collider** (LHC) is a particle smasher capable of colliding protons at a centre of mass energy of 14 TeV.\n",
    "**ATLAS** is general purpouse particle detectors tasked with recording the remnants of proton collisions at the collicion point. The main purpouse of this experiment is to test the SM rigorously, and ATLAS was one of two expeririments (ATLAS+CMS) responsible for the discovery of the **Higgs boson in 2012**. \n",
    "\n",
    "Find an animation of how particles are reconstructed within a slice of the ATLAS detector here: https://videos.cern.ch/record/2770812. Electrons, muons, photons, quark jets, etc, will interact with different layers of the detector in different ways, making it possible to design algorithms which distinguish reconstructed particles, measure their trajectories, charge and energy, and identify them as particular types.\n",
    "\n",
    "Figure 2 shows an event display from a data event in ATLAS in which 2 muons (red), 2 electrons (green), and 1 quark-jet (purple cone) are found. This event is a candidate to a Higgs boson decaying to four leptons with an associated jet: $$H (+j)\\rightarrow 2\\mu 2e (+j)$$ \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://twiki.cern.ch/twiki/pub/AtlasPublic/EventDisplayRun2Physics/JiveXML_327636_1535020856-RZ-LegoPlot-EventInfo-2017-10-18-19-01-24.png\" alt=\"Higgs to leptons\" style=\"width: 600px;\"/>\n",
    "    <figcaption>Fig.2 - Event display of a Higgs candidate decaying to two muons and two electrons.</figcaption>\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Particles are shown transversing the detector material. The 3D histogram show \n",
    "* the azimuth $\\phi$ ( angle around the beam, 0 is up)\n",
    "* pseudo-rapidity $\\eta$ (trajectory along the beam) positions of the particle directions with respect to the interaction point.\n",
    "* The total energy measured for the particle is denoted by $E$,\n",
    "* the transverse momentum ($p_T$) deposited by the particle in giga-electronvolts (GeV) are shown by the hight of the histograms.\n",
    "\n",
    "A particle kinematics can then be described by a four-vector  $$\\bar{p} = (E,p_T,\\eta,\\phi)$$\n",
    "\n",
    "An additional importan quantity is the missing energy in the transverse plane (MET). This is calculated by taking the negative sum of the transverse momentum of all particles in the event.\n",
    "$$\\mathrm{MET} = -\\sum p_T$$\n",
    "\n",
    "With perfect detector performance the MET will sum to 0 if all outgoing particles are observed by the detector. Neutrinos cannot be measured by the detector and hence their precense produces non-zero MET."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655ada0c",
   "metadata": {},
   "source": [
    "## Anomally detection dataset\n",
    "\n",
    "For the anomally detection project we will use the dataset discussed in this publication: <p><a href=\"https://arxiv.org/pdf/2105.14027.pdf\" title=\"Anomalies\">The Dark Machines Anomaly Score Challenge:\n",
    "Benchmark Data and Model Independent Event\n",
    "Classification for the Large Hadron Collider</a></p>\n",
    "\n",
    "Familiarise yourself with the paper, in particular from sections 2.1 to 4.4.\n",
    "\n",
    "---\n",
    "\n",
    "The dataset contains a collection of simulated proton-proton collisions in a general particle physics detector (such as ATLAS). We will use a dataset containing `340 000` SM events (referred to as channel 2b in the paper) which have at least 2 electrons/muons in the event with $p_T>15$ GeV. \n",
    "\n",
    "**The events can be found in `background_chan2b_7.8.csv`**\n",
    "\n",
    "\n",
    "You can see all the SM processes that are simulated in Table 2 of the paper, \n",
    "\n",
    "    e.g., an event with a process ID of `w_jets` is a simulated event of two protons producing a lepton and neutrino and at least two jets.\n",
    "    \n",
    "$$pp\\rightarrow \\ell\\nu(+2j)$$\n",
    "\n",
    "---\n",
    "\n",
    "The datasets are collected as CSV files where each line represents a single event, with the current format:\n",
    "\n",
    "`event ID; process ID; event weight; MET; METphi; obj1, E1, pt1, eta1, phi1; obj2, E2, pt2, eta2, phi2; ...`\n",
    "See Section 2.2 for a description of the dataset.\n",
    "Variables are split by a semicolon `\";\"`\n",
    "- `event ID`: an identifier for the event number in the simulation\n",
    "- `process ID`: an identifier for the event simulation type\n",
    "- `event weight`: the weight associated to the simulated event (how important that event is)\n",
    "- `MET`: the missing transverse energy\n",
    "- `METphi`: the azimuth angle (direction) of the MET\n",
    "\n",
    "the a list of objects (particles) whose variables are split by commas `\",\"` in the following orger:\n",
    "- `obj`: the object type,\n",
    "\n",
    "    |Key|Particle|\n",
    "    |---|---|\n",
    "    |j|jet|\n",
    "    |b|b-jet|\n",
    "    |e-|electron|\n",
    "    |e+|positron|\n",
    "    |m-|muon|\n",
    "    |m+|muon+|\n",
    "    |g|photon|\n",
    "    \n",
    "    *see Table 1 of the paper*\n",
    "- `E`: the total measured particle energy in MeV, [0,inf]\n",
    "- `pt`: the transverse mementum in MeV, [0,inf]\n",
    "- `eta`: pseudo-rapidity, [-inf,inf]\n",
    "- `phi`: azimuth angle, radians [-3.14,3.14]\n",
    "\n",
    "---\n",
    "\n",
    "In addition to the SM events we are also provided simulated events from `Beyond Standard Model` (BSM) exotic physics models. They are summarised here:\n",
    "\n",
    "|Model | File Name | \n",
    "|---|---|\n",
    "|**SUSY chargino-chargino process**||\n",
    "||`chacha_cha300_neut140_chan2b.csv`|\n",
    "||`chacha_cha400_neut60_chan2b.csv`|\n",
    "||`chacha_cha600_neut200_chan2b.csv`|\n",
    "|**SUSY chargino-neutralino processes**||\n",
    "||`chaneut_cha200_neut50_chan2b.csv`|\n",
    "||`chaneut_cha250_neut150_chan2b.csv`|\n",
    "|**$Z'$ decay to leptons**||\n",
    "||`pp23mt_50_chan2b.csv`|\n",
    "||`pp24mt_50_chan2b.csv`|\n",
    "|**Gluino and RPV SUSY**||\n",
    "||`gluino_1000.0_neutralino_1.0_chan2b.csv`||\n",
    "||`stlp_st1000_chan2b.csv`||\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686809e6",
   "metadata": {},
   "source": [
    "## Project description\n",
    "*Responsible:* Robert Currie (<rob.currie@ed.ac.uk>)\n",
    "\n",
    "### Overview\n",
    "The task is to design an anomaly detection algorithm which is trained on the SM dataset and which can be used to flag up interesting (exotic) events from the BSM physics models.\n",
    "\n",
    "You will do this by designing a robust `AutoEncoder` which is trained on the event level variables `MET; METphi` and the kinematics of the particle level objects. The `AutoEncoder` needs to duplicate the input as output effectively while going through a laten space (bottleneck). \n",
    "\n",
    "You will then need to evaluate and discuss the performance of your `AutoEncoder` on the exotic models listed above, and come up with an appropiate metric to identify events from non SM physics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d3beb",
   "metadata": {},
   "source": [
    "# **Breakdown**\n",
    "\n",
    "In the project report you will be assessed in the following way.\n",
    "\n",
    "1. **Data exploration and preprocessing (20%):** Inspect the datasets; visualise the data (e.g. tables, plots, etc) in an appropriate way; study the composition of the dataset; perform any necessary preprocessing.\n",
    "2. **Model selection (30%):** Choose a promissing approach; construct the machine learning model; optimise the relevant hyperparameters; train your chosen model.\n",
    "3. **Performance evaluation (30%):** Evaluate the model in a way that gauges its ability to generalise to unseen data; compare to other approaches; identify the best approach. \n",
    "4. **Discussion, style throughout (20%):** Discuss the reasoning or intuition behind your choices; the results you obtain through your studies; the relative merits of the methods you have developed, _etc._ Similarly, make sure that you write efficient code, document your work, clearly convey your results, and convince us that you have mastered the material.\n",
    "\n",
    "\n",
    "## Data Preprocessing\n",
    "* The data is provided in a CSV (text) format with semicolon and comma seperated list with **one line per event**. We need to convert this into an appropiate format for our neural networks. \n",
    "* Since the number of particles per event is variable you will need to **truncate** and **mask** particles in the event. The following steps need to be perfomed on the SM (background) sample:\n",
    "     1. Create variables where you count the number of electrons, photons, muons, jets and bjets in the event (ignore charge) before any truncation.\n",
    "     2. Choose an appropiate number of particles to study per event (recommended: **8** particles are used in the paper)\n",
    "     3. Check the particles are sorted by energy (largest to smallest)\n",
    "     4. If the event has more than 8 particles choose the **8 particles** with **highest energy and truncate** the rest.\n",
    "     5. convert energy and momentum variables by logarithm (e.g., `log`) - this is to prioritise differences in energy **scale** over more minor differences. \n",
    "     6. If the event has less than 8 particles, create kinematic variables with 0 values for the missing particles.\n",
    "* The final set of training variables should look something like this (the exact format is up to you)\n",
    "    |N ele| N muon| N jets| N bjets| N photons| log(MET)| METphi| log(E1)| log(pt1)| eta1| phi1| ... | phi8|\n",
    "    |-|-|-|-|-|-|-|-|-|-|-|-|-|\n",
    "    \n",
    "    7. After the dataset is ready, use `MinMaxScalar` or similar to standardise the training variables over the SM dataset\n",
    "* After the SM dataset has been processed use the same processing on the BSM (signal samples). Use the same standardisation functions as on the SM dataset, *Do not recalculate the standardisation*.\n",
    "* Keep associated metatata (`event ID; process ID; event weight;`) though this does not need processing. \n",
    "* Randomise and split the SM (background) dataset into training and testing datasets (the BSM samples don't need to be split (*Why?*))\n",
    "* *Hint*: It is suggested that you write a class or function for the preprocessing which takes a csv path as input and provides the processed dataset. After you have done the data processing its suggested you save the datasets so as to not have to recalculate them again if the kernel is restarted. \n",
    "\n",
    "## Training\n",
    "* Design an appropiate algorithm which reconstrucuts the input variables after going though a laten space. Choose an appropiate cost function.\n",
    "    * The suggested method for ease of implementation is the `AutoEncoder`\n",
    "    * However, if you consider learning about or trying something else, as described in the paper, you should feel welcome to try `VAEs`, `ConvAEs`, `ConvVAEs`, etc. Don't feel you **have** to create an `AE`.\n",
    "\n",
    "* Explore different architectures for the model, and explain in detail your choice of model, and the final parameters chosen.\n",
    "* It is suggested to create a class or function around your algorithm which allows you to easily tweek hyperparameters of the model (depth, number of nodes, number of laten variables, activation functions, regularisation, etc)\n",
    "* Train the model over several parameters to find the best algorithm. Document the process throught and discuss your choices. Keep track of validation performance. Save the models the best points. \n",
    "* Explore the results and document your findings. Ask as many questions about your model as you can, and document your findings. Does the model generalise well to data it hasn't seen?\n",
    "\n",
    "## Evaluation\n",
    "In the evaluation explore different datasets an try answer as many questions about the performance as possible. \n",
    "* Evaluate the performance of the `AE` on BSM dataset. Which models are more or less similar to the SM?\n",
    "* Explore the anomaly score as a handle on finding new physics. Consider scanning over different anomaly scores and calculating the signal and background efficiencies at each point (plot this for different BSM models). How might you choose a value which flags up a non-SM event? \n",
    "* Explore SM events. Which look more anomolous than others? Are there any particular features which are responsible, e.g. particle counts, MET ranges, etc.? \n",
    "* Discuss any limitations your algorithm has. How might you update and improve your model in future? Discuss any issues you had, or things you would have liked to try given more time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fe3871b-a380-429f-b27f-cbf91d380b03",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission\n",
    "\n",
    "\n",
    "To complete this project, you should **Submit your Jupyter notebook** as a \"report.\" See the comments below on documentation,\n",
    "\n",
    "**You should submit by Friday 9th Feb 2024 at 10AM**\n",
    "\n",
    "\n",
    "For all task we're not looking for exceptional model performace and high scores (although those are nice too), **we're mostly concerned with _best practices:_** If you are careful and deliberate in your work, and show us that you can use the tools introduced in the course so far, we're happy!\n",
    "\n",
    "Training all of these models in sequence takes a very long time so **don't spend hours on training hundreds of epochs.** Be conservative on epoch numbers (30 is normally more than enough) and use appropiate techniques like EarlyStopping to speed things up. Once you land on a good model you can allow for longer training times if performance can still improve.\n",
    "\n",
    "\n",
    "## Documentation: Annotation and Commentary\n",
    "\n",
    "It is important that __all__ code is annotated and that you provide brief commentary __at each step__ to explain your approach. We expect well-documented jupyter notebooks, not an unordered collection of code snippets. You can also include any failed approaches if you provide reasonable explanation. \n",
    "\n",
    "Unlike weekly checkpoints where you were being guided towards the *''correct''* answer, this project is by design more open ended. It is, therefore, necessary to give some justification for choosing one method over another.\n",
    "Explain *why* you chose a given approach and *discuss* the results. You can also include any failed approaches if you provide reasonable explanation; we care more about you making an effort and showing that you understand the core concepts.\n",
    "\n",
    "This is not in the form of a written report so do not provide pages of background material. Only provide a brief explanation for each step. Aim to clearly present your work so that the markers can easily follow your reasoning and can reproduce each of your steps through your analysis. Aim to convince us that you have understood the material covered in the course.\n",
    "\n",
    "To add commentary above (or below) a code snippet create a new cell and add your text in markdown format. __Do not__ add commentary or significant text as a code comment in the same cell as the code.\n",
    "(Code comments are still helpful!)\n",
    "\n",
    "__20\\% of the mark for each exercise is allocated to coding style and clarity of comments and approach.__\n",
    "\n",
    "## Submission Steps\n",
    "\n",
    "It is important your code is fully functional before it is submitted or this will affect your final mark. \n",
    "\n",
    "When you are ready to submit your report perform the following steps: \n",
    "\n",
    " -  In Jupyter run `Kernel` -> `Restart Kernel ` and ` Clear All Outputs `\n",
    " -  Then `Kernel` -> `Restart & Run All ` to ensure that all your analysis  is reproducible and all output can be regenerated\n",
    " -  Save the notebook, and close Jupyter\n",
    " -  **Change the filename to contain Name_Surname**\n",
    " -  Tar and zip your project folder if you have multiple files in a working directory. You are free to include any supporting code. Make sure this belongs in the project folder and is referenced correctly in your notebook. Do __not__ include any of the input data.\n",
    " -  Submit this file or zipped folder through Learn. In case of problems or if your compressed project folder exceeds 20 MB (first make sure you are not including any CSV files, then) email your submission to Kieran (the course administrator) at the Teaching Office and me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34764f35",
   "metadata": {},
   "source": [
    "# Happy Anomaly Hunting\n",
    "---\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Data Scientist (n.): Person who is better at statistics than any software engineer and better at software engineering than any statistician.</p>&mdash; Josh Wills (@josh_wills) <a href=\"https://twitter.com/josh_wills/status/198093512149958656?ref_src=twsrc%5Etfw\">May 3, 2012</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> \n",
    "\n",
    "---\n",
    "\n",
    "Your code follows...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca51dc7",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "This notebook is currently being developed in a docker container as it is the only way to get tensorflow working with my GPU and as such does not have the DAML enviroment installed. As a result this notebook will run teminal commands to install the required dependencies (currently all depedencies are met by the DAML enviroment). If you asre uncomfortable with this or the command creates conflics with an existing enviroment, you can force the notebook to skip the terminal commands by setting the ```run_cli``` option to ```False``` in the config.yaml file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d50968f",
   "metadata": {},
   "source": [
    "# Download Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947bc7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_911424/1456525576.py:12: UserWarning: Skipping dependency instalation. This may cause dependency issues further down in the notebook\n",
      "  warnings.warn(\"Skipping dependency instalation. This may cause dependency issues further down in the notebook\")\n"
     ]
    }
   ],
   "source": [
    "import os, sys, yaml, warnings, logging \n",
    "\n",
    "# Store sting with cwd\n",
    "CWD = os.getcwd() + \"/\"\n",
    "\n",
    "# Import config settings as dictionary \n",
    "with open(CWD + \"config.yaml\", \"r\") as file:\n",
    "    CFG = yaml.safe_load(file)\n",
    "\n",
    "# Install dependencies\n",
    "if not CFG[\"jupyter\"][\"run_cli\"] or not os.path.isfile(CWD + \"requirements.txt\"):\n",
    "    warnings.warn(\"Skipping dependency instalation. This may cause dependency issues further down in the notebook\")\n",
    "else:\n",
    "    os.system(f\"pip install --upgrade --ignore-installed -r {CWD + 'requirements.txt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97dcdf5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as snc\n",
    "# from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import custom classed and functions\n",
    "from src.core import Dataclass, DataLoader, TorchTrainer\n",
    "\n",
    "# Turn off divide by zero numpy error (we alreadsy take NaN's into account)\n",
    "np.seterr(divide=\"ignore\", invalid=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7362f4",
   "metadata": {},
   "source": [
    "# **Creating Dataloader**\n",
    "\n",
    "Currenly the datasets are stored in csv files with a non-uniform number of columns for each event. As such, we will have to approch the task of importing the data in a non-trivial way. We will create a dataloader class which will be taked with taking a csv path and return a well formatted numpy array which can be used lated down the line for training. \n",
    "\n",
    "This class is called ```DataLoader``` and can be found in the ```src.core``` directory under the name ```DataLoader.py```. The class will take the following steps to load in a csv.\n",
    "\n",
    "### **If we ask ```DataLoader``` to read raw csv:**\n",
    "\n",
    "- We will first parse the raw csv dividing the strings in each line according to the outer ```;``` delimiter. We will call the 1st 5 columns event metadata and stroree them in a pandas dataframe. the remaining string contaings the kinematics for each particle in the event. We will call this string the ```kinematicString``` and store it in a seperate column in the dataframe.\n",
    "\n",
    "We now have a dataset with the following structure:\n",
    "| event_id | process_id | event_weight | met | met_phi | kinematic_string | \n",
    "| :------- | :--------: | :----------: | :-: | :-----: | ---------------: |\n",
    "\n",
    "- We will now parse the kinematicString which is delimited with ```;``` per particle and with ```,``` per particle's kinematic parameter. Once parsed, we will arrange the particles per event in order of decending energy and keep the ```num_max_particles``` most energetic particles in an event. If the event has less than ```num_max_particles```, we will pad the remaining particle kinematic parameters with **0.0**. We will also remove event metadata which we do not want from the dataframe\n",
    "\n",
    "We now have a dataset with the following structure:\n",
    "| met | met_phi | object1 | energy1 | trans_momentum1 | pseudo_rapidity1 | phi1 | ... | phi_n |\n",
    "| :-- | :-----: | :-----: | :-----: | :-------------: | :--------------: | :--: | :-: | ----: |\n",
    "\n",
    "- We now take the logarithm of all the columns containing energy of momentum data dor each event.\n",
    "\n",
    "We now have a dataset with the following structure:\n",
    "| log(met) | met_phi | object1 | log(energy1) | log(trans_momentum1) | pseudo_rapidity1 | phi1 | ... | phi_n |\n",
    "| :------- | :-----: | :-----: | :----------: | :------------------: | :--------------: | :--: | :-: | ----: |\n",
    "\n",
    "- Lastly we will compute the number of particles what appear in each event and remove the object columns from the dataframe. We then save the dataframe as a csv if needed. The datareame or a numpy array of the dataframe's values can then be obtrained from the class \n",
    "\n",
    "We now have a dataset with the following structure:\n",
    "| n_ele | n_muon | n_jets | n_bjets | n_photons | log(met) | met_phi | log(energy1) | log(trans_momentum1) | pseudo_rapidity1 | phi1 | ... | phi_n |\n",
    "| ----: | :----: | :----: | :-----: | :-------: | :------: | :-----: | :----------: | :------------------: | :--------------: | :--: | :-: | ----: |\n",
    "\n",
    "### **If we ask ```DataLoader``` to read processed csv:**\n",
    "\n",
    "- The dataloader will read a processed csv which pas produced and saved previously as a pandas dataframe. The dataframe or a numpy array of the values can then be returned by the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46965288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader instance to load in background SM dataset\n",
    "background_dataloader = DataLoader(\n",
    "    filename = CFG[\"dataloader\"][\"raw_datasets\"][\"background\"],\n",
    "    cwd = CWD,\n",
    "    numParticles = 8,\n",
    "    saveProcessed = True,\n",
    "    loadProcessed = True, \n",
    ")\n",
    "# Return a numpy array containing the processed dataset  \n",
    "background_dataset = background_dataloader.getDataset(asNumpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c05cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(340267, 39)\n"
     ]
    }
   ],
   "source": [
    "print(background_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f54396e",
   "metadata": {},
   "source": [
    "# **Data Transformations**\n",
    "\n",
    "Now that we have written a class capable of reqading a csv file and returning a processed and uniform numpy array,. we define function to transform data from a raw numpy array to TF datasets which we can use for batch training.\n",
    "\n",
    "The script containing all the data transformation functions can be found in the ```scr.utils``` directory in the ```utility_functions.py``` script.\n",
    "\n",
    "The following steps must be taken:\n",
    "- Scale dataset from range (-1, 1). This is done s.t the neural network can more easily compute weights\n",
    "- Shuffle and split the numpy array into a test and train dataset using ```sklearn.model_selection.train_test_split```\n",
    "- Convert to tf.Dataset class s.t we can sample the datasets in batches using ```tf.data.Dataset.from_tensor_slices```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86afed0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([340267, 39])\n"
     ]
    }
   ],
   "source": [
    "# Apply reansfromations and return the tf dataset iterables \n",
    "sm_dataclass = Dataclass(CFG[\"dataloader\"][\"raw_datasets\"][\"background\"], CWD, 8, loadProcessed=True, saveProcessed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6d49dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39])\n"
     ]
    }
   ],
   "source": [
    "print(sm_dataclass[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef131a3",
   "metadata": {},
   "source": [
    "# **VAE Model**\n",
    "\n",
    "We now create the VAE model which will be used to trained and discriminate anomalies. We will first attempt to use two VAE models and compare their performance. One will be a regular Fully-Connected Dense VAE which will take a 1D vector as an input. The Second VAE will be a Convolutional VAE which will take a 2D image-like input.\n",
    "\n",
    "Both networks can be found in the ```src.models``` directory.\n",
    "\n",
    "For the fully-connected VAE, it's architecture is:\n",
    "\n",
    "- Encoder: \n",
    "    - Dense Layer (outDims = 32) + LeakyReLU\n",
    "    - Dense Layer (outDims = 16) + LeakyReLU\n",
    "    - Dense Layer (outDims = 10)\n",
    "\n",
    "- Decoder:\n",
    "    - Dense Layer (outDims = 16) + LeakyReLU\n",
    "    - Dense Layer (outDims = 32) + LeakyReLU\n",
    "    - Dense Layer (outDims = inputDims) + Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429cf1a",
   "metadata": {},
   "source": [
    "# **Constructing trainer class**\n",
    "\n",
    "Now that we have network classes and a dataset, we can build a class which can be used to train and validate our models. The class will take as an input the configuration dictionary s.t we can modify training hyperparameters from the config yaml file.\n",
    "\n",
    "The trainier class can be fount in the ```src.core``` directory in the ```trainer.py``` file.\n",
    "\n",
    "In order to run the training process, we call the ```runBatchProcess()``` method of the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb9a24f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### EPOCH 1 #####\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m TorchTrainer(CFG, sm_dataclass)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunBatchProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/DAML/report-2/src/core/torchTrainer.py:193\u001b[0m, in \u001b[0;36mTorchTrainer.runBatchProcess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Compute iteration metrics\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m     runningTrainLoss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainStep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Compute overall training loss\u001b[39;00m\n\u001b[1;32m    196\u001b[0m trainLoss \u001b[38;5;241m=\u001b[39m runningTrainLoss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainData)\n",
      "File \u001b[0;32m~/Github/DAML/report-2/src/core/torchTrainer.py:111\u001b[0m, in \u001b[0;36mTorchTrainer.trainStep\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    109\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Compute loss function\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlossFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Preform backwards step\u001b[39;00m\n\u001b[1;32m    114\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Github/DAML/report-2/src/core/torchTrainer.py:95\u001b[0m, in \u001b[0;36mTorchTrainer.denseVAELoss\u001b[0;34m(self, xTruth, xGenerated, mean, logvar)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdenseVAELoss\u001b[39m(\u001b[38;5;28mself\u001b[39m, xTruth, xGenerated, mean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, logvar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    Loss function consists of (1-beta)*MSE + beta*KL \u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     l1Loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxTruth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxGenerated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# klLoss  = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# return (1-self.beta)*mseLoss + self.beta*klLoss\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m l1Loss\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\n",
      "File \u001b[0;32m~/.local/conda_env/torch/lib/python3.11/site-packages/torch/nn/functional.py:3328\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[1;32m   3325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3326\u001b[0m         mse_loss, (\u001b[38;5;28minput\u001b[39m, target), \u001b[38;5;28minput\u001b[39m, target, size_average\u001b[38;5;241m=\u001b[39msize_average, reduce\u001b[38;5;241m=\u001b[39mreduce, reduction\u001b[38;5;241m=\u001b[39mreduction\n\u001b[1;32m   3327\u001b[0m     )\n\u001b[0;32m-> 3328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3329\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3330\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3333\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   3334\u001b[0m     )\n\u001b[1;32m   3335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "trainer = TorchTrainer(CFG, sm_dataclass)\n",
    "trainer.runBatchProcess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c29c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoss = trainer.metrics[\"train\"][\"loss\"][1:]\n",
    "valLoss = trainer.metrics[\"val\"][\"loss\"][1:]\n",
    "\n",
    "plt.plot(trainLoss)\n",
    "plt.plot(valLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1ab833",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b620bb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_dataloader =  DataLoader(\n",
    "    filename = CFG[\"dataloader\"][\"raw_datasets\"][\"chargino_chargino\"][0],\n",
    "    cwd = CWD,\n",
    "    numParticles = 8,\n",
    "    saveProcessed = True,\n",
    "    loadProcessed = False,\n",
    ")\n",
    "bsm_dataset = bsm_dataloader.getDataset(asNumpy=True)\n",
    "bsm_train_data, bsm_val_data = apply_data_transformations(bsm_dataset, batch_size=CFG[\"dataloader\"][\"batch_size\"], test_size=0.95,)\n",
    "trained_network = trainer.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee79649",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_score = []\n",
    "for input in iter(bsm_val_data):\n",
    "    output = trained_network(input)\n",
    "\n",
    "\n",
    "    # Compute loss function\n",
    "    loss = (tf.keras.metrics.mean_squared_error(input, output)*100).numpy()\n",
    "    bsm_score.extend(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4363b8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_score = []\n",
    "for idx, input in enumerate(iter(val_data)):\n",
    "    output = trained_network(input)\n",
    "\n",
    "    # Compute loss function\n",
    "    loss = (tf.keras.metrics.mean_squared_error(input, output)*100).numpy()\n",
    "    sm_score.extend(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b9312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(bsm_score, alpha=0.5, bins=50, log=True)\n",
    "plt.hist(sm_score,  alpha=0.5, bins=50, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb8110c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

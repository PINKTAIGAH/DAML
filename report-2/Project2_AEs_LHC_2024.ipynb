{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c322e2ea",
   "metadata": {},
   "source": [
    "# Project2: Anomaly Detection for Exotic Event Identification at the Large Hadron Collider \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225d014c",
   "metadata": {},
   "source": [
    "## Brief Introduction to the Standard Model and Large Hadron Collider\n",
    "\n",
    "\n",
    "The Standard model (SM) of Particle Physics is the most complete model physicists have for understanding the interactions of the fundamental particles in the universe. The elementary particles of the SM are shown in Fig.1.\n",
    "\n",
    "---\n",
    "<figure>\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Standard_Model_of_Elementary_Particles.svg/627px-Standard_Model_of_Elementary_Particles.svg.png\" alt=\"SM\" style=\"width: 600px;\"/>\n",
    "    <figcaption>Fig.1 - Elementary particles of the Standard Model.</figcaption>\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "It is comprised of matter particles (**fermions**):\n",
    "- **leptons**\n",
    "    - electrons\n",
    "    - muon\n",
    "    - tau\n",
    "    - and respective neutrinos\n",
    "- **quarks** which are the building blocks of protons\n",
    "\n",
    "as well as force carrier particles (**bosons**):\n",
    "- photon and W/Z bosons (electroweak force)\n",
    "- gluons (strong force)\n",
    "\n",
    "and the Higgs boson which is attributed to the mechanism which gives particles their mass.\n",
    "\n",
    "\n",
    "Though the SM has experimentally stood the test of time, many outstanding questions about the universe and the model itself remain, and scientist continue to probe for inconsistencies in the SM in order to find new physics. More exotic models such as **Supersymmetry (SUSY)** predic mirror particles which may exist and have alluded detection thus far. \n",
    "\n",
    "---\n",
    "\n",
    "The **Large Hadron Collider** (LHC) is a particle smasher capable of colliding protons at a centre of mass energy of 14 TeV.\n",
    "**ATLAS** is general purpouse particle detectors tasked with recording the remnants of proton collisions at the collicion point. The main purpouse of this experiment is to test the SM rigorously, and ATLAS was one of two expeririments (ATLAS+CMS) responsible for the discovery of the **Higgs boson in 2012**. \n",
    "\n",
    "Find an animation of how particles are reconstructed within a slice of the ATLAS detector here: https://videos.cern.ch/record/2770812. Electrons, muons, photons, quark jets, etc, will interact with different layers of the detector in different ways, making it possible to design algorithms which distinguish reconstructed particles, measure their trajectories, charge and energy, and identify them as particular types.\n",
    "\n",
    "Figure 2 shows an event display from a data event in ATLAS in which 2 muons (red), 2 electrons (green), and 1 quark-jet (purple cone) are found. This event is a candidate to a Higgs boson decaying to four leptons with an associated jet: $$H (+j)\\rightarrow 2\\mu 2e (+j)$$ \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://twiki.cern.ch/twiki/pub/AtlasPublic/EventDisplayRun2Physics/JiveXML_327636_1535020856-RZ-LegoPlot-EventInfo-2017-10-18-19-01-24.png\" alt=\"Higgs to leptons\" style=\"width: 600px;\"/>\n",
    "    <figcaption>Fig.2 - Event display of a Higgs candidate decaying to two muons and two electrons.</figcaption>\n",
    "</figure>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Particles are shown transversing the detector material. The 3D histogram show \n",
    "* the azimuth $\\phi$ ( angle around the beam, 0 is up)\n",
    "* pseudo-rapidity $\\eta$ (trajectory along the beam) positions of the particle directions with respect to the interaction point.\n",
    "* The total energy measured for the particle is denoted by $E$,\n",
    "* the transverse momentum ($p_T$) deposited by the particle in giga-electronvolts (GeV) are shown by the hight of the histograms.\n",
    "\n",
    "A particle kinematics can then be described by a four-vector  $$\\bar{p} = (E,p_T,\\eta,\\phi)$$\n",
    "\n",
    "An additional importan quantity is the missing energy in the transverse plane (MET). This is calculated by taking the negative sum of the transverse momentum of all particles in the event.\n",
    "$$\\mathrm{MET} = -\\sum p_T$$\n",
    "\n",
    "With perfect detector performance the MET will sum to 0 if all outgoing particles are observed by the detector. Neutrinos cannot be measured by the detector and hence their precense produces non-zero MET."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655ada0c",
   "metadata": {},
   "source": [
    "## Anomally detection dataset\n",
    "\n",
    "For the anomally detection project we will use the dataset discussed in this publication: <p><a href=\"https://arxiv.org/pdf/2105.14027.pdf\" title=\"Anomalies\">The Dark Machines Anomaly Score Challenge:\n",
    "Benchmark Data and Model Independent Event\n",
    "Classification for the Large Hadron Collider</a></p>\n",
    "\n",
    "Familiarise yourself with the paper, in particular from sections 2.1 to 4.4.\n",
    "\n",
    "---\n",
    "\n",
    "The dataset contains a collection of simulated proton-proton collisions in a general particle physics detector (such as ATLAS). We will use a dataset containing `340 000` SM events (referred to as channel 2b in the paper) which have at least 2 electrons/muons in the event with $p_T>15$ GeV. \n",
    "\n",
    "**The events can be found in `background_chan2b_7.8.csv`**\n",
    "\n",
    "\n",
    "You can see all the SM processes that are simulated in Table 2 of the paper, \n",
    "\n",
    "    e.g., an event with a process ID of `w_jets` is a simulated event of two protons producing a lepton and neutrino and at least two jets.\n",
    "    \n",
    "$$pp\\rightarrow \\ell\\nu(+2j)$$\n",
    "\n",
    "---\n",
    "\n",
    "The datasets are collected as CSV files where each line represents a single event, with the current format:\n",
    "\n",
    "`event ID; process ID; event weight; MET; METphi; obj1, E1, pt1, eta1, phi1; obj2, E2, pt2, eta2, phi2; ...`\n",
    "See Section 2.2 for a description of the dataset.\n",
    "Variables are split by a semicolon `\";\"`\n",
    "- `event ID`: an identifier for the event number in the simulation\n",
    "- `process ID`: an identifier for the event simulation type\n",
    "- `event weight`: the weight associated to the simulated event (how important that event is)\n",
    "- `MET`: the missing transverse energy\n",
    "- `METphi`: the azimuth angle (direction) of the MET\n",
    "\n",
    "the a list of objects (particles) whose variables are split by commas `\",\"` in the following orger:\n",
    "- `obj`: the object type,\n",
    "\n",
    "    |Key|Particle|\n",
    "    |---|---|\n",
    "    |j|jet|\n",
    "    |b|b-jet|\n",
    "    |e-|electron|\n",
    "    |e+|positron|\n",
    "    |m-|muon|\n",
    "    |m+|muon+|\n",
    "    |g|photon|\n",
    "    \n",
    "    *see Table 1 of the paper*\n",
    "- `E`: the total measured particle energy in MeV, [0,inf]\n",
    "- `pt`: the transverse mementum in MeV, [0,inf]\n",
    "- `eta`: pseudo-rapidity, [-inf,inf]\n",
    "- `phi`: azimuth angle, radians [-3.14,3.14]\n",
    "\n",
    "---\n",
    "\n",
    "In addition to the SM events we are also provided simulated events from `Beyond Standard Model` (BSM) exotic physics models. They are summarised here:\n",
    "\n",
    "|Model | File Name | \n",
    "|---|---|\n",
    "|**SUSY chargino-chargino process**||\n",
    "||`chacha_cha300_neut140_chan2b.csv`|\n",
    "||`chacha_cha400_neut60_chan2b.csv`|\n",
    "||`chacha_cha600_neut200_chan2b.csv`|\n",
    "|**SUSY chargino-neutralino processes**||\n",
    "||`chaneut_cha200_neut50_chan2b.csv`|\n",
    "||`chaneut_cha250_neut150_chan2b.csv`|\n",
    "|**$Z'$ decay to leptons**||\n",
    "||`pp23mt_50_chan2b.csv`|\n",
    "||`pp24mt_50_chan2b.csv`|\n",
    "|**Gluino and RPV SUSY**||\n",
    "||`gluino_1000.0_neutralino_1.0_chan2b.csv`||\n",
    "||`stlp_st1000_chan2b.csv`||\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686809e6",
   "metadata": {},
   "source": [
    "## Project description\n",
    "*Responsible:* Robert Currie (<rob.currie@ed.ac.uk>)\n",
    "\n",
    "### Overview\n",
    "The task is to design an anomaly detection algorithm which is trained on the SM dataset and which can be used to flag up interesting (exotic) events from the BSM physics models.\n",
    "\n",
    "You will do this by designing a robust `AutoEncoder` which is trained on the event level variables `MET; METphi` and the kinematics of the particle level objects. The `AutoEncoder` needs to duplicate the input as output effectively while going through a laten space (bottleneck). \n",
    "\n",
    "You will then need to evaluate and discuss the performance of your `AutoEncoder` on the exotic models listed above, and come up with an appropiate metric to identify events from non SM physics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d3beb",
   "metadata": {},
   "source": [
    "# **Breakdown**\n",
    "\n",
    "In the project report you will be assessed in the following way.\n",
    "\n",
    "1. **Data exploration and preprocessing (20%):** Inspect the datasets; visualise the data (e.g. tables, plots, etc) in an appropriate way; study the composition of the dataset; perform any necessary preprocessing.\n",
    "2. **Model selection (30%):** Choose a promissing approach; construct the machine learning model; optimise the relevant hyperparameters; train your chosen model.\n",
    "3. **Performance evaluation (30%):** Evaluate the model in a way that gauges its ability to generalise to unseen data; compare to other approaches; identify the best approach. \n",
    "4. **Discussion, style throughout (20%):** Discuss the reasoning or intuition behind your choices; the results you obtain through your studies; the relative merits of the methods you have developed, _etc._ Similarly, make sure that you write efficient code, document your work, clearly convey your results, and convince us that you have mastered the material.\n",
    "\n",
    "\n",
    "## Data Preprocessing\n",
    "* The data is provided in a CSV (text) format with semicolon and comma seperated list with **one line per event**. We need to convert this into an appropiate format for our neural networks. \n",
    "* Since the number of particles per event is variable you will need to **truncate** and **mask** particles in the event. The following steps need to be perfomed on the SM (background) sample:\n",
    "     1. Create variables where you count the number of electrons, photons, muons, jets and bjets in the event (ignore charge) before any truncation.\n",
    "     2. Choose an appropiate number of particles to study per event (recommended: **8** particles are used in the paper)\n",
    "     3. Check the particles are sorted by energy (largest to smallest)\n",
    "     4. If the event has more than 8 particles choose the **8 particles** with **highest energy and truncate** the rest.\n",
    "     5. convert energy and momentum variables by logarithm (e.g., `log`) - this is to prioritise differences in energy **scale** over more minor differences. \n",
    "     6. If the event has less than 8 particles, create kinematic variables with 0 values for the missing particles.\n",
    "* The final set of training variables should look something like this (the exact format is up to you)\n",
    "    |N ele| N muon| N jets| N bjets| N photons| log(MET)| METphi| log(E1)| log(pt1)| eta1| phi1| ... | phi8|\n",
    "    |-|-|-|-|-|-|-|-|-|-|-|-|-|\n",
    "    \n",
    "    7. After the dataset is ready, use `MinMaxScalar` or similar to standardise the training variables over the SM dataset\n",
    "* After the SM dataset has been processed use the same processing on the BSM (signal samples). Use the same standardisation functions as on the SM dataset, *Do not recalculate the standardisation*.\n",
    "* Keep associated metatata (`event ID; process ID; event weight;`) though this does not need processing. \n",
    "* Randomise and split the SM (background) dataset into training and testing datasets (the BSM samples don't need to be split (*Why?*))\n",
    "* *Hint*: It is suggested that you write a class or function for the preprocessing which takes a csv path as input and provides the processed dataset. After you have done the data processing its suggested you save the datasets so as to not have to recalculate them again if the kernel is restarted. \n",
    "\n",
    "## Training\n",
    "* Design an appropiate algorithm which reconstrucuts the input variables after going though a laten space. Choose an appropiate cost function.\n",
    "    * The suggested method for ease of implementation is the `AutoEncoder`\n",
    "    * However, if you consider learning about or trying something else, as described in the paper, you should feel welcome to try `VAEs`, `ConvAEs`, `ConvVAEs`, etc. Don't feel you **have** to create an `AE`.\n",
    "\n",
    "* Explore different architectures for the model, and explain in detail your choice of model, and the final parameters chosen.\n",
    "* It is suggested to create a class or function around your algorithm which allows you to easily tweek hyperparameters of the model (depth, number of nodes, number of laten variables, activation functions, regularisation, etc)\n",
    "* Train the model over several parameters to find the best algorithm. Document the process throught and discuss your choices. Keep track of validation performance. Save the models the best points. \n",
    "* Explore the results and document your findings. Ask as many questions about your model as you can, and document your findings. Does the model generalise well to data it hasn't seen?\n",
    "\n",
    "## Evaluation\n",
    "In the evaluation explore different datasets an try answer as many questions about the performance as possible. \n",
    "* Evaluate the performance of the `AE` on BSM dataset. Which models are more or less similar to the SM?\n",
    "* Explore the anomaly score as a handle on finding new physics. Consider scanning over different anomaly scores and calculating the signal and background efficiencies at each point (plot this for different BSM models). How might you choose a value which flags up a non-SM event? \n",
    "* Explore SM events. Which look more anomolous than others? Are there any particular features which are responsible, e.g. particle counts, MET ranges, etc.? \n",
    "* Discuss any limitations your algorithm has. How might you update and improve your model in future? Discuss any issues you had, or things you would have liked to try given more time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fe3871b-a380-429f-b27f-cbf91d380b03",
   "metadata": {},
   "source": [
    "---\n",
    "## Submission\n",
    "\n",
    "\n",
    "To complete this project, you should **Submit your Jupyter notebook** as a \"report.\" See the comments below on documentation,\n",
    "\n",
    "**You should submit by Friday 9th Feb 2024 at 10AM**\n",
    "\n",
    "\n",
    "For all task we're not looking for exceptional model performace and high scores (although those are nice too), **we're mostly concerned with _best practices:_** If you are careful and deliberate in your work, and show us that you can use the tools introduced in the course so far, we're happy!\n",
    "\n",
    "Training all of these models in sequence takes a very long time so **don't spend hours on training hundreds of epochs.** Be conservative on epoch numbers (30 is normally more than enough) and use appropiate techniques like EarlyStopping to speed things up. Once you land on a good model you can allow for longer training times if performance can still improve.\n",
    "\n",
    "\n",
    "## Documentation: Annotation and Commentary\n",
    "\n",
    "It is important that __all__ code is annotated and that you provide brief commentary __at each step__ to explain your approach. We expect well-documented jupyter notebooks, not an unordered collection of code snippets. You can also include any failed approaches if you provide reasonable explanation. \n",
    "\n",
    "Unlike weekly checkpoints where you were being guided towards the *''correct''* answer, this project is by design more open ended. It is, therefore, necessary to give some justification for choosing one method over another.\n",
    "Explain *why* you chose a given approach and *discuss* the results. You can also include any failed approaches if you provide reasonable explanation; we care more about you making an effort and showing that you understand the core concepts.\n",
    "\n",
    "This is not in the form of a written report so do not provide pages of background material. Only provide a brief explanation for each step. Aim to clearly present your work so that the markers can easily follow your reasoning and can reproduce each of your steps through your analysis. Aim to convince us that you have understood the material covered in the course.\n",
    "\n",
    "To add commentary above (or below) a code snippet create a new cell and add your text in markdown format. __Do not__ add commentary or significant text as a code comment in the same cell as the code.\n",
    "(Code comments are still helpful!)\n",
    "\n",
    "__20\\% of the mark for each exercise is allocated to coding style and clarity of comments and approach.__\n",
    "\n",
    "## Submission Steps\n",
    "\n",
    "It is important your code is fully functional before it is submitted or this will affect your final mark. \n",
    "\n",
    "When you are ready to submit your report perform the following steps: \n",
    "\n",
    " -  In Jupyter run `Kernel` -> `Restart Kernel ` and ` Clear All Outputs `\n",
    " -  Then `Kernel` -> `Restart & Run All ` to ensure that all your analysis  is reproducible and all output can be regenerated\n",
    " -  Save the notebook, and close Jupyter\n",
    " -  **Change the filename to contain Name_Surname**\n",
    " -  Tar and zip your project folder if you have multiple files in a working directory. You are free to include any supporting code. Make sure this belongs in the project folder and is referenced correctly in your notebook. Do __not__ include any of the input data.\n",
    " -  Submit this file or zipped folder through Learn. In case of problems or if your compressed project folder exceeds 20 MB (first make sure you are not including any CSV files, then) email your submission to Kieran (the course administrator) at the Teaching Office and me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34764f35",
   "metadata": {},
   "source": [
    "# Happy Anomaly Hunting\n",
    "---\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Data Scientist (n.): Person who is better at statistics than any software engineer and better at software engineering than any statistician.</p>&mdash; Josh Wills (@josh_wills) <a href=\"https://twitter.com/josh_wills/status/198093512149958656?ref_src=twsrc%5Etfw\">May 3, 2012</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> \n",
    "\n",
    "---\n",
    "\n",
    "Your code follows...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca51dc7",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "This notebook is currently being developed in a docker container as it is the only way to get tensorflow working with my GPU and as such does not have the DAML enviroment installed. As a result this notebook will run teminal commands to install the required dependencies (currently all depedencies are met by the DAML enviroment). If you asre uncomfortable with this or the command creates conflics with an existing enviroment, you can force the notebook to skip the terminal commands by setting the ```run_cli``` option to ```False``` in the config.yaml file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d50968f",
   "metadata": {},
   "source": [
    "# Download Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947bc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, yaml, warnings \n",
    "\n",
    "# Store sting with cwd\n",
    "CWD = os.getcwd() + \"/\"\n",
    "\n",
    "# Import config settings as dictionary \n",
    "with open(CWD + \"config.yaml\", \"r\") as file:\n",
    "    CFG = yaml.safe_load(file)\n",
    "\n",
    "# Install dependencies\n",
    "if not CFG[\"run_cli\"] or not os.path.isfile(CWD + \"requirements.txt\"):\n",
    "    warnings.warn(\"Skipping dependency instalation. This may cause dependency issues further down in the notebook\")\n",
    "else:\n",
    "    os.system(f\"pip install --upgrade --ignore-installed -r {CWD + 'requirements.txt'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dcdf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as snc\n",
    "# from tqdm import tqdm \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Turn off divide by zero numpy error (we alreadsy take NaN's into account)\n",
    "np.seterr(divide=\"ignore\", invalid=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8696fd",
   "metadata": {},
   "source": [
    "# Importing datasets\n",
    "\n",
    "Currenly the datasets are stored in csv files with a non-uniform number of columns for each event. As such, we will have to approch the task of imoporting the data in a non-trivial way. We will attempt to parse thorugh the csv by reading in each line as a string and seperate each string using the ```;``` charachter as our delimiter. Since we know that that the frist 5 columns are metadata, we can then create a pandas dataframe containing the first 5 columns and a sixth column with th ereamining string containing the particle kinematics in the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename, delimiter=\";\", init_keys=[\"event_id\", \"process_id\", \"event_weight\", \"met\", \"met_phi\",]):\n",
    "    \"\"\"\n",
    "    Return Dataframe containing event metadata and string containing all event kinematic information\n",
    "    \"\"\" \n",
    "    filepath = CWD + filename\n",
    "    # Define the name for the first five keys of the dataframe\n",
    "    dataset = {key: [] for key in init_keys}\n",
    "    kinematic_string_list = []\n",
    "    \n",
    "    if not os.path.isfile(CWD + filename):\n",
    "        raise Exception(f\"File {CWD + filename} not found.\")\n",
    "\n",
    "    # Parse individual lines of csv and return them as a numpy array of strings\n",
    "    dataset_event_strings = np.loadtxt(filepath, dtype=str)\n",
    "\n",
    "    # Split datapoint in the event of the array delimited by ';'\n",
    "    dataset_event_strings = np.char.split(dataset_event_strings, delimiter)\n",
    "\n",
    "    # Iterate over each event in array and start to build up dictionary containing data\n",
    "    for event in dataset_event_strings:\n",
    "        \n",
    "        # Join all particle kinematic data of event into string with no spacing inbetween elements\n",
    "        kinematic_string = \";\".join(event[len(init_keys):])\n",
    "        # Append kinematic string to list\n",
    "        kinematic_string_list.append(kinematic_string)\n",
    "\n",
    "        # Fill in first five keys of dataset\n",
    "        for idx, key in enumerate(dataset.keys()):\n",
    "            # Append kinematic string to last key of dictionary\n",
    "            if \"met\" in key:\n",
    "                dataset[key].append(float(event[idx]))\n",
    "            else:\n",
    "                dataset[key].append(event[idx])\n",
    "\n",
    "    # Convert dictionary to dataframe, add kinematioc strings as a column and return it\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    dataset[\"kinematics_string\"] = kinematic_string_list\n",
    "    return dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fea199f",
   "metadata": {},
   "source": [
    "# Preprocessing dataset\n",
    "\n",
    "So now we have a pandas dataframe which contains the following columns:\n",
    "\n",
    "| event_id | process_id | event_weight | met | met_phi | kinematic_string | \n",
    "| :------- | :--------: | :----------: | :-: | :-----: | ---------------: |\n",
    "\n",
    "where **kinematic_string** is a string containing the object type and particle kinematics for each particle in an event.\n",
    "\n",
    "We now look to preprocess the image, i.e: we look to massage the data into a format that will be easily readable s.t we can proccess the data easily further down the line. By the end of the preprocess stage we will have a dataframe with the following columns:\n",
    "\n",
    "| event_id | kinematic_string | n_ele | n_muon | n_jets | n_bjets | n_photons | n_tot_particles | object1 | energy1 | trans_momentum1 | pseudo_rapidity1 | phi1 | ... | phin |\n",
    "| :------- | :--------------: | :---: | :----: | :----: | :-----: | :-------: | :-------------: | :-----: | :-----: | :-------------: | :--------------: | :--: | :-: | ---: |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, n_particles=8, kinematic_params_keys=[\"object\", \"energy\", \"trans_momentum\", \"pseudo_rapidity\", \"phi\"],):\n",
    "    \"\"\"\n",
    "    Apply data preprocess to prepare data for anomaly detection\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a list containing the column name for the number of a specific partice and the particle's id in the kinematic sting\n",
    "    n_particle_keys = [\"n_ele\", \"n_muon\", \"n_jets\", \"n_bjets\", \"n_photons\"]\n",
    "    particle_object_types = [\"e\", \"m\", \"j\", \"b\", \"g\"]\n",
    "    drop_metadata_keys = [\"event_id\", \"process_id\", \"event_weight\",]\n",
    "\n",
    "    # Create series containg the kinematic strings of the dataset\n",
    "    kinematic_strings = dataset[\"kinematics_string\"]\n",
    "\n",
    "    # Split elements in kinematic string s.t each particle's kinematics is contained to one element\n",
    "    kinematic_strings_split = np.char.split(np.array(kinematic_strings, dtype=str), \";\")\n",
    "\n",
    "    # Find maximum number of particles in an event for the entire dataset\n",
    "    # maximum_n_particles = max(map(len, kinematic_strings_split))\n",
    "    maximum_n_particles = 14\n",
    "\n",
    "    # Create kinematic parameter keys for each partice (up to maximum_n_particles)\n",
    "    kinematic_params_keys = [[kinematic_params_key + str(idx+1) for kinematic_params_key in kinematic_params_keys] for idx in range(maximum_n_particles)]\n",
    "\n",
    "    # Flatten kinematic_params_keys\n",
    "    kinematic_params_keys = [x for xs in kinematic_params_keys for x in xs]\n",
    "\n",
    "    # Define empty array structure inside dictionary which is of size (max_n_particles * num_kinematic_params, max_n_particles) which we will build \n",
    "    particle_kinematics = {key : [] for key in kinematic_params_keys}\n",
    "\n",
    "    # Create string which will be used to fill padded elements \n",
    "    padding = ( \"0,\" * len(n_particle_keys) )[:-1]         # indexing is used at the end to get rid of last ',' char\n",
    "\n",
    "    # Pad out kinematic stirngs by adding additional particle kinematic elements padded by 0 to a total of n_tot_particle elements in array\n",
    "    # In order to convert to numpy array (rather than an akward array)\n",
    "    for event in kinematic_strings_split:\n",
    "\n",
    "        ##### Test if this is slower than building a new kinematic_stirng_splity_padded structure from scratch #####\n",
    "\n",
    "        # Due to way csv is set up the last element in this array is an empty string, so we remove it\n",
    "        event = event[:-1]   \n",
    "        # Compute how many elements we need to pad to achieve max_n_particles\n",
    "        n_padding_elements = maximum_n_particles - len(event)\n",
    "\n",
    "        # Pad element\n",
    "        for _ in range(n_padding_elements): event.append(padding)\n",
    "\n",
    "        # Split kinematic stirings further such that each elements corresponds to each indfividual kinematic parameter for each particle\n",
    "        kinematic_strings_split_all = np.char.split( np.array(event, dtype=str), \",\")\n",
    "        # Flatten kinematic_strings_split_all\n",
    "        kinematic_strings_split_all = np.array([x for xs in kinematic_strings_split_all for x in xs])\n",
    "\n",
    "        # Create array with only energies\n",
    "        particle_energies = np.array(kinematic_strings_split_all[1::5], dtype=float)\n",
    "        # Find indexes of energies in decending order\n",
    "        sorted_energy_idx = np.argsort(particle_energies)[::-1]\n",
    "        # Make arrey containing all indexes for sorted particles\n",
    "        sorted_kinematic_index = [list(range(idx*5, idx*5+5)) for idx in sorted_energy_idx]\n",
    "        # Flatten the list\n",
    "        sorted_kinematic_index = [x for xs in sorted_kinematic_index for x in xs]\n",
    "        # Rearrange kinematic_string_slit all in terms of energy\n",
    "        kinematic_strings_split_all = kinematic_strings_split_all[sorted_kinematic_index]\n",
    "\n",
    "        # Append individual kinematic parameter to dictionary\n",
    "        for idx, key in enumerate(kinematic_params_keys):\n",
    "            if \"object\" in key:\n",
    "                particle_kinematics[key].append(str(kinematic_strings_split_all[idx]))\n",
    "            else:\n",
    "                particle_kinematics[key].append(float(kinematic_strings_split_all[idx]))\n",
    "\n",
    "    # Append the existing dataset with the particle kinematics entries\n",
    "    ##### IS THERE A BETTER WAY OF DOING THIS? PROBABLY. BUT EVERY PANDAS FUNCTION I TRIED TO USE WOULD NOT WORK, SO HERE WE ARE BRUTE FORCING IT #####\n",
    "    for key in particle_kinematics:\n",
    "        if str(n_particles+1) in key:\n",
    "            break\n",
    "        dataset[key] = particle_kinematics[key]\n",
    "    \n",
    "    # Drop the kinematic string column of the dataframe as it is no longer needed\n",
    "    dataset = dataset.drop(columns=\"kinematics_string\")\n",
    "\n",
    "    # Drop any undesirable metadata which may be a part of the dataset\n",
    "    for key in drop_metadata_keys:\n",
    "        # Check if key is in dataset\n",
    "        if key in list(dataset.columns):\n",
    "            dataset = dataset.drop(columns=key)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be844778",
   "metadata": {},
   "source": [
    "# Process Dataset\n",
    "\n",
    "In this step we will first look to sort the particles in the dataframe by energy (largest to smallest), we will limit the number of particles in the dataframe to the maximum_paticles parmater defined in the config file.\n",
    "\n",
    "After this we will take the log of the energy and momentum variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dataset, maximum_particles=8,):\n",
    "    \"\"\"\n",
    "    Apply final steps of data processing before converting to tensor\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Make a list of all the energy keys and momentum in dataset\n",
    "    data_energy_keys = [energy_key for energy_key in list(dataset.columns) if \"energy\" in energy_key]\n",
    "    data_momentum_keys = [momentum_key for momentum_key in list(dataset.columns) if \"momentum\" in momentum_key]\n",
    "    data_met_keys = [met_key for met_key in list(dataset.columns) if \"met\" == met_key]\n",
    "    # print(data_met_keys)\n",
    "    \n",
    "    # Iterate over all momentum keys and take natural log\n",
    "    for key in data_momentum_keys:\n",
    "        # Create new object , Take log and replace -infs with 0\n",
    "        # This is done to work around pandas shenanigans\n",
    "        column = np.log(dataset[key])\n",
    "        column[np.isneginf(column)] = 0.0\n",
    "        dataset[key] = column\n",
    "\n",
    "    # Take log of any energy datapoint in dataset\n",
    "    for key in data_energy_keys:\n",
    "        # Create new object , Take log and replace -infs with 0\n",
    "        # This is done to work around pandas shenanigans\n",
    "        column = np.log(dataset[key])\n",
    "        column[np.isneginf(column)] = 0.0\n",
    "        dataset[key] = column\n",
    "\n",
    "    # Take log of any energy datapoint in dataset\n",
    "    for key in data_met_keys:\n",
    "        # Create new object , Take log and replace -infs with 0\n",
    "        # This is done to work around pandas shenanigans\n",
    "        column = np.log(dataset[key])\n",
    "        column[np.isneginf(column)] = 0.0\n",
    "        dataset[key] = column\n",
    "            \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(CFG[\"datasets\"][\"background\"], )\n",
    "dataset.head()\n",
    "dataset = preprocess_data(dataset)\n",
    "dataset = process_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2cc25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651176b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a626e85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced3219",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # Itewrate over all n_particle columns we want to add\n",
    "    # for object_type, n_particle_key in zip(particle_object_types, n_particle_keys):\n",
    "\n",
    "    #     # Compute the number of particles in an event and add a new column to dataset \n",
    "    #     dataset[n_particle_key] = kinematic_strings.str.count(object_type) \n",
    "\n",
    "    # # Add a column containig the total number of particles in event\n",
    "    # dataset[\"n_tot_particle\"] = dataset[n_particle_keys].sum(axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

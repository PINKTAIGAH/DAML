{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.integrate import quad\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import sys, os\n",
    "from iminuit import Minuit\n",
    "from scipy.stats import chi2\n",
    "from scipy.special import erfinv\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections.abc import Sequence\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **¡¡¡ README !!!**\n",
    "\n",
    "In this project the CSV files containing the signal and background events are not included.\n",
    "\n",
    "Thus notebook assumes that the directory listed on the variable ```DATAFILE_DIR``` is the directory which contains the four CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Defining notebook constants**\n",
    "\n",
    "Here we will define any global variables that will have to be used throughout the notebook and we don't plan on changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE_DIR = \"/home/giorgio/data/report_4\" + \"/\"\n",
    "# DATAFILE_DIR = \"/home/s1835083/Desktop/report_4_data/\"\n",
    "KEYS = {\n",
    "    \"features\" : [\"lep1_pt\", \"lep2_pt\", \"fatjet_pt\", \"fatjet_eta\", \"fatjet_D2\", \"Zll_mass\", \"Zll_pt\", \"MET\",],\n",
    "    \"features_plus_mass\" : [\"lep1_pt\", \"lep2_pt\", \"fatjet_pt\", \"fatjet_eta\", \"fatjet_D2\", \"Zll_mass\", \"Zll_pt\", \"MET\", \"reco_zv_mass\"],\n",
    "    \"targets\"  : [\"isSignal\"],\n",
    "    \"features_and_targets\" : [\"lep1_pt\", \"lep2_pt\", \"fatjet_pt\", \"fatjet_eta\", \"fatjet_D2\", \"Zll_mass\", \"Zll_pt\", \"MET\", \"isSignal\"],\n",
    "    \"weights\"  : [\"FullEventWeight\"],\n",
    "} \n",
    "SEED = 42\n",
    "N_BINS = 75\n",
    "SUBRANGE = (0.7e6, 1.5e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataloading**\n",
    "\n",
    "Before any analysis, we first need to load in the signal and backround datasets from the csv files.\n",
    "\n",
    "To do this, we create a funtion called  ```load_data``` which will return a dictironary containing a dataframe of each dataset represented by a CSV file in the ```DATAFILE_DIR``` directory. The key of the dictionary entries are the names of the CSV files. Hence the structurte of the dictionary will be:\n",
    "\n",
    "- Top\n",
    "- Diboson\n",
    "- ggH1000\n",
    "- Zjets\n",
    "\n",
    "The dictionary structure is used do that we can easily iterate over the dicitionary to apply functions to all four dataframes of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in data from csv\n",
    "print(os.listdir(DATAFILE_DIR))\n",
    "\n",
    "def load_data(data_directory):\n",
    "    \"\"\"\n",
    "    Function which will load in all 4 csv files as a pandas dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make list containg directory of all csv files\n",
    "    file_names = [DATAFILE_DIR + file_name for file_name in os.listdir(data_directory)]\n",
    "    \n",
    "    # Make list containing the key for each csv file\n",
    "    data_keys = [key.rsplit(\".\")[0] for key in os.listdir(data_directory)]\n",
    "\n",
    "    # Define dictionary which will contain each pandas dataframe\n",
    "    dataset_dictionary = {}\n",
    "\n",
    "    # Load in the CSV files by iterating over all files in list\n",
    "    for key, file_name in zip(data_keys, file_names):\n",
    "        dataset_dictionary[key] = pd.read_csv(file_name, sep=\",\", header=0, comment=\"#\", index_col=0)\n",
    "    return dataset_dictionary\n",
    "\n",
    "# Load in the datasets\n",
    "dataset_dictionary = load_data(DATAFILE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded in our datasets, we will do some basic exploration. First we will remove all NaNs from each of the dataframes s.t none of our entries contain a NaN. After this, we will print out a summary for each of the datasets so that we can get a basic overview of what it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nans(dataset_dictionary):\n",
    "    \"\"\"\n",
    "    Remove all nans in a dicionary containing multiple pandas dataframes\n",
    "    \"\"\"    \n",
    "\n",
    "    # iterate over all datasets\n",
    "    for key in dataset_dictionary:\n",
    "        # Remove entries with nans\n",
    "        dataset_dictionary[key].dropna(inplace=True)\n",
    "\n",
    "def print_summaries(dataset_dictionary):\n",
    "    \"\"\"\n",
    "    Print a summary of each dataframe in a dictionarty\n",
    "    \"\"\"\n",
    "\n",
    "    # iterate over all datasets\n",
    "    for key in dataset_dictionary:\n",
    "        # Remove entries with nans\n",
    "        print(f\"##### {key} #####\")\n",
    "        print(dataset_dictionary[key].info())\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "# Remove Nans for all dataframes\n",
    "remove_nans(dataset_dictionary)\n",
    "\n",
    "# Print summary of all datasets\n",
    "print_summaries(dataset_dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further analyse each of the datasets, we print out the first 6 rows of each of the dataframes to get better understanding of what they contain, what are the units and datatypes used to represent themm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dictionary[\"Top\"].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dictionary[\"Zjets\"].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dictionary[\"Diboson\"].head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dictionary[\"ggH1000\"].head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done our basic preprocessing and a cursory analysis of the entries on each of the dataframes, we want to go more in depth into the distributuions of each of the datasets. Given that we are going to have to apply cuts to some of the parameters in the dataset, it is in our interest to visualise the distributions s.t we can tell where our initial cuts are going to have to be.\n",
    "\n",
    "In order to do this, we will write a function which will plot the distributuions for each of the datasets in our dictionary for each of the kinematic parameters which we have labeled as features in the global constants. This will make it simpler to plot the distributions every time we are applying cuts so that we can see the effects of each of the cuts.\n",
    "\n",
    "After defining a function which will plot the distributuions, we plot them before any cuts.\n",
    "\n",
    "The parameters that are going to be plotted every time we call the funcction will be:\n",
    "\n",
    "- lep1_pt\n",
    "- lep2_pt\n",
    "- fatjet_pt\n",
    "- fatjet_eta\n",
    "- fatjet_D2\n",
    "- Zll_mass\n",
    "- Zll_pt\n",
    "- MET\n",
    "- reco_zv_mass\n",
    "\n",
    "It should be noted that the distributions being plotted include the Fuill event weights, hence we get the correct scaling of the singal events compared to the background events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kinematic_parameters(dataset_dictionary):\n",
    "    \"\"\"\n",
    "    Function which will plot the lkinematic parameters which will be used as inputs for neutral network\n",
    "    \"\"\"\n",
    "\n",
    "    # Define plotting parameters for each dataset\n",
    "    colors = [\"darkblue\", \"maroon\", \"darkgreen\", \"violet\"]\n",
    "    labels = dataset_dictionary.keys()\n",
    "    x_labels = [\"Momentum (MeV/c)\", \"Momentum (MeV/c)\",\"Momentum (MeV/c)\",\"Pseudorapidity\",r\"D_2\",r\"Mass (MeV/c^2)\",\"Momentum (MeV/c)\",\"Energy (MeV)\", r\"Mass (MeV/c^2)\"]\n",
    "\n",
    "    fig, ax = plt.subplots(3, 3, figsize=(10,10))\n",
    "\n",
    "    # Iterate over every axis in the subplot to pot a specific kinematic parameter\n",
    "    for idx, (parameter_key, axis) in enumerate(zip(KEYS[\"features_plus_mass\"], ax.flatten())):\n",
    "\n",
    "        # Iterate over the datasets\n",
    "        for label, color, dataset_key in zip(labels, colors, dataset_dictionary.keys()):\n",
    "            axis.hist(dataset_dictionary[dataset_key][parameter_key], bins=50, histtype=\"step\", log=True, label=label, color=color, weights=dataset_dictionary[dataset_key][KEYS[\"weights\"]])\n",
    "            axis.legend()\n",
    "            axis.set(\n",
    "                title=parameter_key,\n",
    "                ylabel=\"Values\",\n",
    "                xlabel=x_labels[idx]\n",
    "            )\n",
    "            axis.ticklabel_format(axis='x', style='sci', scilimits=(0,0))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Plot the kinematic parameters\n",
    "print(\"KINEMATIC PARAMETERS: NO CUTS\\n\")\n",
    "plot_kinematic_parameters(dataset_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define cuts which we will apply to each kinematic parameter in each dataset\n",
    "\n",
    "We will define each cutoffs in a dictionary in order to facilitate applying the cuts. The dictionary is used such that we can add a new key containing a new set of cuts. Therefore, if we want to apply a new set of cuts, all we have to do is change the key used in the function which applies cuts.\n",
    "\n",
    "The dictionary contain the upper and lower bound to a cut that we wish to make. By including a lower bound, we can increase how harsh we can make our cuts if we wish\n",
    "\n",
    "There are tree sets of cutoffs which have been analysied:\n",
    "\n",
    "- Standard cuts: These cuts have been found to have efficencies of:\n",
    "    - Signal efficiency: 77.1%\n",
    "    - Background efficiency: 11.2%\n",
    "\n",
    "- Harsh cuts: These are a set of harshed cuts aimed to reduce the background efficneny without a significant impact in signal efficiency:\n",
    "    - Signal efficiency: 47.5%\n",
    "    - Background efficiency: 3.0%\n",
    "\n",
    "- Loose cuts: These are a set of harshed cuts aimed to increase the signal efficneny without a care on the reduction of the background efficiency:\n",
    "    - Signal efficiency: 99.6%\n",
    "    - Background efficiency: 96.6%\n",
    "\n",
    "Harsh cuts increase the visibility of the signal, but at the cost of **reducing the number of signal entries by 14239 entries**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of harsh cuts to improve signal visibility\n",
    "harsh_cut_dictionary =  {\n",
    "    # Define the kinematic parameter to be cut\n",
    "    \"lep1_pt\":  {\n",
    "        # Define bounds of cut\n",
    "        \"minimum\"   : 1.2e5,\n",
    "        \"maximum\"   : 6e5,\n",
    "    },\n",
    "    \"lep2_pt\":  {\n",
    "        \"minimum\"   : 5e4,\n",
    "        \"maximum\"   : 3.5e5,\n",
    "    },\n",
    "    \"fatjet_pt\":  {\n",
    "        \"minimum\"   : 2e5,\n",
    "        \"maximum\"   : 7.8e5,\n",
    "    },\n",
    "    \"fatjet_D2\":    {\n",
    "        \"minimum\"   : 0.0,\n",
    "        \"maximum\"   : 1.2, \n",
    "    },\n",
    "    \"Zll_mass\":    {\n",
    "        \"minimum\"   : 6e4,  \n",
    "        \"maximum\"   : 1.3e5,\n",
    "    },\n",
    "    \"Zll_pt\":    {\n",
    "        \"minimum\"   : 2.2e5,  \n",
    "        \"maximum\"   : 6.8e5,\n",
    "    },\n",
    "    \"MET\":    {\n",
    "        \"minimum\"   : 1e2,  \n",
    "        \"maximum\"   : 2.5e5,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define a set of loose cuts\n",
    "loose_cut_dictionary =  {\n",
    "    # Define the kinematic parameter to be cut\n",
    "    \"lep1_pt\":  {\n",
    "        # Define bounds of cut\n",
    "        \"minimum\"   : 0,\n",
    "        \"maximum\"   : 7.5e5,\n",
    "    },\n",
    "    \"lep2_pt\":  {\n",
    "        \"minimum\"   : 0,\n",
    "        \"maximum\"   : 4e5,\n",
    "    },\n",
    "    \"fatjet_pt\":  {\n",
    "        \"minimum\"   : 0,\n",
    "        \"maximum\"   : 1.2e6,\n",
    "    },\n",
    "    \"fatjet_D2\":    {\n",
    "        \"minimum\"   : 0,\n",
    "        \"maximum\"   : 1.4e1, \n",
    "    },\n",
    "    \"Zll_mass\":    {\n",
    "        \"minimum\"   : 0,  \n",
    "        \"maximum\"   : 5e5,\n",
    "    },\n",
    "    \"Zll_pt\":    {\n",
    "        \"minimum\"   : 0,  \n",
    "        \"maximum\"   : 1.25e6,\n",
    "    },\n",
    "    \"MET\":    {\n",
    "        \"minimum\"   : 0,  \n",
    "        \"maximum\"   : 4.4e5,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Define a set of standard cuts\n",
    "standard_cut_dictionary =  {\n",
    "    # Define the kinematic parameter to be cut\n",
    "    \"lep1_pt\":  {\n",
    "        # Define bounds of cut\n",
    "        \"minimum\"   : 9e4,\n",
    "        \"maximum\"   : 5e5,\n",
    "    },\n",
    "    \"lep2_pt\":  {\n",
    "        \"minimum\"   : 2.5e4,\n",
    "        \"maximum\"   : 6.5e5,\n",
    "    },\n",
    "    \"fatjet_pt\":  {\n",
    "        \"minimum\"   : 2.5e5,\n",
    "        \"maximum\"   : 7.5e5,\n",
    "    },\n",
    "    \"fatjet_D2\":    {\n",
    "        \"minimum\"   : 1e-2,\n",
    "        \"maximum\"   : 2.5, \n",
    "    },\n",
    "    \"Zll_mass\":    {\n",
    "        \"minimum\"   : 7.5e4,  \n",
    "        \"maximum\"   : 1.05e5,\n",
    "    },\n",
    "    \"Zll_pt\":    {\n",
    "        \"minimum\"   : 2.5e5,  \n",
    "        \"maximum\"   : 8e5,\n",
    "    },\n",
    "    \"MET\":    {\n",
    "        \"minimum\"   : 50,  \n",
    "        \"maximum\"   : 1e6,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the dictionary to contain our cuts, we must make a function which is capable of applying the cuts to each of our datasets. \n",
    "\n",
    "To do this, the function will first loop over each of the datasets in the dataset dictionaries and then use the query method to find all entries within the bounds defined by the cuts for each kinematic parameter. \n",
    "\n",
    "The funcnction will then return two dataframes, the original dataset dictionary prior to applying the cuits in addition to a dataset dictionary were each of the dataframes have had the kinematic cuts applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cuts(dataset_dictionary, cut_dictionary, ):\n",
    "    \"\"\"\n",
    "    Make a copy of the dataset dictionary and apply the cuts passed to the function. Return the new dictionary with the cuts applied\n",
    "    \"\"\"\n",
    "    # Make a copy of the dataset dictionary with same keys and empty values\n",
    "    dictionary_dataset_precut = {key:None for key in dataset_dictionary.keys()}\n",
    "    dictionary_dataset_postcut = {key:None for key in dataset_dictionary.keys()}\n",
    "\n",
    "    # Make deep copies of the dasatset dataframes and assign them to the copy dictionary\n",
    "    for key in dictionary_dataset_precut:\n",
    "        dictionary_dataset_precut[key] = dataset_dictionary[key].copy(deep=True)\n",
    "        dictionary_dataset_postcut[key] = dataset_dictionary[key].copy(deep=True)\n",
    "\n",
    "    # Apply the cuts\n",
    "    for cut_key in cut_dictionary.keys():\n",
    "        for dataset_key in dataset_dictionary.keys():\n",
    "            # query_string = f\"{cut_key} < {cut_dictionary[cut_key][dataset_key]}\"\n",
    "            query_string = f\"{cut_key} > {cut_dictionary[cut_key]['minimum']}\" + \" & \" + f\"{cut_key} < {cut_dictionary[cut_key]['maximum']}\"\n",
    "            dictionary_dataset_postcut[dataset_key] = dictionary_dataset_postcut[dataset_key].query(query_string)\n",
    "\n",
    "    return dictionary_dataset_precut, dictionary_dataset_postcut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the function which will apply our kinematic cuts, we will apply all three cuts (harsh, standard and loose) top the dataset dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the harsh cuts to the dataframe\n",
    "dataset_dictionary_pre_harsh_cut, dataset_dictionary_post_harsh_cut = apply_cuts(dataset_dictionary, harsh_cut_dictionary)\n",
    "\n",
    "# Apply the standard cuts on the dataframe\n",
    "dataset_dictionary_pre_standard_cut, dataset_dictionary_post_standard_cut = apply_cuts(dataset_dictionary, standard_cut_dictionary)\n",
    "\n",
    "# Apply the standard cuts on the dataframe\n",
    "dataset_dictionary_pre_loose_cut, dataset_dictionary_post_loose_cut = apply_cuts(dataset_dictionary, loose_cut_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now visualise the kineamic parameter distributions post cut for all three cuts defined. This will allow us to view how the cuts affected the distribution, which parameters can be used to better remove the most background event possible by spotting if any distributuion has a different mode for background entries and signal entries.\n",
    "\n",
    "This will help us tweak the cuts to improve their performance in increasing signal efficiency or reducing bacground efficiency.\n",
    "\n",
    "Below are the distribution plots for all three kinematic cuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the datasets with a standard cut\n",
    "print(\"KINEMATIC PARAMETERS: STANDARD CUTS\\n\")\n",
    "plot_kinematic_parameters(dataset_dictionary_post_standard_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the datasets with a harsh cut\n",
    "print(\"KINEMATIC PARAMETERS: HARSH CUTS\\n\")\n",
    "plot_kinematic_parameters(dataset_dictionary_post_harsh_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the datasets with a loose cut\n",
    "print(\"KINEMATIC PARAMETERS: LOOSE CUTS\\n\")\n",
    "plot_kinematic_parameters(dataset_dictionary_post_loose_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something we notice from these distributions is the fact that for most parameters, the mode of the distributions of signal and backgoeund entries match. However, for the transversal momentum distriutuions (```lep1_pt```, ```lep2_pt```, ```fatjet_pt``` and ```Zll_pt```), the mode of the ggH1000 distribution is skewed more towards higher masses. \n",
    "\n",
    "Hence for these parametes, we canuse cuts which remove a majority of the background entris without majorly affecting the signal distributions. These are the kinematic parameters of uinterects when tweaking the cuts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we still have all four datasets in individual dataframes within the library, we will now want to merge them to have one large dataframe with mixed signal and background entries.\n",
    "\n",
    "TYo do this, we create a function which will concatinate all dataframes within a dataset dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(dataset_dictionary):\n",
    "    \"\"\"\n",
    "    Function to merge the all datasets in a dictionary \n",
    "    \"\"\"\n",
    "    \n",
    "    # Merge the dataset\n",
    "    merged_dataset = pd.concat(\n",
    "        [dataset_dataframe for dataset_dataframe in dataset_dictionary.values()],\n",
    "        ignore_index = True\n",
    "    )\n",
    "\n",
    "    # Shuffle the merged dataset\n",
    "    merged_dataset = shuffle(merged_dataset, random_state=SEED)\n",
    "\n",
    "    # Reset indexes of merged sdataset\n",
    "    merged_dataset = merged_dataset.reset_index(drop=True)\n",
    "\n",
    "    return merged_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our function to merge datasrt dictionaries, we will merge all the dataset dictionaeies we have, including those pre and post the application of our kinematic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the pre and post cuts for standard cut\n",
    "merge_dataset_pre_standard_cut = merge_datasets(dataset_dictionary_pre_standard_cut)\n",
    "merge_dataset_post_standard_cut = merge_datasets(dataset_dictionary_post_standard_cut)\n",
    "\n",
    "# Merge the pre and post cuts for harsh cut\n",
    "merge_dataset_pre_harsh_cut = merge_datasets(dataset_dictionary_pre_harsh_cut)\n",
    "merge_dataset_post_harsh_cut = merge_datasets(dataset_dictionary_post_harsh_cut)\n",
    "\n",
    "# Merge the pre and post cuts for loose cut\n",
    "merge_dataset_pre_loose_cut = merge_datasets(dataset_dictionary_pre_loose_cut)\n",
    "merge_dataset_post_loose_cut = merge_datasets(dataset_dictionary_post_loose_cut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the mass distributions for each kinematic cuts which were uses, a function was written which will plot a stacked historgeram of the entries in the reconstructed mass distributuion. The two distributions which are stacked are the signal entries and the bakcgreound entries. \n",
    "\n",
    "Additionally, the full weighted distribution of the combined signal and background mass distrivution is plotted so that we can accuratelly gauge how noticable the signal is whn using the correct weighiting between background events and signal events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_merged_datasets(merged_post, range=(0.7e6, 1.5e6), plot_title=\"\"):\n",
    "    \"\"\"\n",
    "    Plot the merged datasets to visualise events and the effects of the kinematic cuts applied to the entire merged dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Plot the weighted distributions\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    # List of events \n",
    "    events = [merged_post.query(\"isSignal==0\")[\"reco_zv_mass\"], merged_post.query(\"isSignal==1\")[\"reco_zv_mass\"]]\n",
    "    labels = [\"Background\", \"Signal\"]\n",
    "\n",
    "    # Iterate over all merged datasets to plot\n",
    "    ax[0].hist(events, histtype=\"step\", log=True, bins=75, label=labels, stacked=True, range=range)\n",
    "    ax[1].hist(merged_post[\"reco_zv_mass\"], log=False, bins=75, range=range, histtype=\"step\", weights=merged_post[\"FullEventWeight\"])\n",
    "    \n",
    "    ax[0].set_title(\"Entry distribution for Reco Mass\", fontsize=14)\n",
    "    ax[1].set_title(\"Event distribution for Reco Mass\", fontsize=14)\n",
    "    ax[0].set_xlabel(\"Reco Mass (MeV)\", fontsize=12)\n",
    "    ax[1].set_xlabel(\"Reco Mass (MeV)\", fontsize=12)\n",
    "    ax[0].set_ylabel(\"Number of Entries (Log)\", fontsize=12)\n",
    "    ax[1].set_ylabel(\"Number of Events\", fontsize=12)\n",
    "    ax[0].legend()\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(f\"Dataset w/ {plot_title}\", y=1.05, fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to plotting the mass distrivutions for each of the kinematic cuts, we calso want to parametrise the efficiencies for signal and background entries pre and post applying cuts.  The plots will show the range of (0.7e6, 1.5e6) so that we can easily spot the signal peak.\n",
    "\n",
    "These efficiencies will help us understant just how harsh each set of cuts is, and can help us tweak the cuts if we realise we have lost signal efficiency without a reduction in background efficiencies.\n",
    "\n",
    "Lastly, we also develop a function which can integrate the weighted mass distribution for both signal and background events so that we can quatify exactly how many events we have post kinematic cuts. \n",
    "\n",
    "All of these fuctions cvan help us judge the quality of the cuts and allow us to select a set of cuts for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_merged_efficiencies(merged_pre, merged_post,):\n",
    "    \"\"\"\n",
    "    Function will print the efficiencies on the background and signal enties after appling kinematic cuts\n",
    "    \"\"\"\n",
    "    # Define parameters for cunction\n",
    "    isSignal_values = [0, 1]\n",
    "    isSignal_labels = [\"Background\", \"Signal\"]\n",
    "\n",
    "    # Iterate over signal and background\n",
    "    for value, label in zip(isSignal_values, isSignal_labels):\n",
    "        \n",
    "        # Find number of variables pre and post application of cuts\n",
    "        precut_entries = merged_pre.query(f\"isSignal == {value}\").shape[0] \n",
    "        postcut_entries = merged_post.query(f\"isSignal == {value}\").shape[0] \n",
    "\n",
    "        # Calculate efficiency\n",
    "        efficiency = postcut_entries/precut_entries\n",
    "\n",
    "        # Print the data computed in the function\n",
    "        print(f\"{label}:   Efficiency: {efficiency:.1%} ##### Precut Length: {precut_entries} ##### Postcut Length: {postcut_entries}\")\n",
    "\n",
    "def integrate_dataset_events(merged_dataset,):\n",
    "    \"\"\"\n",
    "    Perform an integral on the signal and background sitributions and return the number fo events for each distribution\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the reco_mass and weights for all entries for both the signal entries\n",
    "    reco_mass_signal = merged_dataset.query(\"isSignal == 1\")[\"reco_zv_mass\"]\n",
    "    event_weights_signal = merged_dataset.query(\"isSignal == 1\")[\"FullEventWeight\"]\n",
    "\n",
    "    # Get the reco_mass and weights for all entries for both the background entries\n",
    "    reco_mass_background = merged_dataset.query(\"isSignal == 0\")[\"reco_zv_mass\"]\n",
    "    event_weights_background = merged_dataset.query(\"isSignal == 0\")[\"FullEventWeight\"]\n",
    "\n",
    "    # Compute the integral for the signals\n",
    "    values_signal, bins_signal = np.histogram(reco_mass_signal, weights=event_weights_signal)\n",
    "    integrated_area_signal = sum(np.diff(bins_signal)*values_signal)\n",
    "\n",
    "    # Compute the integral for the background\n",
    "    values_background, bins_background = np.histogram(reco_mass_background,)\n",
    "    integrated_area_background = sum(np.diff(bins_background)*values_background)\n",
    "\n",
    "    print(f\"Number of events in signal distribution: {integrated_area_signal:.2e}\")\n",
    "    print(f\"Number of events in background distribution: {integrated_area_background:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot a pairplot for all the kinemaric parameters. As this plot is desiged to see the correlations between the kinematic paramerters, and the correlations shouldnt be affected by the cuts; we will only compute the pairplot for one of the datasets post cut.\n",
    "\n",
    "We will use the dataset with the harsh cut as we are happy that the dataset has removed any major outliers.\n",
    "\n",
    "Due to the size of the datasets, the creation of the pairplot can take various minutes, hence there is a method to force the notebook to skip the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pairplot = False\n",
    "\n",
    "if plot_pairplot:\n",
    "    sns.pairplot(\n",
    "        merge_dataset_post_harsh_cut[KEYS[\"features_and_targets\"]],\n",
    "        hue = KEYS[\"targets\"][0],\n",
    "        plot_kws=dict(marker=\"+\", linewidth=1, alpha=0.5),\n",
    "        corner = False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will go through each of thwe merged datasets post application of the kinematic cuts and compute the efficiencies, the event integrals and plottting each of the mass distribuitions to visualise the signal peak visibility.\n",
    "\n",
    "We will do this for all kineamic cut sets (standard, loose and harsh) and discuss them at the end of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print efficiencies and number of entries pre and post harsh cuts for signal and background events\n",
    "print(\"EFFICIENCIES AND PURITIES FOR DATASET WITH STANDARD CUT\\n\")\n",
    "calculate_merged_efficiencies(merge_dataset_pre_standard_cut, merge_dataset_post_standard_cut)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print the number of events in the signal and background distribution\n",
    "integrate_dataset_events(merge_dataset_post_standard_cut)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Plot the distribution of events (weighted) pre and post harsh cuts\n",
    "plot_merged_datasets(merge_dataset_post_standard_cut, plot_title=\"Standard cuts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print efficiencies and number of entries pre and post harsh cuts for signal and background events\n",
    "print(\"EFFICIENCIES AND PURITIES FOR DATASET WITH HARSH CUT\\n\")\n",
    "calculate_merged_efficiencies(merge_dataset_pre_harsh_cut, merge_dataset_post_harsh_cut)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print the number of events in the signal and background distribution\n",
    "integrate_dataset_events(merge_dataset_post_harsh_cut)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Plot the distribution of events (weighted) pre and post harsh cuts\n",
    "plot_merged_datasets(merge_dataset_post_harsh_cut, plot_title=\"Harsh cuts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print efficiencies and number of entries pre and post loose cuts for signal and background events\n",
    "print(\"EFFICIENCIES AND PURITIES FOR DATASET WITH LOOSE CUT\\n\")\n",
    "calculate_merged_efficiencies(merge_dataset_pre_loose_cut, merge_dataset_post_loose_cut)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print the number of events in the signal and background distribution\n",
    "integrate_dataset_events(merge_dataset_post_loose_cut)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Plot the distribution of events (weighted) pre and post loose cuts\n",
    "plot_merged_datasets(merge_dataset_post_loose_cut, plot_title=\"Loose cuts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "From the efficencies, which were quoted previously, we see that the harsh cut does the best job at reducuing the number of background enties to 3% of what they were pre application of the cuts. However, this sadly came with the downside of reducing the efficiencies of signal entries. \n",
    "\n",
    "This did significantly increase the visibility of the signal peak though, with the standard cuts being the second most visible signal peak and the loose cuts not showing the signal peak at all.\n",
    "\n",
    "Another thing to not that the cuts affect is the shape of the background distribution. From the loose cut, we can tell that the background shape is ecxponential. However, as the cuts get sharper, the distribution becomes more linear. This distortion is the downside of reducing the background efficiency too much. Although the harsh cuts increase the signal peak, they also distort the shape of the backgeound, hence our hypothesis test may not be as accurate as we initially expect.\n",
    "\n",
    "As such, we will attemnt to use the harsh cuts for our hypothesis testing as even though the background signal is distorted, it is the cut that best increases the visibility of the signal peak in the weighted distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Hypothesis testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing and distribution visualisation\n",
    "\n",
    "Now that we are satisfied with our processing of the distrivution data, we can start to build the code which we will use to fit our null and alternate hypothesis to the reconstructed mass distribution.\n",
    "\n",
    "We will start with writitng a funtion which will take out merged distribnution post cut and return individual distributions conmtaining only the signal and background entries. This is done so that we can fit functions to the signal and backgeround distributions seperately.\n",
    "\n",
    "This will not only help us find out which functions best describe the signal and background distributions, but also help us determine individual values when fitting the combined distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_signal_background(dataset, subrange=(0.7e6, 1.5e6), classification_score_threshold=None):\n",
    "    \"\"\"\n",
    "    This function will split a merged dataframe into two dataframes containing Signal and Background entries respectivly. \n",
    "    Function is also capable of applying a subrange to the dataset if an ArrayLike object of length 2.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply subrange to dataset if subrange is an ArrayLike object with two limits\n",
    "    if isinstance(subrange, Sequence) and len(subrange)==2:\n",
    "        signal_background_dataset = dataset.query(f\"reco_zv_mass > {subrange[0]} & reco_zv_mass < {subrange[1]}\")\n",
    "    else:\n",
    "        signal_background_dataset = dataset\n",
    "\n",
    "    # If a classification score is provided, remove all signal entries below the threshold\n",
    "    if classification_score_threshold:\n",
    "        # Create a datasety with all background entries and all signal entries above the threshold\n",
    "        signal_background_dataset = pd.concat([ signal_background_dataset.query(\"isSignal==1 & nn_classification_score>=0.5\"), signal_background_dataset.query(\"isSignal==0\") ])    \n",
    "\n",
    "    # Create individual datasets for signal \n",
    "    signal_dataset = signal_background_dataset.query(\"isSignal==1\")\n",
    "\n",
    "    # Create individual datasets for background \n",
    "    background_dataset = signal_background_dataset.query(\"isSignal==0\")\n",
    "\n",
    "    return signal_background_dataset, signal_dataset, background_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function which will return calculate our reconstructed mass distributuion's histograms and return the bin ranges, and counts in each bins of the distribution.\n",
    "\n",
    "Additionally, the function will compute the center of the bins so that we can plot the hstogram as a scatter plot. \n",
    "\n",
    "Lastly, the function will go through each entry in a dataset and assign the index of the bin to which the entry corresponds in the mass spectrum distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dataset_histograms(dataset, n_bins=60):\n",
    "    \"\"\"\n",
    "    Compute the histogram bins and counts for a dataset. This function will also add a column to the dataset with the index \n",
    "    of the histogram bin to which the entry correspons to\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute the counts and the bins for the dataset according to the event weights\n",
    "    dataset_counts, dataset_bins = np.histogram(dataset[\"reco_zv_mass\"], bins=n_bins, weights=dataset[\"FullEventWeight\"])\n",
    "\n",
    "    # Compute the center point of the bins of the dataset histogram\n",
    "    dataset_bins_center = ( dataset_bins + (dataset_bins[1]-dataset_bins[0])/2 )[:-1]\n",
    "\n",
    "    # Compute an array containing the bin index to which each index corresponds to\n",
    "    dataset_bin_indices = np.digitize(dataset[\"reco_zv_mass\"], bins=dataset_bins) - 1\n",
    "    \n",
    "    # Add a column with the bin idex to which each entry corresponds to\n",
    "    dataset[\"bin_index\"] = dataset_bin_indices\n",
    "\n",
    "    return dataset, dataset_counts, dataset_bins, dataset_bins_center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that out distributuions are weighted corresponding to the process which created the entry, we need to compute the weighted number of events and the weighted standard seviations in order to latyer compute the chi squared of a fit.\n",
    "\n",
    "Hence we write a function which will take a dataframe of a specific distribution and compute the weighted observed events in addition tothe weighted errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dataset_binned_events(dataset, dataset_bins_center):\n",
    "    \"\"\"\n",
    "    Find the weighted number of events and the weighted errors per bin of the dataset. \n",
    "    \"\"\"\n",
    "    # Create an empty array to contain the weighted events and errors per bin\n",
    "    n_observed = np.zeros([dataset_bins_center.size])\n",
    "    sigma_squared = np.zeros([dataset_bins_center.size])\n",
    "\n",
    "    # Iterate over all bins in the dataset\n",
    "    for i in range(dataset_bins_center.size):\n",
    "        # Compute and assign the weighted number of observed events\n",
    "        n_observed[i] = np.sum( dataset.query(f\"bin_index == {i}\")[\"FullEventWeight\"] )\n",
    "\n",
    "        # Compute and assign the weighted errors \n",
    "        sigma_squared[i] = np.sum( dataset.query(f\"bin_index == {i}\")[\"FullEventWeight\"]**2 )\n",
    "    \n",
    "    return n_observed, sigma_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defiend all out functions to process out datasets prior to curve fitting, we can apply them in order to obtain all relevant parameters for the signal, bakcground and signal+backgreound reconstructed mass distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into signal and background entries\n",
    "signal_background_dataset, signal_dataset, background_dataset = split_dataset_signal_background(merge_dataset_post_harsh_cut)\n",
    "\n",
    "# Compute the histogram bins and counts for each dataset\n",
    "signal_background_dataset, signal_background_counts, _, signal_background_bins_center = compute_dataset_histograms(signal_background_dataset, n_bins=N_BINS)\n",
    "signal_dataset, signal_counts, _, signal_bins_center = compute_dataset_histograms(signal_dataset, N_BINS)\n",
    "background_dataset, background_counts, _, background_bins_center = compute_dataset_histograms(background_dataset, N_BINS)\n",
    "\n",
    "# Compute the weigthed number of events and errors for each histogram bin\n",
    "n_observed_signal_background, sigma_squared_signal_background = find_dataset_binned_events(signal_background_dataset, signal_background_bins_center)\n",
    "n_observed_signal, sigma_squared_signal = find_dataset_binned_events(signal_dataset, signal_bins_center)\n",
    "n_observed_background, sigma_squared_background = find_dataset_binned_events(background_dataset, background_bins_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualise the mass distribution before fitting, we can plot the weighted signal, backgeround and signal+background distributions. This will help us visualise the peak, in addition to see the shapes of the signal and background distributions seperately.\n",
    "\n",
    "We also notice that the number of bins plays a critical role in the visibility of the peak. We selected 75 bins as it gives us a nice balance between increasing the granularity of the distribution to show visibility of the signal peak without reducing too much the number of events in each bin, which could negativly affect the fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rlot the Counts and pos to verify that they represent the histogram correctly\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 5))\n",
    "titles = [\"Total Mass distribution\", \"Signal mass distribution\", \"Background mass distribution\"]\n",
    "\n",
    "for idx, (dataset, bins_center, counts) in enumerate(zip([signal_background_dataset, signal_dataset, background_dataset], [signal_background_bins_center, signal_bins_center, background_bins_center], [signal_background_counts, signal_counts, background_counts])):\n",
    "    ax[idx].scatter(bins_center, counts, c=\"r\", marker=\"x\")\n",
    "    ax[idx].hist(dataset[\"reco_zv_mass\"], weights=dataset[\"FullEventWeight\"], bins=N_BINS, alpha=0.5, color=\"blue\")\n",
    "    ax[idx].set_title(titles[idx], fontsize=14)\n",
    "    ax[idx].set_xlabel(\"Reco Mass (MeV)\", fontsize=12)\n",
    "    ax[idx].set_ylabel(\"Number of Events\", fontsize=12)\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the functions which we are going to fit. \n",
    "\n",
    "Initially, we are going to describe the signal with a gaussian function and the background with a fourth order polynomial.\n",
    "\n",
    "We also define a signal+background function combning the gaussian and the fourth order polynomial which we will treat as our alternative hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mu, sigma, norm):\n",
    "    \"\"\"\n",
    "    Gaussian function to describe signal distribution\n",
    "    \"\"\"\n",
    "    return (norm * np.exp(-0.5 * ((x - mu) / sigma) ** 2))\n",
    "\n",
    "def fourth_order_poly(x, a, b, c, d, e):\n",
    "    \"\"\"\n",
    "    Fourth order polynomial to describe background distribution\n",
    "    \"\"\"\n",
    "    return a + b*x + c*x**2 + d*x**3 + e*x**4\n",
    "\n",
    "def second_order_poly(x, a, b, c,):\n",
    "    \"\"\"\n",
    "    second order polynomial to describe background distribution\n",
    "    \"\"\"\n",
    "    return a + b*x + c*x**2 \n",
    "\n",
    "def first_order_poly(x, a, b,):\n",
    "    \"\"\"\n",
    "    first order polynomial to describe background distribution\n",
    "    \"\"\"\n",
    "    return a + b*x \n",
    "\n",
    "def gaussian_plus_fourth_poly(x, a, b, c, d, e, mu, sigma, norm):\n",
    "    \"\"\"\n",
    "    Funciton combining a gaussian with a fourth order polynomial to represent our alternative hypothjesis of the overall reconstructed mass distribution\n",
    "    \"\"\"\n",
    "    return gaussian(x, mu, sigma, norm,) + fourth_order_poly(x, a, b, c, d, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the different chi square functions which we will be using throughout out minimisation processes. \n",
    "\n",
    "Here we will define a chi squared function for:\n",
    "\n",
    "- A pure signal distribution dewscribed by a gaussian function\n",
    "\n",
    "- A pure background distribution described by a fourth order polynomial\n",
    "\n",
    "- A signal + background function described by a fourth order polynomial (This will act as our null hypothesis)\n",
    "\n",
    "- A signal + background function described by a gaussian + fourth order polynomial (This will act as our alternate hypothesis)\n",
    "\n",
    "The chi squared functions are modified to take into account the faxct that our distributions are weighted by different weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared_signal_gaussian(mu, sigma, norm):\n",
    "    \"\"\"\n",
    "    Modified weighted chi squared function for a signal distribution described by gaussian\n",
    "    \"\"\"\n",
    "    numerator = (n_observed_signal - gaussian(signal_bins_center, mu, sigma, norm) )**2\n",
    "    return np.sum( numerator/sigma_squared_signal )\n",
    "\n",
    "def chi_squared_signal_second_poly( a, b, c, ):\n",
    "    \"\"\"\n",
    "    Modified weighted chi squared function for a signal distribution described by second order polynomial \n",
    "    \"\"\"\n",
    "    numerator = (n_observed_signal - second_order_poly(signal_bins_center, a, b, c, ) )**2\n",
    "    return np.sum( numerator/sigma_squared_signal )\n",
    "\n",
    "def chi_squared_background_fourth_poly(a, b, c, d, e):\n",
    "    \"\"\"\n",
    "    Modified weighted chi squared function for a background distribution described by fourth order polynomial \n",
    "    \"\"\"\n",
    "    numerator = (n_observed_background - fourth_order_poly(background_bins_center, a, b, c, d, e) )**2\n",
    "    return np.sum( numerator/sigma_squared_background )\n",
    "\n",
    "def chi_squared_background_first_poly(a, b,):\n",
    "    \"\"\"\n",
    "    Modified weighted chi squared function for a background distribution described by first order polynomial \n",
    "    \"\"\"\n",
    "    numerator = (n_observed_background - first_order_poly(background_bins_center, a, b, ) )**2\n",
    "    return np.sum( numerator/sigma_squared_background )\n",
    "\n",
    "def chi_squared_alternative_gaussian_plus_fourth_poly( a, b, c, d, e, mu, sigma, norm,):\n",
    "    \"\"\"\n",
    "    Modified weighted chi squared function for a signal + background distribution described by a gaussian + fourth order polynomial \n",
    "    \"\"\"\n",
    "    numerator = (n_observed_signal_background - gaussian_plus_fourth_poly(signal_background_bins_center, a, b, c, d, e, mu, sigma, norm) )**2\n",
    "    return np.sum( numerator/sigma_squared_signal_background )\n",
    "\n",
    "def chi_squared_null_fourth_poly( a, b, c, d, e,):\n",
    "    \"\"\"\n",
    "    Modified weighted chi squared function for a signal + background distribution described by fourth order polynomial \n",
    "    \"\"\"\n",
    "    numerator = (n_observed_signal_background - fourth_order_poly(signal_background_bins_center, a, b, c, d, e) )**2\n",
    "    return np.sum( numerator/sigma_squared_signal_background )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fitting the signal distribution\n",
    "\n",
    "We will now fit a our gaussian function to the signal mass distribution and validate that it indeed is a good description of the signal.\n",
    "\n",
    "We will also plot the distrbution to validate the fit.\n",
    "\n",
    "We will first compute the signal fit with fourth order polynomial and then with a gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define minimiser to minimise the signal function\n",
    "signal_fit_fourth_poly_results = Minuit(\n",
    "    chi_squared_signal_second_poly,\n",
    "    a=2e6,\n",
    "    b=7e6,\n",
    "    c=6e6, \n",
    ")\n",
    "\n",
    "# Minimise the signal function\n",
    "signal_fit_fourth_poly_results.migrad()\n",
    "signal_fit_fourth_poly_results.hesse()\n",
    "\n",
    "# Print minimised parameters\n",
    "print(signal_fit_fourth_poly_results.params)\n",
    "\n",
    "# Print the minimised chi squared\n",
    "print('Final chisq for gaussian signal: ', signal_fit_fourth_poly_results.fval)\n",
    "\n",
    "# Plot the minimised signal function\n",
    "plt.scatter(signal_bins_center, signal_counts, c=\"r\", marker=\"x\", label=\"Observed events\")\n",
    "plt.hist(signal_dataset[\"reco_zv_mass\"], weights=signal_dataset[\"FullEventWeight\"], bins=N_BINS, alpha=0.3, color=\"blue\",)\n",
    "plt.plot(signal_bins_center, second_order_poly(signal_bins_center, *signal_fit_fourth_poly_results.values), color=\"green\", label=\"quadratic fit\")\n",
    "plt.title(\"Quadratic fit on signal mass distribution\", fontsize=14)\n",
    "plt.xlabel(\"Reco Mass (MeV)\", fontsize=12)\n",
    "plt.ylabel(\"Number of Events\", fontsize=12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define minimiser to minimise the signal function\n",
    "signal_fit_results = Minuit(\n",
    "    chi_squared_signal_gaussian,\n",
    "    mu=1e6,\n",
    "    sigma=1e6, \n",
    "    norm=10,\n",
    ")\n",
    "\n",
    "# Minimise the signal function\n",
    "signal_fit_results.migrad()\n",
    "signal_fit_results.hesse()\n",
    "\n",
    "# Print minimised parameters\n",
    "print(signal_fit_results.params)\n",
    "\n",
    "# Print the minimised chi squared\n",
    "print('Final chisq for gaussian signal: ', signal_fit_results.fval)\n",
    "\n",
    "# Plot the minimised signal function\n",
    "plt.scatter(signal_bins_center, signal_counts, c=\"r\", marker=\"x\", label=\"Observed events\")\n",
    "plt.hist(signal_dataset[\"reco_zv_mass\"], weights=signal_dataset[\"FullEventWeight\"], bins=N_BINS, alpha=0.3, color=\"blue\")\n",
    "plt.plot(signal_bins_center, gaussian(signal_bins_center, *signal_fit_results.values), color=\"green\", label=\"Gaussian fit\")\n",
    "plt.title(\"Gaussian fit on signal mass distribution\", fontsize=14)\n",
    "plt.xlabel(\"Reco Mass (MeV)\", fontsize=12)\n",
    "plt.ylabel(\"Number of Events\", fontsize=12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our fit, we obtain the best fit from a gaussian with a chi squared of 1805. From the plot, we are relativly happy with the fit and can continue to use it as the model for our signal in the alternate hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the background\n",
    "\n",
    "We will now fit a our first and fourth order polynomial function to the background mass distribution and validate that it indeed is a good description of the bakckground.\n",
    "\n",
    "We will also plot the distrbution to validate the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define minimiser to minimise the background function\n",
    "background_fit_results = Minuit(\n",
    "    chi_squared_background_first_poly,\n",
    "    a=2e3,\n",
    "    b=7e6,\n",
    ")\n",
    "\n",
    "# Minimise the background function\n",
    "background_fit_results.migrad()\n",
    "background_fit_results.hesse()\n",
    "\n",
    "# Print minimised parameters\n",
    "print(background_fit_results.params)\n",
    "\n",
    "# Print the minimised chi squared\n",
    "print('Final chisq for fourth order polynomial background: ', background_fit_results.fval)\n",
    "\n",
    "# Plot the minimised background function\n",
    "plt.scatter(background_bins_center, background_counts, c=\"r\", marker=\"x\", label=\"Observed events\")\n",
    "plt.hist(background_dataset[\"reco_zv_mass\"], weights=background_dataset[\"FullEventWeight\"], bins=N_BINS, alpha=0.3, color=\"blue\")\n",
    "plt.plot(background_bins_center, first_order_poly(background_bins_center, *background_fit_results.values), label=\"linear fit\", color=\"green\")\n",
    "plt.title(\"1th order polynomial fit  on background mass distribution\", fontsize=14)\n",
    "plt.xlabel(\"Reco Mass (MeV)\", fontsize=12)\n",
    "plt.ylabel(\"Number of Events\", fontsize=12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define minimiser to minimise the background function\n",
    "background_fit_results = Minuit(\n",
    "    chi_squared_background_fourth_poly,\n",
    "    a=2e6,\n",
    "    b=7e6,\n",
    "    c=-6e6, \n",
    "    d=2e6,\n",
    "    e=-1e6\n",
    ")\n",
    "\n",
    "# Minimise the background function\n",
    "background_fit_results.migrad()\n",
    "background_fit_results.hesse()\n",
    "\n",
    "# Print minimised parameters\n",
    "print(background_fit_results.params)\n",
    "\n",
    "# Print the minimised chi squared\n",
    "print('Final chisq for fourth order polynomial background: ', background_fit_results.fval)\n",
    "\n",
    "# Plot the minimised background function\n",
    "plt.scatter(background_bins_center, background_counts, c=\"r\", marker=\"x\", label=\"Observed events\")\n",
    "plt.hist(background_dataset[\"reco_zv_mass\"], weights=background_dataset[\"FullEventWeight\"], bins=N_BINS, alpha=0.3, color=\"blue\")\n",
    "plt.plot(background_bins_center, fourth_order_poly(background_bins_center, *background_fit_results.values), label=\"Fourth order poly fit\", color=\"green\")\n",
    "plt.title(\"4th order polynomial fit  on background mass distribution\", fontsize=14)\n",
    "plt.xlabel(\"Reco Mass (MeV)\", fontsize=12)\n",
    "plt.ylabel(\"Number of Events\", fontsize=12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our fit, we obtain the best fit from a background with a chi squared of 129 compared to the first order polynomial with a worse chi squared. From the plot, we are relativly happy with the fit and can continue to use it as the model for our background in the alternate hypothesis and our null hpyothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal + Background fit on merged dataset (alternate hypothsis)\n",
    "\n",
    "Now that we have found functions which we are happy with describin gthe signal and backgeround distributions individually, we can merge them togethewr to obtain a fit on our alternate hypothesis.\n",
    "\n",
    "To imporve the fit, we will be using the estimated parameters obtained form the signal and background fits as our initiall guesses for this fit. In addition, we will be fixing the mean and standard deviation of the signal distributuion.\n",
    "\n",
    "As before, we will plot the total distribution overlayed on top of the fitted alternate hypothesis to validate that the fit indeed is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define minimiser to minimise the alternatice hypothesis\n",
    "signal_background_fit_alternative_results = Minuit(\n",
    "    chi_squared_alternative_gaussian_plus_fourth_poly,\n",
    "    a=background_fit_results.values[0],\n",
    "    b=background_fit_results.values[1],\n",
    "    c=background_fit_results.values[2], \n",
    "    d=background_fit_results.values[3],\n",
    "    e=background_fit_results.values[4],\n",
    "    mu=signal_fit_results.values[0],\n",
    "    sigma=signal_fit_results.values[1],\n",
    "    norm=signal_fit_results.values[2],\n",
    ")\n",
    "\n",
    "# Fix the mean and std of the signal\n",
    "signal_background_fit_alternative_results.fixed[\"mu\"] = True \n",
    "signal_background_fit_alternative_results.fixed[\"sigma\"] = True\n",
    "\n",
    "# Minimise the alternatice hypothsis \n",
    "signal_background_fit_alternative_results.migrad()\n",
    "signal_background_fit_alternative_results.hesse()\n",
    "\n",
    "# Print minimised parameters\n",
    "print(signal_background_fit_alternative_results.params)\n",
    "\n",
    "# Print the minimised chi squared\n",
    "print('Final chisq for alternative hypothesis: ', signal_background_fit_alternative_results.fval)\n",
    "\n",
    "# Plot the minimised alternative hypothesis \n",
    "plt.scatter(signal_background_bins_center, signal_background_counts, c=\"r\", marker=\"x\", label=\"Observed events\")\n",
    "plt.hist(signal_background_dataset[\"reco_zv_mass\"], weights=signal_background_dataset[\"FullEventWeight\"], bins=N_BINS, alpha=0.3, color=\"blue\")\n",
    "plt.plot(signal_background_bins_center, gaussian_plus_fourth_poly(signal_background_bins_center, *signal_background_fit_alternative_results.values), color=\"green\", label=\"Alternate hupothesis\")\n",
    "plt.title(\"Alternate hypothesis fit on total mass distribution\", fontsize=14)\n",
    "plt.xlabel(\"Reco Mass (MeV)\", fontsize=12)\n",
    "plt.ylabel(\"Number of Events\", fontsize=12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " From the fit, we can tell that the alternate hypothesis does a good job at descirbing the distribution and the signal peak in particluar. We get an overall chi sqwuared for the alternate hypotheiss of 82.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background fit on Merged dataset\n",
    "\n",
    "Now that we have obtained our alternate hypothesis fit, we can move on to obtaining our null hypothesis fit.\n",
    "\n",
    "To imporve the fit, we will be using the estimated parameters obtained form the background fit as our initiall guesses for this fit. \n",
    "\n",
    "As before, we will plot the total distribution overlayed on top of the fitted alternate hypothesis to validate that the fit indeed is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define minimiser to minimise the null hypothesis\n",
    "signal_background_null_fit_results = Minuit(\n",
    "    chi_squared_null_fourth_poly,\n",
    "    a=background_fit_results.values[0],\n",
    "    b=background_fit_results.values[1],\n",
    "    c=background_fit_results.values[2], \n",
    "    d=background_fit_results.values[3],\n",
    "    e=background_fit_results.values[4],\n",
    ")\n",
    "\n",
    "# Minimise the null hypothesis\n",
    "signal_background_null_fit_results.migrad()\n",
    "signal_background_null_fit_results.hesse()\n",
    "\n",
    "# Print minimised parameters\n",
    "print(signal_background_null_fit_results.params)\n",
    "\n",
    "# Print the minimised chi squared\n",
    "print('Final chisq for the null hypothesis: ', signal_background_null_fit_results.fval)\n",
    "\n",
    "# Plot the minimised null hypothesis \n",
    "plt.scatter(signal_background_bins_center, signal_background_counts, c=\"r\", marker=\"x\", label=\"Observed events\")\n",
    "plt.hist(signal_background_dataset[\"reco_zv_mass\"], weights=signal_background_dataset[\"FullEventWeight\"], bins=N_BINS, alpha=0.3, color=\"blue\")\n",
    "plt.plot(signal_background_bins_center, fourth_order_poly(signal_background_bins_center, *signal_background_null_fit_results.values), label=\"Null hypothesis\", color=\"green\")\n",
    "plt.title(\"Null hypothesis fit on total mass distribution\", fontsize=14)\n",
    "plt.xlabel(\"Reco Mass (MeV)\", fontsize=12)\n",
    "plt.ylabel(\"Number of Events\", fontsize=12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is to be expected, the null hypothesis fit is worse as it does not take into account the signal peak. This is reflected with an increased chi squared of 118.3, which is larger than the chi squared obtained by the alternate hypothesis. \n",
    "\n",
    "Below we plot the null and alternate hypothesis overlayed on top of each other, where it is more noticable that the alternate fit describes the overall signal + background distribution better than the null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.scatter(signal_background_bins_center, signal_background_counts, c=\"r\", marker=\"x\")\n",
    "plt.hist(signal_background_dataset[\"reco_zv_mass\"], weights=signal_background_dataset[\"FullEventWeight\"], bins=N_BINS, alpha=0.3, color=\"blue\")\n",
    "plt.plot(signal_background_bins_center, fourth_order_poly(signal_background_bins_center, *signal_background_null_fit_results.values), label=\"Null hypothesis\", color=\"darkgreen\")\n",
    "plt.plot(signal_background_bins_center, gaussian_plus_fourth_poly(signal_background_bins_center, *signal_background_fit_alternative_results.values), label=\"Alternate hypothesis\", color=\"black\")\n",
    "plt.title(\"Total mass distribution with multiple hypotheses fits\", fontsize=14)\n",
    "plt.xlabel(\"Reco Mass (MeV)\", fontsize=12)\n",
    "plt.ylabel(\"Number of Events\", fontsize=12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametrising the significance of the alternate hypothesis\n",
    "\n",
    "Now that we have obtained a null and alternate hypothesis, we can apply Wilks theorem to quantify the significnse of the alternate fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the delta chi squared between the null and alternative hypothesis\n",
    "delt_chi = signal_background_null_fit_results.fval - signal_background_fit_alternative_results.fval\n",
    "\n",
    "# Define the difference number of degrees of freedom vetween the two fits\n",
    "dof = 1 \n",
    "\n",
    "# Compute the significance and the p score of the differentcwe in hypotheses\n",
    "p_value = chi2.sf(delt_chi, dof)\n",
    "z_score = np.sqrt(2)*erfinv(1-p_value)\n",
    "\n",
    "print(f\"The p value of the alternate hypothesis is {p_value:.3e}\")\n",
    "print(f\"The statistical significance of the signal deviation is {z_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the significance of the alternate fit is almost 6 sigma, which is above the 5 sigma threshold for accepting the alternate hypothesis. Hence, we are happy accept that there indeed a additional contribution to the mass distribution which is atributable to higgs decay and which can be modeled as a gaussian centered around 1e6MeV on top of the standard background which is to be expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Develpment of a NN Classifier**\n",
    "\n",
    "Now that we have confirmed that there is indeed an additional signal in our reconstructed mass distribution (which was to be expected given that we put it there), we will now move on to developing an ML algrothim which can classify whether an entry from our dataframnes belongs to a signal of wherther it belongs to a backgeound entry.\n",
    "\n",
    "We can go about it multiple ways, however we will initially stick a feed forward neural network.\n",
    "\n",
    "Below we define some global hyperparametes which will be used for all NN processes while training them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some hyperparametes\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 2048\n",
    "N_SIGNAL_SIZE = 22000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, to train a NN classifier, we will need to process the datasets which will be fed into the network. As such, we will create a function which will take as an input a dataframe containing all the signal and backgeound entries, and will return a training and validation features and targets which represent a 50/50 admixture of signal and events. \n",
    "\n",
    "This is done to allow the network to effectivly learn both the features for signal and background entries. If we didnt impose this 50/50 admixture condition, we would have an over representation of background entries and our network would only get the opportuinitiy to effectivly learn to classify background entires. \n",
    "\n",
    "The function first created a dataframe containing a 50/50 admixture, it then creates a numpy array containing the features and targets which will be used to train the network. The features are then passed through a scaller s.t the fistrivution of values for the features is better scaled to improve network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ml_training_datasets(dataset, admixture_size=10000, scaler=PowerTransformer(), split_dataset=True, validation_size=0.2, feature_keys=KEYS[\"features\"]):\n",
    "    \"\"\"\n",
    "    Apply preprocessing steps to dataset and create a training and validation dataset for training an ml classification algorithm\n",
    "    \"\"\"\n",
    "    # Create a dataset with a 50/50 admixture of signal and background entires\n",
    "    dataset_admixture = pd.concat([dataset.query(\"isSignal==1\").iloc[:admixture_size], dataset.query(\"isSignal==0\").iloc[:admixture_size]])\n",
    "\n",
    "    # Find the targets and featurs from the dataset\n",
    "    features, targets = dataset_admixture[feature_keys], dataset_admixture[KEYS[\"targets\"]]\n",
    "\n",
    "    # Scale the features for imporved nn training\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    # # Make a one hot encoded version of the targets\n",
    "    # targets_onehot = np.squeeze( tf.one_hot(targets.to_numpy(), depth=2), 1 )\n",
    "\n",
    "    # Create a test train split if sequested\n",
    "    if split_dataset:\n",
    "        # Create a validation and training dataset\n",
    "        features_train, features_val, targets_train, targets_val = train_test_split(features, targets, test_size=validation_size, random_state=SEED)\n",
    "        \n",
    "        # # Make a one hot encoded version of the targets\n",
    "        # targets_val_onehot = np.squeeze( tf.one_hot(targets_val.to_numpy(), depth=2), 1 )\n",
    "        # targets_train_onehot = np.squeeze( tf.one_hot(targets_train.to_numpy(), depth=2), 1 )\n",
    "\n",
    "        return features_train, targets_train, features_val, targets_val\n",
    "    \n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function defined above, we create training and validation datasets for our datasets before and after having applied kineametic cuts.\n",
    "\n",
    "The idea is to train a network on both datasets and see which dataset can better train the model. This will help us identify which dataset we should use mmoving forward to train the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainign and validation datasets for datasewts with no and harsh kinematic cuts\n",
    "no_cut_features_train, no_cut_targets_train, no_cut_features_val, no_cut_targets_val = create_ml_training_datasets(merge_dataset_pre_harsh_cut, admixture_size=N_SIGNAL_SIZE)\n",
    "harsh_cut_features_train, harsh_cut_targets_train, harsh_cut_features_val, harsh_cut_targets_val = create_ml_training_datasets(merge_dataset_pre_harsh_cut, admixture_size=N_SIGNAL_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training our networks, we define some functions wich will help us with the evaluation of our networks. Each function will plot a specific figure. The are:\n",
    "\n",
    "- The Loss and accuracies throughout the training of the network. This will allow us to visually inspect the performance of the network.\n",
    "\n",
    "- Print the final validation accuracy of the network. Will work in conjnction with the plot of aaccuracy to help us determine which model is better.\n",
    "\n",
    "- Plot a confusion matrix of the validation dataset using the predictions made by a network. Can help us delve into what specifically the network is misclassifying.\n",
    "\n",
    "- Plot a ROC curve which will give us an indication of the preformance of the network as a classification algorithm.\n",
    "\n",
    "- Plot the distriburtion of classificvation scores outputted by the model for both signal and background entries. This will tell us if there is a range of classification scores where it is ambigous as the distribution for signal and background entries overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_metrics(history_dict):\n",
    "    \"\"\"\n",
    "    Function to plot the training and validation loss of the network\n",
    "    \"\"\"\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    # Plot the loss \n",
    "    ax1.plot(history_dict[\"loss\"], color=\"maroon\", label=\"Training\")\n",
    "    ax1.plot(history_dict[\"val_loss\"], color=\"darkblue\", label=\"Validation\")\n",
    "    ax1.legend()\n",
    "    ax1.set(\n",
    "        xlabel = \"Epochs\",\n",
    "        ylabel = \"BCE loss\",\n",
    "        title  = \"Loss during training\",\n",
    "    )\n",
    "\n",
    "    # Plot the  accuracy\n",
    "    ax2.plot(history_dict[\"accuracy\"], color=\"maroon\", label=\"Training\")\n",
    "    ax2.plot(history_dict[\"val_accuracy\"], color=\"darkblue\", label=\"Validation\")\n",
    "    ax2.legend()\n",
    "    ax2.set(\n",
    "        xlabel = \"Epochs\",\n",
    "        ylabel = \"Accuracy\",\n",
    "        title  = \"Accuracy during training\",\n",
    "    )\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "def print_final_accuracy(history_dict):\n",
    "    \"\"\"\n",
    "    Print the final accuracy of the network\n",
    "    \"\"\"\n",
    "\n",
    "    # Find index of largest val accuracy\n",
    "    # max_idx = np.argmax(np.array(history_dict[\"val_accuracy\"]))\n",
    "\n",
    "    print(f\"The final training accuracy is {history_dict['accuracy'][-1]:.2%}\")\n",
    "    print(f\"The final validation accuracy is {history_dict['val_accuracy'][-1]:.2%}\")\n",
    "\n",
    "def plot_confusion_matrix(classification_scores, targets, labels=[\"Background\", \"Signal\"]):\n",
    "    \"\"\"\n",
    "    Compute and plot the confusion matrix using the classificatrion scores provided by the ml classification algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    conf = confusion_matrix(targets, classification_scores,)\n",
    "    df = pd.DataFrame(conf, columns=labels, index=labels)\n",
    "    perc = df.copy()\n",
    "    cols=perc.columns.values\n",
    "    perc[cols]=perc[cols].div(perc[cols].sum(axis=1), axis=0).multiply(100)\n",
    "\n",
    "    annot=df.round(0).astype(str) + \"\\n\" + perc.round(1).astype(str) + \"%\"\n",
    "\n",
    "    x = sns.heatmap(df, annot=annot, fmt='', cmap=\"Blues\",  annot_kws={\"fontsize\":16}, linewidth=1, cbar=False)\n",
    "    x.set_xticklabels(x.get_xmajorticklabels(), fontsize = 14)\n",
    "    x.set_yticklabels(x.get_ymajorticklabels(), fontsize = 14)\n",
    "    plt.xlabel(\"Predicted label\", fontsize=16, )\n",
    "    plt.ylabel(\"Truth label\", fontsize=16, )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(classification_socre_train, classification_score_val, targets_train, targets_val):\n",
    "    \"\"\"\n",
    "    Plot the roc curve using the validation scores of the classification scores ml classification algorithm\n",
    "    \"\"\"\n",
    "    # Compute the false positive and true positive rate of for the roc curve\n",
    "    fpr_val, tpr_val, _     = roc_curve(targets_val, classification_score_val,)\n",
    "    fpr_train, tpr_train, _ = roc_curve(targets_train, classification_socre_train,)\n",
    "\n",
    "    # Compute the area under curve for the roc curve\n",
    "    aoc_val     = roc_auc_score(targets_val, classification_score_val,)\n",
    "    aoc_train   = roc_auc_score(targets_train, classification_socre_train,)\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    plt.plot(fpr_val, tpr_val, color=\"maroon\", label=f\"Validation (AUC = {aoc_val:.2f})\")\n",
    "    plt.plot(fpr_train, tpr_train, color=\"darkblue\", label=f\"Training (AUC = {aoc_train:.2f})\")\n",
    "    plt.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100), color=\"black\", label=\"Random (AUC = 0.5)\", linestyle=\"--\")\n",
    "    plt.xlabel(\"False positive rate\")\n",
    "    plt.ylabel(\"True positive rate\")\n",
    "    plt.title(\"Improved classifier ROC curve\", fontsize=14, y=1.03)\n",
    "    plt.legend()\n",
    "\n",
    "def plot_classification_score_distribution(classification_scores, targets):\n",
    "    \"\"\"\n",
    "    Plot the distribution of classification scores for the signal and background entries\n",
    "    \"\"\"\n",
    "    background_mask, signal_mask = (targets.ravel() == 0), (targets.ravel() == 1)\n",
    "    plt.hist(classification_scores.ravel()[background_mask], bins=100, label=\"Background\", color=\"maroon\", alpha=0.5,)\n",
    "    plt.hist(classification_scores.ravel()[signal_mask], bins=100, label=\"Signal\", color=\"darkblue\", alpha=0.5, )\n",
    "    plt.xlabel(\"Classification score\")\n",
    "    plt.ylabel(\"Number of entries\")\n",
    "    plt.title(\"Classification score distribution\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a function which we can feed it all the necesary parameters to create a compuled neural network. by giving it a different list for the ```features``` parameter, we can modily the number of hidden layers and the number of features each layer contains. This will permit us to rapidly create new NN oibjects to test different layouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn_classifier(num_inputs, num_outputs, features, activation, loss=\"binary_crossentropy\", learning_rate=5e-4, metrics=[\"accuracy\"], initializer=\"normal\", final_layer=\"sigmoid\"):\n",
    "    \"\"\"\n",
    "    Function that will create a new compiled neural network \n",
    "    \"\"\"\n",
    "\n",
    "    # Define initializer\n",
    "    initializer = tf.keras.initializers.GlorotNormal(seed=SEED) if initializer == \"xavier\" else \"normal\"\n",
    "\n",
    "    # Define initial layer of the network\n",
    "    network = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(features[0], activation=activation, input_dim=num_inputs, kernel_initializer=initializer),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "    ])\n",
    "\n",
    "    # Add the hidden layers of the neural network\n",
    "    for feature in features[1:]:\n",
    "        network.add(\n",
    "            tf.keras.layers.Dense(feature, activation=activation, kernel_initializer=initializer)\n",
    "        )\n",
    "        network.add(\n",
    "            tf.keras.layers.Dropout(0.2)\n",
    "        )\n",
    "    \n",
    "    # Add output layer of neural network\n",
    "    network.add(\n",
    "        tf.keras.layers.Dense(num_outputs, activation=final_layer, kernel_initializer=initializer,)\n",
    "    )\n",
    "\n",
    "    # Define optimizer object\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile and return the network\n",
    "    network.compile(optimizer=optimizer, loss=loss, metrics=metrics,)\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will create two identicle compiled neural networks which will be used to test which one of the two datasets (pre or post kinematic cuts) provides the better trained network.\n",
    "\n",
    "We will use a standard upscale form the number of inputs to 512 nodes. Follwed by subsequent hidden layers downscaleing the latent vector down untill se provide a binary output sclaed from 0 to 1 which will serve as our classification score. Where 0 represents a perfect xlassification as a backgreound event and vice versa.\n",
    "\n",
    "To train the network, we will use the standard binary cross entropy loss function.\n",
    "\n",
    "The activation fuctnions used after every hidden layer will be tested between RELU,. GELU and SELU. We will check which one precisely results in better classification accuracies.\n",
    "\n",
    "Lastly we will test the kernal initialised which is used by the network, between a normal distribution and Xaviar. \n",
    "\n",
    "Below is also a network which output normalised probabilities for both the classification of backgeound and signal using one hot encoded vectors. While uncommon, its worth a try.\n",
    "\n",
    "The training of all the networks wtested will be contained in this notbook at the same time, this is to cut down on the execution time of the notebook. However, the results of the training of networks which are ommited will be mentioned later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters for improved network\n",
    "improved_network_hyperparameters = {\"features\": [512, 256, 128, 64, 32, 16, 8, 4]}\n",
    "\n",
    "# # Create the classifier object\n",
    "# classifier_network_no_cut = create_nn_classifier(\n",
    "#     num_inputs          = len(KEYS[\"features\"]),\n",
    "#     num_outputs         = 2,\n",
    "#     features            = improved_network_hyperparameters[\"features\"],\n",
    "#     activation          = \"selu\",\n",
    "#     initializer         = \"normal\",\n",
    "#     final_layer         = \"softmax\",\n",
    "#     loss = \"categorical_crossentropy\"\n",
    "# )\n",
    "\n",
    "# Create the classifier object\n",
    "classifier_network_no_cut = create_nn_classifier(\n",
    "    num_inputs          = len(KEYS[\"features\"]),\n",
    "    num_outputs         = len(KEYS[\"targets\"]),\n",
    "    features            = improved_network_hyperparameters[\"features\"],\n",
    "    activation          = \"selu\",\n",
    "    initializer         = \"normal\",\n",
    ")\n",
    "\n",
    "classifier_network_harsh_cut = create_nn_classifier(\n",
    "    num_inputs          = len(KEYS[\"features\"]),\n",
    "    num_outputs         = len(KEYS[\"targets\"]),\n",
    "    features            = improved_network_hyperparameters[\"features\"],\n",
    "    activation          = \"selu\",\n",
    "    initializer         = \"normal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on no cut dataset\n",
    "\n",
    "First we will train the network on the dataset without kinematic parameters. The loss and accuracies throughout the training are also plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_no_cut = classifier_network_no_cut.fit(\n",
    "    no_cut_features_train,\n",
    "    no_cut_targets_train,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = (no_cut_features_val, no_cut_targets_val),\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the final accuracy of the model \n",
    "print_final_accuracy(history_no_cut.history)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# plot the loss and accuracy while training simpole model\n",
    "plot_training_metrics(history_dict=history_no_cut.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try with dataset using harsh kinematic cuts and compare the difference in the two networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_harsh_cut = classifier_network_harsh_cut.fit(\n",
    "    harsh_cut_features_train,\n",
    "    harsh_cut_targets_train,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = (harsh_cut_features_val, harsh_cut_targets_val),\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the final accuracy of the model \n",
    "print_final_accuracy(history_harsh_cut.history)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# plot the loss and accuracy while training simpole model\n",
    "plot_training_metrics(history_dict=history_harsh_cut.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we we see that there is no real overtraing (in fact the validation dataset is slightly better than the training dataset). We also see that the model trained of the harsh cuts performs marginally better than the dataset with no cuts, hence we will use harshly cut datasets for the training of subsequent models.\n",
    "\n",
    "The reason for this small improvement is likely due tio the network not having to learn outlier background events which are far out of the range of the mass range of the signal events. that sair, given that they do not make up for a majority of the events, the increase is not significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding better architectures for NN classifier\n",
    "\n",
    "Now that we have identified which dataset better trains the network, we can move onto improving the architecture of the network to func a specific number of hidden layers and nodes which will improve the network.\n",
    "\n",
    "Here we will also test other improvements like learning rates, different kernel initialised and different activation funtions for the hidden layers.\n",
    "\n",
    "Similar to the procidure used above, we will train a specific network, and then we will plot the training metrics to evaluate the network performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier object\n",
    "classifier_network_harsh_cut_deep = create_nn_classifier(\n",
    "    num_inputs          = len(KEYS[\"features\"]),\n",
    "    num_outputs         = len(KEYS[\"targets\"]),\n",
    "    features            = [1024, 512, 256, 128, 64, 32, 16, 8, 4],\n",
    "    activation          = \"selu\",\n",
    "    initializer         = \"normal\",\n",
    ")\n",
    "classifier_network_harsh_cut_deeper = create_nn_classifier(\n",
    "    num_inputs          = len(KEYS[\"features\"]),\n",
    "    num_outputs         = len(KEYS[\"targets\"]),\n",
    "    features            = [2048, 1024, 512, 256, 128, 64, 32, 16, 8, 4],\n",
    "    activation          = \"selu\",\n",
    "    initializer         = \"normal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_harsh_cut_deep = classifier_network_harsh_cut_deep.fit(\n",
    "    harsh_cut_features_train,\n",
    "    harsh_cut_targets_train,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = (harsh_cut_features_val, harsh_cut_targets_val),\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the final accuracy of the model \n",
    "print_final_accuracy(history_harsh_cut_deep.history)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# plot the loss and accuracy while training simpole model\n",
    "plot_training_metrics(history_dict=history_harsh_cut_deep.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_harsh_cut_deeper = classifier_network_harsh_cut_deeper.fit(\n",
    "    harsh_cut_features_train,\n",
    "    harsh_cut_targets_train,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = (no_cut_features_val, no_cut_targets_val),\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the final accuracy of the model \n",
    "print_final_accuracy(history_harsh_cut_deeper.history)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# plot the loss and accuracy while training simpole model\n",
    "plot_training_metrics(history_dict=history_harsh_cut_deeper.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can tell that the deeper network with 10 hidden layers preformns the best with an accuracy of 90.5%. This marginal increase is likely due to the additional hidden layer allowing the network to make further abstractions on the input features which allow it to make an improved prediction on a specific entry.\n",
    "\n",
    "Additional configurations that were tested but ommited were:\n",
    "\n",
    "- Using GELU activation fucntion on the deeper NN $\\rightarrow$ 89% validation accuracy\n",
    "\n",
    "- Using RELU activation fucntion on the deeper NN $\\rightarrow$ 87% validation accuracy\n",
    "\n",
    "- Using 1e3 learning rate on the deeper NN $\\rightarrow$ 90.1% validation accuracy\n",
    "\n",
    "- Using Xaviar kernal initialisation on the deeper NN $\\rightarrow$ 90.0% validation accuracy\n",
    "\n",
    "From all the networks tested, it was forund that the 10 hidden layer feed forward network with SELU activation function trained on the dataset with harsh cuts applied performend the best when it came to classifting Signal and Background entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best performing network, we compute the classification scores and the final predictions of the NN on the training ands validation dataset so that we cam make the additional plots to evaluate the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on validation dataset\n",
    "harsh_cut_deeper_classification_scores_val = classifier_network_harsh_cut_deeper.predict(no_cut_features_val)\n",
    "harsh_cut_deeper_predictions_val = (harsh_cut_deeper_classification_scores_val >= 0.5).astype(int) \n",
    "\n",
    "# Get predictions on training dataset\n",
    "harsh_cut_deeper_classification_scores_train = classifier_network_harsh_cut_deeper.predict(no_cut_features_train)\n",
    "harsh_cut_deeper_predictions_train = (harsh_cut_deeper_classification_scores_train >= 0.5).astype(int) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can conform that the deeper network using the dataset pre kinematic cuts is the best.\n",
    "\n",
    "Using the functions created previously, we can make all relevant plots to evaluate the functionality of the network. \n",
    "\n",
    "We will first make all relevant plots and then comment on them atg the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_score_distribution(harsh_cut_deeper_classification_scores_val, no_cut_targets_val.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(harsh_cut_deeper_classification_scores_train, harsh_cut_deeper_classification_scores_val, harsh_cut_targets_train, harsh_cut_targets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(harsh_cut_deeper_predictions_val.ravel(), harsh_cut_targets_val.to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots, we can tell that the classiciation algorithm which was developed is very good. It is capable of predicting entries with an accuracy of ~90% efficneicy. From the classification plot distribution, we see that the distributions are skewed to their ideal classification scores with minimal overlap bvetween the distributions.\n",
    "\n",
    "This is supported by the ROC curve with an aalmost perfect ROC curve with an area under the curve of 0.96 for the validation dataset.\n",
    "\n",
    "From the confusion matrix, we can tell that the netweks has a slightly harder time at classifying background events, however the difference is marginal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now lets try BDT\n",
    "\n",
    "Given that we only need a classification algorithm, we are not limited to only using neural networks, we can also use gradient boosted decision trees such as the BDT algorithm.\n",
    "\n",
    "We can use HalvingGridSearchCV to scan through the hyperparameters to find the optimal set and compare it to the neural network.\n",
    "\n",
    "We will define a numer of numpy arrays with hyperparameters for the grid search top scan through and return them. Wa can then create a BDT with the optimal parameters to compute the classification socres.\n",
    "\n",
    "Similar to our best NN, we will be using the datasts with the harsh kineamtic cuts to train the BDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grid Search parameters\n",
    "# param_grid = {\n",
    "#     \"n_estimators\": [100],\n",
    "#     \"learning_rate\": np.linspace(1e-2, 4e-2, 10),\n",
    "#     \"max_depth\":    np.arange(10, 30),\n",
    "#     \"min_samples_leaf\": np.arange(30, 50,),\n",
    "# }\n",
    "\n",
    "# # Number of threads to use to speed up the grid search\n",
    "# n_jobs = 20\n",
    "\n",
    "# # Set the numpy pseudo RNG to our set seed \n",
    "# np.random.seed(SEED)\n",
    "\n",
    "# # Initialte our BDt object with base parameters which we will feed into our grid search\n",
    "# bdt_classifier = GradientBoostingClassifier(random_state=SEED)\n",
    "\n",
    "# # Initite our grid search object which will scan through our hyperparameter space\n",
    "# grid_search = HalvingGridSearchCV(estimator=bdt_classifier, param_grid=param_grid, n_jobs=n_jobs, verbose =1,)\n",
    "\n",
    "# grid_search.fit(harsh_cut_features_train, harsh_cut_targets_train.to_numpy().ravel())\n",
    "# print(\"Best estimator:\")\n",
    "# print(grid_search.best_estimator_)\n",
    "# print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What theearch found was that the ideal hyperparameters for the BDT are:\n",
    "\n",
    "- Learninig rate: 2.33e-2\n",
    "\n",
    "- Max depth: 11\n",
    "\n",
    "- minimum sample leaves: 45\n",
    "\n",
    "Hence we will create a BDT with these hyperparametrs and compute the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialte the BDT with optimal hyperparameters\n",
    "optimal_bdt = GradientBoostingClassifier(\n",
    "    learning_rate=2.33e-2,\n",
    "    max_depth=11,\n",
    "    min_samples_leaf=45,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Train the BDT on the training dataset\n",
    "optimal_bdt.fit(harsh_cut_features_train, harsh_cut_targets_train.to_numpy().ravel(),)\n",
    "\n",
    "# Use the trained BDT to compute the classification scores\n",
    "harsh_cut_bdt_classification_scores_val = optimal_bdt.predict(harsh_cut_features_val)\n",
    "harsh_cut_bdt_predictions_val = (harsh_cut_bdt_classification_scores_val >= 0.5).astype(int)\n",
    "\n",
    "# Compute the validation accuracy of the optimal BDT\n",
    "bdt_val_accuracy = accuracy_score(harsh_cut_targets_val.to_numpy().ravel(), harsh_cut_bdt_classification_scores_val)\n",
    "\n",
    "# Print the validation accuracy\n",
    "print(f\"The validation accuracy of the optimal BDT is {bdt_val_accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(harsh_cut_bdt_predictions_val.ravel(), harsh_cut_targets_val.to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While able to compete with the neural networks, the accuracy of the BDT is just shy of the best perfomring neural network. From the confusion matrix, we see that it has very similar perfomance to the Neural networks.\n",
    "\n",
    "Ultimately, due to it not being able to outpreform the NN, we will not investigate BDTs further in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Use classifcation network to improve fitting**\n",
    "\n",
    "Now that we have developed a NN classification algorithm, we can move on to using it to attempt to improve the hypothesis tensting by adding an additional restriction that only signal enties with a classification score above 0.5 will be included in the sigal mass distrivbution.\n",
    "\n",
    "Below we take the kinematic features used as the input for the neural network from the dataset after having applied harsh cuts and use the NN to obtrain classification scores. We then add those classification scores to the dataset's dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make network input array using all entries in the pre kiunematic cut dataset\n",
    "harsh_cut_all_features = merge_dataset_post_harsh_cut[KEYS[\"features\"]]\n",
    "\n",
    "# Use network to get classification scores\n",
    "harsh_cut_all_classification_scores = classifier_network_harsh_cut_deeper.predict(PowerTransformer().fit_transform(harsh_cut_all_features), batch_size=8192)\n",
    "\n",
    "# Create array with all the predictions using the classificaton socres\n",
    "harsh_cut_all_predictions = (harsh_cut_all_classification_scores >= 0.5).astype(int)\n",
    "\n",
    "# Add the predicitons to the merged dataframe with the precuts\n",
    "merge_dataset_post_harsh_cut[\"nn_prediction\"] = harsh_cut_all_predictions\n",
    "merge_dataset_post_harsh_cut[\"nn_classification_score\"] = harsh_cut_all_classification_scores\n",
    "\n",
    "merge_dataset_post_harsh_cut.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing this, we can follow the same frocedure wheich was undertaken on the Hypothesis testing section to fit the alternate and null hpyothesis to the new cleaned up signal+background distribution. The two fits are then used in conjucnction with Wilks theorem to compute the updated significance. \n",
    "\n",
    "Given that the procefure is almost identicle to the procedure used previously and all the funcitons get reused, we will not add any additional commentary to this section untill we obtain the updated significance and we can comment on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply kinematic cuts on the dataset\n",
    "for cut_key in harsh_cut_dictionary.keys():\n",
    "    # query_string = f\"{cut_key} < {cut_dictionary[cut_key][dataset_key]}\"\n",
    "    query_string = f\"{cut_key} > {harsh_cut_dictionary[cut_key]['minimum']}\" + \" & \" + f\"{cut_key} < {harsh_cut_dictionary[cut_key]['maximum']}\"\n",
    "    merge_dataset_pre_harsh_cut = merge_dataset_pre_harsh_cut.query(query_string)\n",
    "\n",
    "# Create datasets with signal and background entries in addition to applying threshold on classifcation score\n",
    "signal_background_dataset_cleaned, signal_dataset_cleaned, background_dataset_cleaned = split_dataset_signal_background(merge_dataset_post_harsh_cut, classification_score_threshold=0.5)\n",
    "\n",
    "# Compute the histogram bins and counts for each dataset\n",
    "signal_background_dataset_cleaned, signal_background_counts_cleaned, _, signal_background_bins_center_cleaned = compute_dataset_histograms(signal_background_dataset_cleaned, n_bins=N_BINS)\n",
    "signal_dataset_cleaned, signal_counts_cleaned, _, signal_bins_center_cleaned = compute_dataset_histograms(signal_dataset_cleaned, n_bins=N_BINS)\n",
    "background_dataset_cleaned, background_counts_cleaned, _, background_bins_center_cleaned = compute_dataset_histograms(background_dataset_cleaned, n_bins=N_BINS)\n",
    "\n",
    "# Compute the weigthed number of events and errors for each histogram bin\n",
    "n_observed_signal_background_cleaned, sigma_squared_signal_background_cleaned = find_dataset_binned_events(signal_background_dataset_cleaned, signal_background_bins_center_cleaned)\n",
    "n_observed_signal_cleaned, sigma_squared_signal_cleaned = find_dataset_binned_events(signal_dataset_cleaned, signal_bins_center_cleaned)\n",
    "n_observed_background_cleaned, sigma_squared_background_cleaned = find_dataset_binned_events(background_dataset_cleaned, background_bins_center_cleaned,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Counts and pos to verify that they represent the histogram correctly\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 5))\n",
    "titles = [\"Total Mass distribution\", \"Signal mass distribution\", \"Background mass distribution\"]\n",
    "\n",
    "for idx, (dataset, bins_center, counts) in enumerate(zip([signal_background_dataset_cleaned, signal_dataset_cleaned, background_dataset_cleaned], [signal_background_bins_center_cleaned, signal_bins_center_cleaned, background_bins_center_cleaned], [signal_background_counts_cleaned, signal_counts_cleaned, background_counts_cleaned])):\n",
    "    ax[idx].scatter(bins_center, counts, c=\"r\", marker=\"x\", label=\"Post NN\")\n",
    "    ax[idx].hist(dataset[\"reco_zv_mass\"], weights=dataset[\"FullEventWeight\"], bins=N_BINS, alpha=0.5, color=\"blue\", label=\"Post NN\")\n",
    "    ax[idx].set_xlabel(\"Reco Mass (MeV)\", fontsize=12)\n",
    "    ax[idx].set_ylabel(\"Number of Events\", fontsize=12)\n",
    "\n",
    "for idx, (dataset, bins_center, counts) in enumerate(zip([signal_background_dataset, signal_dataset, background_dataset], [signal_background_bins_center, signal_bins_center, background_bins_center], [signal_background_counts, signal_counts, background_counts])):\n",
    "    ax[idx].scatter(bins_center, counts, c=\"r\", marker=\"o\", label=\"Pre NN\")\n",
    "    ax[idx].hist(dataset[\"reco_zv_mass\"], weights=dataset[\"FullEventWeight\"], bins=N_BINS, alpha=0.5, color=\"green\", label=\"Pre NN\")\n",
    "    ax[idx].set_title(titles[idx], fontsize=14)\n",
    "    ax[idx].set_xlabel(\"Reco Mass (MeV)\", fontsize=12)\n",
    "    ax[idx].set_ylabel(\"Number of Events\", fontsize=12)\n",
    "    ax[idx].legend()\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared_alternative_gaussian_plus_fourth_poly_cleaned( a, b, c, d, e, mu, sigma, norm,):\n",
    "    numerator = (n_observed_signal_background_cleaned - gaussian_plus_fourth_poly(signal_background_bins_center_cleaned, a, b, c, d, e, mu, sigma, norm) )**2\n",
    "    return np.sum( numerator/sigma_squared_signal_background_cleaned )\n",
    "\n",
    "def chi_squared_null_fourth_poly_cleaned( a, b, c, d, e,):\n",
    "    numerator = (n_observed_signal_background_cleaned - fourth_order_poly(signal_background_bins_center_cleaned, a, b, c, d, e) )**2\n",
    "    return np.sum( numerator/sigma_squared_signal_background_cleaned )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define minimiser to minimise the alternatice hypothesis\n",
    "signal_background_cleaned_fit_alternate_results = Minuit(\n",
    "    chi_squared_alternative_gaussian_plus_fourth_poly_cleaned,\n",
    "    a=background_fit_results.values[0],\n",
    "    b=background_fit_results.values[1],\n",
    "    c=background_fit_results.values[2], \n",
    "    d=background_fit_results.values[3],\n",
    "    e=background_fit_results.values[4],\n",
    "    mu=signal_fit_results.values[0],\n",
    "    sigma=signal_fit_results.values[1],\n",
    "    norm=signal_fit_results.values[2],\n",
    ")\n",
    "\n",
    "# Fix the mean and std of the signal\n",
    "signal_background_cleaned_fit_alternate_results.fixed[\"mu\"] = True \n",
    "signal_background_cleaned_fit_alternate_results.fixed[\"sigma\"] = True\n",
    "\n",
    "# Minimise the null hypothesis\n",
    "signal_background_cleaned_fit_alternate_results.migrad()\n",
    "signal_background_cleaned_fit_alternate_results.hesse()\n",
    "\n",
    "# Print minimised parameters\n",
    "print(signal_background_cleaned_fit_alternate_results.params)\n",
    "\n",
    "# Print the minimised chi squared\n",
    "print('Final chisq for alternative hypothesis on cleaned dataset: ', signal_background_cleaned_fit_alternate_results.fval)\n",
    "\n",
    "# Plot the minimised alternative hypothesis \n",
    "plt.scatter(signal_background_bins_center_cleaned, signal_background_counts_cleaned, c=\"r\", marker=\"x\", label=\"Observed events\")\n",
    "plt.hist(signal_background_dataset_cleaned[\"reco_zv_mass\"], weights=signal_background_dataset_cleaned[\"FullEventWeight\"], bins=N_BINS, alpha=0.3, color=\"blue\")\n",
    "plt.plot(signal_background_bins_center_cleaned, gaussian_plus_fourth_poly(signal_background_bins_center_cleaned, *signal_background_cleaned_fit_alternate_results.values), color=\"green\", label=\"Alternate hypothesis\")\n",
    "plt.title(\"Alternative hypothesis fit on total mass distribution\", fontsize=14)\n",
    "plt.xlabel(\"Reco Mass (MeV)\", fontsize=12)\n",
    "plt.ylabel(\"Number of Events\", fontsize=12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define minimiser to minimise the null hypothesis\n",
    "signal_background_cleaned_null_fit_results = Minuit(\n",
    "    chi_squared_null_fourth_poly_cleaned,\n",
    "    a=background_fit_results.values[0],\n",
    "    b=background_fit_results.values[1],\n",
    "    c=background_fit_results.values[2], \n",
    "    d=background_fit_results.values[3],\n",
    "    e=background_fit_results.values[4],\n",
    ")\n",
    "\n",
    "# Minimise the null hypothesis\n",
    "signal_background_cleaned_null_fit_results.migrad()\n",
    "signal_background_cleaned_null_fit_results.hesse()\n",
    "\n",
    "# Print minimised parameters\n",
    "print(signal_background_cleaned_null_fit_results.params)\n",
    "\n",
    "# Print the minimised chi squared\n",
    "print('Final chisq for the null hypothesis: ', signal_background_cleaned_null_fit_results.fval)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(signal_background_bins_center_cleaned, signal_background_counts_cleaned, c=\"r\", marker=\"x\", label=\"Observed events\")\n",
    "plt.hist(signal_background_dataset_cleaned[\"reco_zv_mass\"], weights=signal_background_dataset_cleaned[\"FullEventWeight\"], bins=N_BINS, alpha=0.3, color=\"blue\")\n",
    "plt.plot(signal_background_bins_center_cleaned, fourth_order_poly(signal_background_bins_center_cleaned, *signal_background_cleaned_null_fit_results.values), color=\"green\", label=\"Null hypothesis\")\n",
    "plt.title(\"Null hypothesis fit on total mass distribution\", fontsize=14)\n",
    "plt.xlabel(\"Reco Mass (MeV)\", fontsize=12)\n",
    "plt.ylabel(\"Number of Events\", fontsize=12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.scatter(signal_background_bins_center_cleaned, signal_background_counts_cleaned, c=\"r\", marker=\"x\")\n",
    "plt.hist(signal_background_dataset_cleaned[\"reco_zv_mass\"], weights=signal_background_dataset_cleaned[\"FullEventWeight\"], bins=N_BINS, alpha=0.3, color=\"blue\")\n",
    "plt.plot(signal_background_bins_center_cleaned, fourth_order_poly(signal_background_bins_center_cleaned, *signal_background_cleaned_null_fit_results.values), label=\"Null hypothesis\", color=\"darkgreen\")\n",
    "plt.plot(signal_background_bins_center_cleaned, gaussian_plus_fourth_poly(signal_background_bins_center_cleaned, *signal_background_cleaned_fit_alternate_results.values), label=\"Alternate hypothesis\", color=\"black\")\n",
    "plt.title(\"Total mass distribution with hypotheses fits\", fontsize=14)\n",
    "plt.xlabel(\"Reco Mass (MeV)\", fontsize=12)\n",
    "plt.ylabel(\"Number of Events\", fontsize=12)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the delta chi squared between the null and alternative hypothesis\n",
    "delt_chi = signal_background_cleaned_null_fit_results.fval - signal_background_cleaned_fit_alternate_results.fval\n",
    "\n",
    "# Define the difference number of degrees of freedom vetween the two fits\n",
    "dof = 1 \n",
    "\n",
    "# Compute the significance and the p score of the differentcwe in hypotheses\n",
    "p_value = chi2.sf(delt_chi, dof)\n",
    "z_score = np.sqrt(2)*erfinv(1-p_value)\n",
    "\n",
    "print(f\"The p value of the alternate hypothesis is {p_value:.3e}\")\n",
    "print(f\"The statistical significance of the signal deviation is {z_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this Updated dignificance, we immediatly see that the significance using the NN algorithm to clean up the signal has made the significanc smaller. This is contradictiry to what one would expect, however it makes sence.\n",
    "\n",
    "Given that the harsh cuts alreadyt significantly reduced the number of signal events in th mass distribution, adding another filter which will further reduce the total efficiency prior to hypothesis tensting only works to recuce the number of statistics for the signal distribution. This in turn will make the signal peak slightly less visiable, hence redicing the significance of the alternate hypotheses.\n",
    "\n",
    "Were we to have cuts which have a greater signal efficiency, or we had more signal entires to begin with and using the harsh cuts + the NN clasisfier, we could perhapse see some imporvements in the significance and the alternate hypothesis. However given the current circumstances of this analysis, it not possible.\n",
    "\n",
    "This reduction in visibuility in the signal peak is made more evident in the plot which overlays thew distribution pre application of the NN and opost application. There are less events in the signal distribution, while the backgeound remains unchanged given that we never applied the NN to background events, hence recucing the visibuility of the signal peak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Improving the NN classifier**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we retrain the NN with the inclusion of the reco_zv_mass in the training features to see how it will affect the network. Givent aht the procedure is identicle to that employed in the previous NN section, no additional commentary is provided untill the end where the results of the section are discussed.\n",
    "\n",
    "The basic overview on the seps which are applied are: Create a new dataset identicle to that used to train the last network with the inclusion of the reconstructed mass, We create a compiled NN, we train it and then we plot all relevant plots to evaluate the network's preformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new features key list including reco_zv_mass\n",
    "feature_keys_imporved = KEYS[\"features\"]\n",
    "feature_keys_imporved.append(\"reco_zv_mass\")\n",
    "print(feature_keys_imporved)\n",
    "\n",
    "# Create a new dataset including the reco zv mass\n",
    "harsh_cut_features_train_improved, harsh_cut_targets_train_improved, harsh_cut_features_val_improved, harsh_cut_targets_val_improved = create_ml_training_datasets(\n",
    "    merge_dataset_post_harsh_cut,\n",
    "    admixture_size=N_SIGNAL_SIZE,\n",
    "    feature_keys = feature_keys_imporved\n",
    ")\n",
    "\n",
    "# Create nn object for new imporved dataset\n",
    "classifier_network_harsh_cut_deeper_improved = create_nn_classifier(\n",
    "    num_inputs          = len(KEYS[\"features\"]),\n",
    "    num_outputs         = len(KEYS[\"targets\"]),\n",
    "    features            = [2048, 1024, 512, 256, 128, 64, 32, 16, 8, 4],\n",
    "    activation          = \"selu\",\n",
    "    initializer         = \"normal\",\n",
    ")\n",
    "\n",
    "# Fit the neural network to the imporved dataset\n",
    "history_harsh_cut_deeper_improved = classifier_network_harsh_cut_deeper_improved.fit(\n",
    "    harsh_cut_features_train_improved,\n",
    "    harsh_cut_targets_train_improved,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = (harsh_cut_features_val_improved, harsh_cut_targets_val_improved),\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the metrics of the network throughout training\n",
    "print_final_accuracy(history_harsh_cut_deeper_improved.history)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# plot the loss and accuracy while training simpole model\n",
    "plot_training_metrics(history_dict=history_harsh_cut_deeper_improved.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on validation dataset\n",
    "harsh_cut_deeper_classification_scores_val_improved = classifier_network_harsh_cut_deeper_improved.predict(harsh_cut_features_val_improved)\n",
    "harsh_cut_deeper_predictions_val_improved = (harsh_cut_deeper_classification_scores_val_improved >= 0.5).astype(int) \n",
    "\n",
    "# Get predictions on training dataset\n",
    "harsh_cut_deeper_classification_scores_train_improved = classifier_network_harsh_cut_deeper_improved.predict(harsh_cut_features_train_improved)\n",
    "harsh_cut_deeper_predictions_train_improved = (harsh_cut_deeper_classification_scores_train_improved >= 0.5).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_score_distribution(harsh_cut_deeper_classification_scores_val_improved, harsh_cut_targets_val_improved.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(harsh_cut_deeper_classification_scores_train_improved, harsh_cut_deeper_classification_scores_val_improved, harsh_cut_targets_train_improved, harsh_cut_targets_val_improved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(harsh_cut_deeper_predictions_val_improved.ravel(), harsh_cut_targets_val_improved.to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By including the reconstructed mass as a featurew in the NN, we find that the accuracy was slightly impoved to a validation accuracy of 90.7%. While the incerease is marginal, it is better than the NN which trained without the reconstructed mass.\n",
    "\n",
    "The exact effects are broken down in the consufion matrix. This imporved NN impoved the classification of signal entries by 3%. This is likely due to the fact that with the reocnstructed mass, it is easier for the network to identify which enties are signals, given that the signal only shows up in a very specific energy range, hence any entries that have a reconstructed mass outside of this range can automatically rejected as a signal by the network.\n",
    "\n",
    "Paradoxiacally, while the signal accuracy of the network increased, the backgeound accuracy reduces slightly by 2%. It is unknown if this is due to the stocastic nature of neural networks causing a worse performance on this seed with the addition of the new features, and if we were to change the seed we would se an inprovement in backgeound accuracy. Either way, given the marginal increase in overall accuracy, added with the fact that the efficeincy of the network is still not perfect, and would cause in the reduction signal entgries and hence most likely still result in the reducction of the significanse of the alternate hypothesis if it were applied ot the signal+bakcground reconstructed mass distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

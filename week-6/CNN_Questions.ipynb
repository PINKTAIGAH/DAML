{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJMA88F7g0DB"
   },
   "source": [
    "# Convolution Deep Neural Networks\n",
    "---\n",
    "*Responsible:* Robert Currie (<rob.currie@ed.ac.uk>)\n",
    "\n",
    "### Description:\n",
    "This is a longer notebook than we have had previously. It is intended as a hands-on example to using CNN/CDN models in different situations.\n",
    "\n",
    "There are 4 sections:\n",
    "\n",
    "**Image Filtering**- This section is intended to give you an idea how the Sobel Operator works.\n",
    "\n",
    "**CNN training with cifar10** - This section is intended to give an example of training a CNN model to a dataset. This also shows you how to avoid problems due to a complex model and a limited dataset size.\n",
    "\n",
    "**VAE examples with mnist** - This section is intended to give you an example of using a CNN VAE model to perform an un-supervised training over a dataset.\n",
    "\n",
    "**Anomaly Detection** - This section shows how you can make use of the latent-space of these models to perform anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking\n",
    "\n",
    "As with last weeks notebook the sections marked **##FINISH ME##** need to be completed for the notebook to work.\n",
    "\n",
    "Marks for the different parts are shown below.\n",
    "\n",
    "* Sections are intended to be tackled in order, i.e. 1->9\n",
    "* In this notebook different sections can be tackled independently\n",
    "* There are bonus problems at the end to tackle but the maximum mark is 10/10\n",
    "\n",
    "| <p align='left'> Title                         | <p align='left'> Parts | <p align='left'> Number of marks |\n",
    "| ------------------------------------- | ----- | --- |\n",
    "| <p align='left'> 1. Image Filtering (question)          | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 2. CNN training with cifar10 (code)     | <p align='left'>  1  | <p align='left'> 1 |\n",
    "| <p align='left'> 2. CNN training with cifar10 (questions)| <p align='left'>  4  | <p align='left'> 4 |\n",
    "| <p align='left'> 3. VAE examples with mnist (sections marked Q) | <p align='left'>  4  | <p align='left'> 4 |\n",
    "| <p align='left'> 4. **Bonus:** Anomaly Detection            | <p align='left'>  3  | <p align='left'> 3 |\n",
    "| <p align='left'> **Total** | | <p align='left'> max **10** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup our notebook env\n",
    "---\n",
    "Import all of the dependencies for our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from tensorflow.keras.layers import AveragePooling2D, Conv2D, MaxPooling2D, Activation\n",
    "from tensorflow.keras.activations import gelu\n",
    "from tensorflow.keras import models, layers, datasets, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, Input, InputLayer, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from sklearn.datasets import fetch_olivetti_faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Filtering\n",
    "---\n",
    "We're now going to go through an example of applying an image filter operator on an input image.\n",
    "\n",
    "In reality we normally tend to use libraries which accelerate this rather than doing these calculations in a higher level language such as Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file = 'image.png'\n",
    "input_image = imread(image_file)  # this is the array representation of the input image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot our Input\n",
    "---\n",
    "Lets have a look at our input image from wikipedia: https://en.wikipedia.org/wiki/File:Eilean_Donan_Castle,_Scotland_-_Jan_2011.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(1)\n",
    "pic1=fig1.add_subplot()\n",
    "pic1.imshow(input_image, cmap=plt.get_cmap('gray'))\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to grayscale\n",
    "---\n",
    "This short section of code downsamples our input image to greyscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting each one of the RGB components\n",
    "r_img, g_img, b_img = input_image[:, :, 0], input_image[:, :, 1], input_image[:, :, 2]\n",
    "# The following operation will take weights and parameters to convert the color image to grayscale\n",
    "gamma = 1.400  # a parameter\n",
    "r_const, g_const, b_const = 0.2126, 0.7152, 0.0722  # weights for the RGB components respectively\n",
    "grayscale_image = r_const * r_img ** gamma + g_const * g_img ** gamma + b_const * b_img ** gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build our sobel operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build our Sobel operators\n",
    "\n",
    "```\n",
    "The kernels Gx and Gy as covered in the last lecture:\n",
    "      _               _                   _                _\n",
    "     |                 |                 |                  |\n",
    "     | 1.0   0.0  -1.0 |                 |  1.0   2.0   1.0 |\n",
    "Gx = | 2.0   0.0  -2.0 |    and     Gy = |  0.0   0.0   0.0 |\n",
    "     | 1.0   0.0  -1.0 |                 | -1.0  -2.0  -1.0 |\n",
    "     |_               _|                 |_                _|\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the matrices associated with the Sobel filter\n",
    "Gx = np.array([\n",
    "     1.0,  0.0,  -1.0,\n",
    "     2.0,  0.0,  -2.0,\n",
    "     1.0,  0.0,  -1.0,\n",
    "])\n",
    "\n",
    "Gy = np.array([\n",
    "      1.0,  2.0,  1.0,\n",
    "      0.0,  0.0,  0.0,\n",
    "     -1.0, -2.0, -1.0,\n",
    "])\n",
    "\n",
    "[rows, columns] = np.shape(grayscale_image)  # we need to know the shape of the input grayscale image\n",
    "\n",
    "sobel_filtered_image_x = np.zeros(shape=(rows, columns))  # initialization of the output image array (all elements are 0)\n",
    "sobel_filtered_image_y = np.zeros(shape=(rows, columns))  # initialization of the output image array (all elements are 0)\n",
    "sobel_filtered_image = np.zeros(shape=(rows, columns))  # initialization of the output image array (all elements are 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the Operator to our input\n",
    "\n",
    "A suitable example method for applying the full sobel operator to our input data is:\n",
    "```\n",
    "# Now we \"sweep\" the image in both x and y directions and compute the output\n",
    "for i in range(rows - 2):\n",
    "    for j in range(columns - 2):\n",
    "        gx = np.sum(np.multiply(Gx, grayscale_image[i:i + 3, j:j + 3]))  # x direction\n",
    "        gy = np.sum(np.multiply(Gy, grayscale_image[i:i + 3, j:j + 3]))  # y direction\n",
    "\n",
    "        sobel_filtered_image[i + 1, j + 1] = np.sqrt(gx ** 2 + gy ** 2)  # calculate the \"hypotenuse\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we \"sweep\" the image in both x and y directions and compute the output\n",
    "for i in range(rows - 2):\n",
    "    for j in range(columns - 2):\n",
    "        # print(Gx)\n",
    "        # print(grayscale_image[i:i + 3, j:j + 3])\n",
    "        gx = np.sum(np.multiply(Gx.reshape(3,3), grayscale_image[i:i + 3, j:j + 3]))  # x direction\n",
    "        gy = np.sum(np.multiply(Gy.reshape(3,3), grayscale_image[i:i + 3, j:j + 3]))  # y direction\n",
    "\n",
    "        sobel_filtered_image_x[i+1, j+1] = gx\n",
    "        sobel_filtered_image_y[i+1, j+1] = gy\n",
    "        sobel_filtered_image[i+1, j+1] = np.sqrt(gx**2 + gy**2)           # calculate the overall sobel filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Plot our output\n",
    "\n",
    "Plot the output of applying G_x, G_y and G operators each on the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(1)\n",
    "pic1=fig1.add_subplot()\n",
    "pic1.imshow(sobel_filtered_image_x, cmap=plt.get_cmap('gray'))\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(1)\n",
    "pic1=fig1.add_subplot()\n",
    "pic1.imshow(sobel_filtered_image_y, cmap=plt.get_cmap('gray'))\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure(1)\n",
    "pic1=fig1.add_subplot()\n",
    "pic1.imshow(sobel_filtered_image, cmap=plt.get_cmap('gray'))\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What is the difference between the G_x and G operator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sobel operator is used find the edges in images. This is typically done by discriminating aread of the image that have a large difference in pixel values between ajacent pixels (computing the $2^{nd}$ order derrivative of an image in a specific direction). \n",
    "\n",
    "The $G_x$ operator will discriminate edges in a horizontal direction, hence any large change between pixels ajacent to each other vertically will not be discriminated and taken into account. \n",
    "\n",
    "Meanwhile, the $G$ operator is the square root of the sum of squares of the $G_x$ and $G_y$ operators, hence the G operator combines the edges of an image in both the horizontal and vertical direction, resulting $G$ finding the edges of an image in all directions. This is a similar result to the application of the laplacian operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN training with cifar10\n",
    "---\n",
    "\n",
    "This section gives an example of using a CNN to train on the cifar10 dataset.\n",
    "\n",
    "The cifar10 dataset is a dataset of labelled images which is composed of the categories `['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']`.\n",
    "\n",
    "As the dataset is composed of colour images, the data is composed of pixels in 3 dimensions, (r,g,b) which are used to construct a full colour image.\n",
    "\n",
    "This dataset contains a lot of low-resolution images that have been hand labelled, however, this is a finite dataset with quite a few categories which means training to the dataset can be tricky. Typically a lot of simple CNN models tend to reach a non-overfitted accuracy of 70-80%.\n",
    "\n",
    "\n",
    "Lets start by loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "#hyper-parameters\n",
    "BATCH_SIZE = 600\n",
    "nb_epochs = 15\n",
    "VALIDATION_SPLIT = 0.2\n",
    "num_classes = 10\n",
    "minimum_pixel_value = X_train.min().astype(\"float32\")\n",
    "maximum_pixel_value = X_train.max().astype(\"float32\")\n",
    "\n",
    "\n",
    "# dataset \n",
    "num_train, img_channels, img_rows, img_cols = X_train.shape\n",
    "num_test, _, _, _= X_test.shape\n",
    "class_names =['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot some examples from our input\n",
    "---\n",
    "Plot a single example from each of the classes in the dataset and label them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax0, ax1, ax2, ax3, ax4,),(ax5, ax6, ax7, ax8, ax9)) = plt.subplots(2, 5)\n",
    "\n",
    "axes = (ax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9,)\n",
    "\n",
    "for i in range(num_classes):\n",
    "    image = X_test[np.argwhere(y_test==i)[0,0]]\n",
    "    axes[i].imshow(image,)\n",
    "    axes[i].set_title(class_names[i])\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we're going to Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to categorical\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "#float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= maximum_pixel_value\n",
    "X_test /= maximum_pixel_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build CNN model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a CNN model using the Sequential API which has:\n",
    "```\n",
    "1. InputLayer\n",
    "2. Conv2D x 32 with kernel 3x3 and strides=2\n",
    "3. ReLU\n",
    "4. Conv2D x 128 with kernel 3x3 and strides=1\n",
    "5. ReLU\n",
    "6. Flatten\n",
    "7. Dense x 1024\n",
    "8. ReLU\n",
    "9. Dense x 1024\n",
    "10. ReLU\n",
    "11. Dense x classes\n",
    "12. softmax\n",
    "```\n",
    "\n",
    "This model will be used multiple times so we want to wrap it in a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add( InputLayer(input_shape=X_train.shape[1:]) )\n",
    "    model.add( Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same') )\n",
    "    model.add( Activation('relu') )\n",
    "    model.add( Conv2D(filters=128, kernel_size=3, strides=1, padding=\"same\") )\n",
    "    model.add( Activation('relu') )\n",
    "    model.add( Flatten() )\n",
    "    model.add( Dense(1024,))# use_bias=True, kernel_initializer=\"RandomUniform\") )\n",
    "    model.add( Activation('relu') )\n",
    "    model.add( Dense(1024,))# use_bias=True, kernel_initializer=\"RandomUniform\") )\n",
    "    model.add( Activation('relu') )\n",
    "    model.add( Dense(num_classes) )\n",
    "    model.add( Activation('softmax') )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_cnn_model()\n",
    "model.compile(loss= 'categorical_crossentropy', optimizer = 'adam', metrics= ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model\n",
    "---\n",
    "\n",
    "As per training a DNN to the mnist dataset we want to train our CDN to our cifar10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = model.fit(X_train, y_train, batch_size = BATCH_SIZE, epochs = nb_epochs, validation_data = (X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now examine the loss functions from this fit\n",
    "---\n",
    "\n",
    "Plot the model losses for the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.plot(model_history.history['loss'])\n",
    "ax1.plot(model_history.history['val_loss'])\n",
    "ax1.set_title(\"Model loss\")\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend(['train','validation'], loc = 'upper right')\n",
    "\n",
    "\n",
    "ax2.plot(model_history.history['accuracy'])\n",
    "ax2.plot(model_history.history['val_accuracy'])\n",
    "ax2.set_title(\"Model accuracy\")\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.legend(['train','validation'], loc = 'lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Has the model converged to a good description of the dataset? If not why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. From the above loss plot, while the training loss of the model converges to what seems like an accurate description of the dataset; the validation loss is diverging from the training loss and the validation acuracy stagnates significantly different value than the training acuracy.\n",
    "\n",
    "This is a clear indication that the model has been overtrained and is no longer capable of being generalied to data samples outside of the training dataset.\n",
    "\n",
    "This is most likely a result of the training dataset being too small, which may be improved by using an image generator to slightly modify training dataset to inclrease the overall number of images used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup an Image Generator\n",
    "---\n",
    "In order to try and mitigate problems of training a complex model to a small dataset, the dataset can be presented to the model in different ways to avoid the model over-training on wrong features within the dataset.\n",
    "\n",
    "This is achieved using a generator function which randomly performs transforms on the input dataset as it's fed to the model for training.\n",
    "\n",
    "We want this image generator to shift the input images in width and height in the range: 0.1, as well as introducing random shears in the range:0.1 and rotations of 15degree.\n",
    "\n",
    "We also want to perform horizontal flips of images randomly to allow our model to train better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmenting training set images\n",
    "datagen = ImageDataGenerator(\n",
    "    zoom_range=0,      # Not sure if range is [0, 1] or 0.1\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,     # Not sure if range is [0, 1] or 0.1\n",
    "    rotation_range=15,\n",
    "    horizontal_flip = True, \n",
    "    vertical_flip = False\n",
    ")\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train a new model using a Generator\n",
    "---\n",
    "\n",
    "As above, but this-time we want to train using an image generator rather than the raw dataset.\n",
    "\n",
    "This means that random images are passed to our model during training. These images are based on the input dataset, but have been modified to help the model produce a more generic description of the input data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_aug = create_cnn_model()\n",
    "model_aug.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_history = model_aug.fit(datagen.flow(X_train, y_train, batch_size = BATCH_SIZE), \n",
    "                                        steps_per_epoch = X_train.shape[0] / BATCH_SIZE,\n",
    "                                        epochs = nb_epochs, verbose = 1,\n",
    "                                        validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again Look at the loss functions from this fit\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.plot(model_history.history['loss'])\n",
    "ax1.plot(model_history.history['val_loss'])\n",
    "ax1.set_title(\"Model loss\")\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend(['train','validation'], loc = 'upper right')\n",
    "\n",
    "\n",
    "ax2.plot(model_history.history['accuracy'])\n",
    "ax2.plot(model_history.history['val_accuracy'])\n",
    "ax2.set_title(\"Model accuracy\")\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.legend(['train','validation'], loc = 'lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Was this training with the generator better than training the same model on raw data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, as we can see in the plots above, the model loss and accuracy for both the validation and training are similar values throughout the entire training process, which is a clear indication that the overtraining issue which plagued the models trained on the raw data has been overcome. \n",
    "\n",
    "Additionally, unlike the model trained on the raw datasets, the loss and accuracy of the model for both datasets have not stagnated after 15 epochs. This means that we could likely continue training the model for some more epochs and still see improvements in the preformance of the model without having to worry (too much) about overtraining (This however is still an assumption). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What advantages are there to training using an image generator?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using an image generator, we can \"$\\textit{artificially}$\" boost the number of images we have in out dataset, hence mitigating overtraining of a model.\n",
    "\n",
    "The word $\\textit{artificially}$ referes to te fact that while images that have had transformations applied to them by the generator are different in the eyes of the model to their \"parent\" image (as the python interpreter will read a different sequence of 1's and 0's compared to the \"parent\" image), the generated images are still ultimately a subset of their parent image and will contain very similar features which the model will learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What disadvantages are there to using an image generator when training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Applying transformations to the raw datasets may alter the images to such a point that they are no longer a correct representation of the data class it is meant to represent.\n",
    "\n",
    "The classic example is rotating images in the MNIST dataset by 180 degrees. By doing this, images that are a member of the class \"6\" will now look very similar to members of the class \"9\". As a result, there will be a discrepency between what the lable of the image says it is and what the image looks like. In this example, it would result in the model not being able to correctly classifiy images in the class \"6\" or \"9\".\n",
    "\n",
    "Another example of this specifically for the CIFAR10 is by applying a random zoom/crop to the image. There may be a case where we transform an image in such a way that we crop out the subject of the image and the generated image only contains the background (see the figure below). This would result in the model not reciving appropiate images for each label, which could result in reduced accuracy and possible model artificial hallucination (This would be an extreme example).\n",
    "\n",
    "The way of mitigatitng this would be to manually inspect that the variables used to define the range of the transformations are resonable and that they do not alter the nature of the classes which each image is meant to represent. This, however, was not done in this example and as a result we have no idea how the datasets created by the image generator actually look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Example of cropping issue with image generators \n",
    "\"\"\"\n",
    "\n",
    "idx = 1\n",
    "image = X_test[idx]\n",
    "cropped_image = image[:10, :10]\n",
    "label = class_names[tf.argmax(y_test[idx])]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.imshow(image,)\n",
    "ax1.set_title(\"Raw image\")\n",
    "ax1.set_xticks([])\n",
    "ax1.set_yticks([])\n",
    "\n",
    "ax2.imshow(cropped_image,)\n",
    "ax2.set_title(\"Cropped image\")\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "fig.suptitle(f\"Example of cropping issue w/ image generators\\n\\nlabel: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE example with mnist dataset\n",
    "---\n",
    "\n",
    "This section goes over what is required to build a full VAE model to train over the mnist dataset.\n",
    "\n",
    "This is an example of performing an un-supervised training on a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QL1mDkVVjTAx"
   },
   "source": [
    "### Data preprocessing and cleaning:\n",
    "---\n",
    "\n",
    "As per the last workshop, load the mnist dataset and normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evxspQgzhQA2"
   },
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = (28, 28) \n",
    "\n",
    "# Load MNIST dataset-\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Specify hyper-parameters-\n",
    "batch_size = 64\n",
    "num_classes = 10\n",
    "num_epochs = 100\n",
    "minimum_pixel_value = X_train.min().astype(\"float32\")\n",
    "maximum_pixel_value = X_train.max().astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize dataset\n",
    "---\n",
    "As per last week we want to normalize our dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQckXNy4hfOn"
   },
   "outputs": [],
   "source": [
    "# Convert datasets to floating point types-\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# Normalize the training and testing datasets-\n",
    "X_train /= maximum_pixel_value\n",
    "X_test /= maximum_pixel_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vhlx2T_uhnUT"
   },
   "outputs": [],
   "source": [
    "# convert class vectors/target to binary class matrices or one-hot encoded values-\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQWFT9oyhtOC"
   },
   "source": [
    "## Define Autoencoder using _Functional API_ & _Convolutional_ layers\n",
    "---\n",
    "\n",
    "Instead of using the sequential API for tensorflow, we now want to use the functional API:\n",
    "```\n",
    "eg:\n",
    "\n",
    "before we had:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SomeLayer)\n",
    "model.add(SomeLayer2)\n",
    "...\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "this now becomes:\n",
    "\n",
    "input = Input()\n",
    "x=SomeLayer()(input)\n",
    "x=SomeLayer2()(x)\n",
    "...\n",
    "model=Activation('softmax')(x)\n",
    "```\n",
    "\n",
    "Using this API you now need to finish the encoder and deoder model descriptions.\n",
    "\n",
    "\n",
    "In the encoder, ensure that every Conv2D layer is followed by a `gelu` activator.\n",
    "\n",
    "In the decoder ensure that the Conv2DTranspose layer uses (3, 3) kernels with 64, 64, 32 and 1 filters in total for each sequential layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our latent space\n",
    "---\n",
    "\n",
    "We are going to chose to use a latent space of 3 dimensions.\n",
    "The numer and size of the dimensions depends on the trade-off between the compression and size of the model and accuracy of the final output from the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YME8yQznhzkW"
   },
   "outputs": [],
   "source": [
    "# Specify latent space dimensions-\n",
    "latent_space_dim = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gfr__HdMh3cz"
   },
   "source": [
    "The last conv layer is flattened and connected to a Dense layer of size 2, which represents our 2-D latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLiym-Erh5N3"
   },
   "outputs": [],
   "source": [
    "# Define encoder-\n",
    "\n",
    "def get_encoder(encoder_output_dim):\n",
    "    encoder_input = Input(shape = (28,28,1))\n",
    "\n",
    "    x = Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same')(encoder_input)\n",
    "    x = gelu(x)\n",
    "\n",
    "    x = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
    "    x = gelu(x)\n",
    "\n",
    "    x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "    x = gelu(x)\n",
    "\n",
    "    x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
    "    x = gelu(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dense(units = encoder_output_dim)(x)\n",
    "    encoder_output = x\n",
    "\n",
    "    encoder_model = Model(encoder_input, encoder_output)\n",
    "\n",
    "    return encoder_model, encoder_input, encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aX3S_3nnh-V4",
    "outputId": "a9bcaed8-5916-46f9-b3d5-5f8d182f64f3"
   },
   "outputs": [],
   "source": [
    "# Sanity check-\n",
    "encoder_model, encoder_input, encoder_output = get_encoder(latent_space_dim)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t41Rr_oLh6_P"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_decoder(decoder_space_dim):\n",
    "    decoder_input = Input(shape = decoder_space_dim)\n",
    "\n",
    "    x = Dense(units = 7 * 7 * 64)(decoder_input)\n",
    "    x = Reshape((7, 7, 64))(x)\n",
    "\n",
    "    x = Conv2DTranspose(filters=64, kernel_size=3 ,strides = (1, 1), padding = 'same')(x)\n",
    "    x = gelu(x)\n",
    "\n",
    "    x = Conv2DTranspose(filters=64, kernel_size=3 ,strides = (2, 2), padding = 'same')(x)\n",
    "    x = gelu(x)\n",
    "\n",
    "    x = Conv2DTranspose(filters=32, kernel_size=3, strides = (2, 2), padding = 'same')(x)\n",
    "    x = gelu(x)\n",
    "\n",
    "    x = Conv2DTranspose(filters=1, kernel_size=1, strides = (1, 1), padding = 'same')(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "\n",
    "    decoder_output = x\n",
    "\n",
    "    decoder_model = Model(decoder_input, decoder_output)\n",
    "    \n",
    "    return decoder_model, decoder_input, decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrhkTZdGh1AA",
    "outputId": "d241db36-5b45-4407-af40-dfce60554ed7"
   },
   "outputs": [],
   "source": [
    "# Sanity check-\n",
    "decoder_model, decoder_input, decoder_output = get_decoder(latent_space_dim)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl4ANV-YimZ6"
   },
   "source": [
    "## Joining the Encoder to the Decoder\n",
    "To train the encoder and decoder simultaneously, we need to define a model that will represent the flow of an image through the encoder and back out through the decoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egJDvoUSh1KI"
   },
   "outputs": [],
   "source": [
    "# The complete autoencoder-\n",
    "\n",
    "# The input to the autoencoder is the same as the input to the encoder.\n",
    "vae_model_input = encoder_input\n",
    "\n",
    "# The output from the autoencoder is the output from the encoder passed through\n",
    "# the decoder.\n",
    "vae_model_output = decoder_model(encoder_output)\n",
    "\n",
    "# The Keras model that defines the full autoencoderâ€”a model that takes an image,\n",
    "# and passes it through the encoder and back out through the decoder to generate\n",
    "# a reconstruction of the original image.\n",
    "vae_model = Model(vae_model_input, vae_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0dhzdrrix1j",
    "outputId": "e6e566ed-83ab-4f7f-a4a1-4c3798deb293"
   },
   "outputs": [],
   "source": [
    "# Final sanity check-\n",
    "vae_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, make a prediction using the vae_model to see what the output looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j6T2UnzxizDc",
    "outputId": "d51b3f57-e205-4a44-da2e-377e510bc14a"
   },
   "outputs": [],
   "source": [
    "# Sanity check-\n",
    "vae_model(X_train[:1, :]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: What is the output shape of the VAE model? Where do the dimension sizes come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the VAE should have the same dimentionality as the input. This is due to the fact that we are using the VAE as a generative model which is capable of generating images which are similar to those in the MNIST model.\n",
    "\n",
    "This is further suported by the fact that in the encoding and decoding sections of the VAE will downsample and upsample the image by the same amount respectivly. This would result in the dimentionality of the image being unchanged.\n",
    "\n",
    "As a result, as we are using the MNIST dataset as an input with a dimentionality of (CHANNEL = 1, HIGHT = 28, WIDTH = 28), the dimentionallity of the output of the VAE will have be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using our Encoder\n",
    "---\n",
    "Like when we used the predict function with the DNN model using the Sequential API, we can now make 'predictions' using our encoder of the distribution of datapoints in our un-trained latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get encoded latent space representation for train images-\n",
    "encoded_X_train = encoder_model(X_train[:10000])\n",
    "encoded_X_train = encoded_X_train.numpy()\n",
    "print(\"encoded_X_train.shape: {}\".format(encoded_X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\textbf{NOT FINISHED. CHECK WITH OTHERS}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Projecting the latent space in 2D\n",
    "---\n",
    "\n",
    "Project the values in the latent space corresponding to the first 10,000 of the points in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latent space of training images-\n",
    "plt.figure(figsize = (9, 7))\n",
    "y_labels=np.argmax(y_train[:10000], axis = 1)\n",
    "\n",
    "datapoints_x = []\n",
    "datapoints_y = []\n",
    "\n",
    "for i in range(10):\n",
    "    datapoints_x = []\n",
    "    datapoints_y = []\n",
    "    mask = y_labels == i\n",
    "    for latent_point in encoded_X_train[mask]:\n",
    "        \n",
    "        projected_latent_point = latent_point[:2]\n",
    "        datapoints_x.append(projected_latent_point[0])\n",
    "        datapoints_y.append(projected_latent_point[1])\n",
    "    plt.scatter(datapoints_x, datapoints_y, s=0.7, label=f\"Label: {i}\")\n",
    "plt.xlabel(\"latent space dim - 1\")\n",
    "plt.ylabel(\"latent space dim - 2\")\n",
    "plt.legend()\n",
    "plt.title(\"MNIST: Training images - Latent space 2D Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Model features\n",
    "\n",
    "We will be using a custom loss function to give a spread distribution in our latent space.\n",
    "\n",
    "We also want to make use of an early_stopping funtion to make sure our model doesn't significatly over-train.\n",
    "\n",
    "In the case of a VAE the early_stopping function is easier to encode. It may be worth considering **why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAQCoaTBirUW"
   },
   "outputs": [],
   "source": [
    "def RMSE_loss(y_true, y_pred):\n",
    "    # RMSE loss function.\n",
    "    return K.mean(K.square(y_true - y_pred), axis = [1, 2, 3])\n",
    "\n",
    "# Compile defined autoencoder model-\n",
    "vae_model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0003), loss = RMSE_loss, metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping criterion-\n",
    "early_stopping = EarlyStopping(monitor = 'loss', min_delta = 0.0001, patience = 3, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our VAE model\n",
    "---\n",
    "\n",
    "As before, now we want to train our VAE model.\n",
    "\n",
    "However, this is an un-supervised training as we will **not** be passing the labels of our dataset to the model and the intention is not to categorize our data, but to construct a decoder which can build images from our dataset based on their latent-space encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7csotk5-hx3p",
    "outputId": "6dd9f09e-ea5f-497a-b73d-15b33bb5bb8d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train autoencoder-\n",
    "vae_training_hist = vae_model.fit(x = X_train, y = X_train,\n",
    "    batch_size = batch_size, shuffle = True, validation_data=(X_test, X_test), verbose=2, epochs = num_epochs, callbacks = [early_stopping, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the training output\n",
    "---\n",
    "\n",
    "Here again we want to look at the loss from the fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize autoencoder training-\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.plot(vae_training_hist.history['loss'])\n",
    "ax1.plot(vae_training_hist.history['val_loss'])\n",
    "ax1.set_title(\"Model loss\")\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend(['train','validation'], loc = 'upper right')\n",
    "\n",
    "\n",
    "ax2.plot(vae_training_hist.history['accuracy'])\n",
    "ax2.plot(vae_training_hist.history['val_accuracy'])\n",
    "ax2.set_title(\"Model accuracy\")\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('accuracy')\n",
    "ax2.legend(['train','validation'], loc = 'lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlPnEo387CY2"
   },
   "source": [
    "## Analysis of _trained_ Autoencoder:\n",
    "---\n",
    "\n",
    "Again we can now make predictions using our encoder of our VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fN5fvpEmjXmY",
    "outputId": "3f6a1c9d-4132-49e9-b0d8-f1f4c6e289fe"
   },
   "outputs": [],
   "source": [
    "# Get encoded latent space representation for train images-\n",
    "encoded_X_train = encoder_model(X_train[:10000])\n",
    "encoded_X_train = encoded_X_train.numpy()\n",
    "print(f\"encoded_X_train.shape: {encoded_X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tK0w9oq6jTA-",
    "outputId": "7e95546a-ae61-4e6b-a32b-5efe106f7a80"
   },
   "outputs": [],
   "source": [
    "# Get encoded latent space representations for test images-\n",
    "encoded_X_test = encoder_model(X_test)\n",
    "encoded_X_test = encoded_X_test.numpy()\n",
    "print(f\"encoded_X_test.shape: {encoded_X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Plot the distribution of images in 2D of our latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "mXjI2u3akC-L",
    "outputId": "f043fa5c-ad60-46bc-cedc-ad79394c796a"
   },
   "outputs": [],
   "source": [
    "# # Visualize latent space of training images-\n",
    "\n",
    "plt.figure(figsize = (9, 7))\n",
    "y_labels=np.argmax(y_train[:10000], axis = 1)\n",
    "\n",
    "datapoints_x = []\n",
    "datapoints_y = []\n",
    "\n",
    "for i in range(10):\n",
    "    datapoints_x = []\n",
    "    datapoints_y = []\n",
    "    mask = y_labels == i\n",
    "    for latent_point in encoded_X_train[mask]:\n",
    "        \n",
    "        projected_latent_point = latent_point[:2]\n",
    "        datapoints_x.append(projected_latent_point[0])\n",
    "        datapoints_y.append(projected_latent_point[1])\n",
    "    plt.scatter(datapoints_x, datapoints_y, s=0.7, label=f\"Label: {i}\")\n",
    "plt.xlabel(\"latent space dim - 1\")\n",
    "plt.ylabel(\"latent space dim - 2\")\n",
    "plt.legend()\n",
    "plt.title(\"MNIST: Training images - Latent space 2D Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "rfFk48PUkq_Q",
    "outputId": "377aab84-9c4b-4486-8371-fbccfab0b925",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize latent space of validation images-\n",
    "\n",
    "plt.figure(figsize = (9, 7))\n",
    "y_labels=np.argmax(y_test[:10000], axis = 1)\n",
    "\n",
    "datapoints_x = []\n",
    "datapoints_y = []\n",
    "\n",
    "for i in range(10):\n",
    "    datapoints_x = []\n",
    "    datapoints_y = []\n",
    "    mask = y_labels == i\n",
    "    for latent_point in encoded_X_test[mask]:\n",
    "        \n",
    "        projected_latent_point = latent_point[:2]\n",
    "        datapoints_x.append(projected_latent_point[0])\n",
    "        datapoints_y.append(projected_latent_point[1])\n",
    "    plt.scatter(datapoints_x, datapoints_y, s=0.7, label=f\"Label: {i}\")\n",
    "plt.xlabel(\"latent space dim - 1\")\n",
    "plt.ylabel(\"latent space dim - 2\")\n",
    "plt.legend()\n",
    "plt.title(\"MNIST: Validation images - Latent space 2D Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDau_giM6Gi-"
   },
   "source": [
    "## Visualize Actual vs. Recreated MNIST Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDau_giM6Gi-"
   },
   "source": [
    "### Q: Using trained autoencoder show 5 're-generated' images from our VAE model and compare this to their original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_entries, mnist_hight, mnist_width, mnist_channel = X_train.shape\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "UOmsaCsF4hyq",
    "outputId": "dee33f4c-26de-40b3-8d39-c3a655d3c81d"
   },
   "outputs": [],
   "source": [
    "recreated_image = vae_model(X_train[:1, :])\n",
    "recreated_image = recreated_image.numpy().reshape(28, 28)\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "axarr[0].imshow(X_train[:1, :].reshape(28, 28), cmap = 'gray')\n",
    "axarr[1].imshow(recreated_image, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "5svzUFPT54UF",
    "outputId": "f9d108c1-7096-4b3f-96fc-a9712ca62028"
   },
   "outputs": [],
   "source": [
    "original_image = X_train[1:2, :]\n",
    "recreated_image = vae_model(original_image).numpy().reshape(28,28)\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "axarr[0].imshow(original_image.reshape(28, 28), cmap = 'gray')\n",
    "axarr[1].imshow(recreated_image, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "11zUUBkI6SdR",
    "outputId": "f46bd30c-1bba-4121-e665-690977579cf4"
   },
   "outputs": [],
   "source": [
    "original_image = X_train[2:3, :]\n",
    "recreated_image = vae_model(original_image).numpy().reshape(28,28)\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "axarr[0].imshow(original_image.reshape(28, 28), cmap = 'gray')\n",
    "axarr[1].imshow(recreated_image, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "N88zKD2W6gBn",
    "outputId": "e1b3efc4-5f83-486c-a980-dca2d768d3f1"
   },
   "outputs": [],
   "source": [
    "original_image = X_train[5:6, :]\n",
    "recreated_image = vae_model(original_image).numpy().reshape(28,28)\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "axarr[0].imshow(original_image.reshape(28, 28), cmap = 'gray')\n",
    "axarr[1].imshow(recreated_image, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "2PNyuN-NrH0H",
    "outputId": "f3daec6e-d3a1-4528-f4fa-6e7c465b9254"
   },
   "outputs": [],
   "source": [
    "original_image = X_train[4:5, :]\n",
    "recreated_image = vae_model(original_image).numpy().reshape(28,28)\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "axarr[0].imshow(original_image.reshape(28, 28), cmap = 'gray')\n",
    "axarr[1].imshow(recreated_image, cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WBUWE65rLNl"
   },
   "source": [
    "# Anomaly detection\n",
    "---\n",
    "\n",
    "We have introduced auto-encoders as a form of unsupervised learning, since we are not using the image labels during training. This means that auto-encoders are not ideal for image classification (at least not since we actually _have_ the labels), but they can be used for something else: anomaly detection. This is the task of identifying examples that the model considers \"anomalous\" with respect to the dataset used during training. \n",
    "\n",
    "First, we'll load in some \"anomalous\" data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = fetch_olivetti_faces(shuffle=True)['images']\n",
    "faces = faces[:,4:-4:2,4:-4:2,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the face images and the auto-encoder result\n",
    "\n",
    "* How many samples does the \"outlier\" dataset (_i.e._ `faces`) contain?\n",
    "* What is the shape of the images and what is the range of pixel intensities? Does this conform with the preprocessed MNIST images?\n",
    "* Display the first few face images.\n",
    "* Get the output/prediction of the auto-encoder from the previous section when applied to all of the faces.\n",
    "* Show the auto-encoded versions of the same faces you showed above. Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces_entries, faces_hight, faces_width, faces_channel = faces.shape\n",
    "faces_pixel_min, faces_pixel_max = faces.min(), faces.max()\n",
    "mnist_pixel_min, mnist_pixel_max = X_train.min(), X_train.max()\n",
    "\n",
    "print(f\"The number of outlier entries is {faces_entries}\\n\")\n",
    "print(f\"The pixel intensity range for the faces dataset is [{faces_pixel_min},{faces_pixel_max}]\")\n",
    "print(f\"The pixel intensity range for the mnist dataset is [{mnist_pixel_min},{mnist_pixel_max}]\\n\")\n",
    "print(f\"The image dimentions of the faces are ({faces_channel}, {faces_hight}, {faces_width}) where (channel, hight, width)\")\n",
    "print(f\"The image dimentions of the MNIST dataset are ({mnist_channel}, {mnist_hight}, {mnist_width}) where (channel, hight, width)\\n\")\n",
    "print(\"The two datasets constain the same dimentionality and pixel ranges\")\n",
    "\n",
    "# Display first 3 images\n",
    "\n",
    "fig, axes = plt.subplots(1,3)\n",
    "print(axes.shape)\n",
    "for idx, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(faces[idx], cmap=\"gray\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass a face image through the VAE and plot the input and output\n",
    "\n",
    "fig, axes = plt.subplots(3, 2)\n",
    "\n",
    "for idx, rowAx in enumerate(axes):\n",
    "    original_image = faces[idx:idx+1, :]\n",
    "    recreated_image = vae_model(original_image).numpy().reshape(28,28)\n",
    "    rowAx[0].imshow(np.squeeze(original_image, axis=0), cmap=\"gray\")\n",
    "    rowAx[1].imshow(recreated_image, cmap=\"gray\")\n",
    "    for ax in rowAx:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Indicative answers:_\n",
    "    \n",
    "* The face images have the same shape and pixel intensity range as the preprocessed MNIST images, so they are valid inputs to the auto-encoder.\n",
    "* We can see that the auto-encoder transformed faces are _very_ unlike the input images. This is because the auto-encoder was trained to learn an efficient representation of hand-written digits which is not necessarily an efficient representation for other image domains, _e.g._ faces. This examples shows that this is clearly the case. From the point of view of the auto-encoder, images of faces are _anomalies_ in that they are fundamentally unlike the images on which it was trained, and therefore we shouldn't expect it to do a good job in encoding them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform anomaly detection\n",
    "Imagine now that we had a dataset comprised of mostly MNIST images, but also a small subset of anomalies or \"outliers\"; here, in the form of black-and-white images of facses, with the same shape as the MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined MNIST images and \"outliers\" in a mixed dataset\n",
    "mixed = np.vstack((X_test, faces))\n",
    "\n",
    "# Shuffle the mixed dataset so the \"outliers\" are randomly distributed\n",
    "indices = np.random.permutation(mixed.shape[0])\n",
    "mixed   = mixed[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The auto-encoder was trained to minimise the difference between the original and the auto-encoded image, so let's use binary cross-entropy (BCE) as our metric for the difference between an image and its auto-encoded version. The `binary_crossentropy` method provided below computes **pixel-wise BCE** for two (arrays of) images: the input and the output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossentropy (img_in, img_out):\n",
    "    assert img_in.shape == img_out.shape\n",
    "    eps = np.finfo(float).eps\n",
    "    img_out = np.clip(img_out, eps, 1. - eps)\n",
    "    return - (img_in * np.log(img_out) + (1 - img_in) * np.log(1 - img_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q: Anomaly Detection\n",
    "---\n",
    "* Define a new method called `difference` which takes as input two arrays of images (`img_in` and `img_out`; similar to the `binary_crossentropy`); computes the **average BCE value for each image,** or row; and returns a vector of these difference measures.\n",
    "* For each image, or row, in `mixed`, compute the difference score of the auto-encoded image wrt. the original image. This is a measure of how \"inlier\"- or \"outlier\"-like an image is.\n",
    "* Make a histogram of these scores, and see if you can identify any structure. It might be useful to use a logarithmic x-axis (see `plt.xscale`) along with logarithmic x-axis bins (see `np.logspace`).\n",
    "* Show the 9 least and the 9 most outlying images, according to this difference score. Discuss the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference (img_in, img_out):\n",
    "    ## This method effectively calculates the difference between what is expected and what was prediced\n",
    "    return ## FINISH ME ##\n",
    "\n",
    "## Use the pre-trained mnist model to make predictions on our dataset\n",
    "p_mixed = vae_model.predict(mixed)\n",
    "score   = difference(mixed, p_mixed)\n",
    "\n",
    "## Plot the distribution of the magnitude of the bce values for different images\n",
    "\n",
    "## FINISH ME ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_indices_scores = sorted(zip(np.arange(score.size), score), key=lambda p: p[1])\n",
    "\n",
    "best_indices  = list(list(zip(*sorted_indices_scores[  :9]))[0])\n",
    "worst_indices = list(list(zip(*sorted_indices_scores[-9: ]))[0])\n",
    "\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "axarr[0].imshow(mixed[best_indices].reshape(9*28,28));\n",
    "axarr[1].imshow(mixed[worst_indices].reshape(9*28,28));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` ## FINISH ME ## ```  Discuss what images are at the 2 extremes of the latent-space when reduced to 1D, i.e. which are best reconstructed and maybe why and which images aren't and why that may be the case..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AutoEncoder_MNIST_TF2_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
